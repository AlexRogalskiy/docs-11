<?xml version="1.0"?>
<fo:root xmlns:fo="http://www.w3.org/1999/XSL/Format" xmlns:db="http://docbook.org/ns/docbook" xmlns:rx="http://www.renderx.com/XSL/Extensions" xmlns:xlink="http://www.w3.org/1999/xlink"><rx:meta-info><rx:meta-field name="author" value="&#10;        Glenn&#10;        Wright&#10;      "/><rx:meta-field name="creator" value="Kx Systems"/><rx:meta-field name="title" value="Migrating a kdb+ HDB to Amazon EC2"/><rx:meta-field name="keywords" value="Kx, Kx Systems, kdb+, Amazon, AWS, EC2, HDB, cloud"/></rx:meta-info><rx:outline><rx:bookmark internal-destination="migrating-a-kdb-hdb-to-amazon-ec2"><rx:bookmark-label>Migrating a kdb+ HDB to Amazon EC2</rx:bookmark-label></rx:bookmark><rx:bookmark internal-destination="in-house-vs-ec2"><rx:bookmark-label>In-house vs EC2</rx:bookmark-label></rx:bookmark><rx:bookmark internal-destination="historical-data-layouts-and-performance-testing"><rx:bookmark-label>Historical data layouts and performance testing</rx:bookmark-label></rx:bookmark><rx:bookmark internal-destination="data-locality"><rx:bookmark-label>Data locality</rx:bookmark-label></rx:bookmark><rx:bookmark internal-destination="getting-your-data-into-ec2"><rx:bookmark-label>Getting your data into EC2</rx:bookmark-label></rx:bookmark><rx:bookmark internal-destination="security-of-your-data-and-secure-access"><rx:bookmark-label>Security of your data and secure access</rx:bookmark-label></rx:bookmark><rx:bookmark internal-destination="getting-your-data-out-of-ec2"><rx:bookmark-label>Getting your data out of EC2</rx:bookmark-label></rx:bookmark><rx:bookmark internal-destination="storing-your-hdb-in-s3"><rx:bookmark-label>Storing your HDB in S3</rx:bookmark-label></rx:bookmark><rx:bookmark internal-destination="disaster-recovery"><rx:bookmark-label>Disaster recovery</rx:bookmark-label></rx:bookmark><rx:bookmark internal-destination="licensing-kdb-in-the-cloud"><rx:bookmark-label>Licensing kdb+ in the Cloud</rx:bookmark-label></rx:bookmark><rx:bookmark internal-destination="encryption"><rx:bookmark-label>Encryption</rx:bookmark-label></rx:bookmark><rx:bookmark internal-destination="benchmarking-methodology"><rx:bookmark-label>Benchmarking methodology</rx:bookmark-label></rx:bookmark><rx:bookmark internal-destination="observations-from-kdb-testing"><rx:bookmark-label>Observations from kdb+ testing</rx:bookmark-label></rx:bookmark><rx:bookmark internal-destination="network-configuration"><rx:bookmark-label>Network configuration</rx:bookmark-label></rx:bookmark><rx:bookmark internal-destination="appendix-a---elastic-block-store-ebs"><rx:bookmark-label>Appendix A - Elastic Block Store (EBS)</rx:bookmark-label></rx:bookmark><rx:bookmark internal-destination="appendix-b-efs-nfs"><rx:bookmark-label>Appendix B – EFS (NFS)</rx:bookmark-label></rx:bookmark><rx:bookmark internal-destination="appendix-c-amazon-storage-gateway-file-mode"><rx:bookmark-label>Appendix C – Amazon Storage Gateway (File mode)</rx:bookmark-label></rx:bookmark><rx:bookmark internal-destination="appendix-d-mapr-fs"><rx:bookmark-label>Appendix D – MapR-FS</rx:bookmark-label></rx:bookmark><rx:bookmark internal-destination="appendix-e---goofys"><rx:bookmark-label>Appendix E - Goofys</rx:bookmark-label></rx:bookmark><rx:bookmark internal-destination="appendix-f---s3fs"><rx:bookmark-label>Appendix F - S3FS</rx:bookmark-label></rx:bookmark><rx:bookmark internal-destination="appendix-g---s3ql"><rx:bookmark-label>Appendix G - S3QL</rx:bookmark-label></rx:bookmark><rx:bookmark internal-destination="appendix-h---objectivefs"><rx:bookmark-label>Appendix H - ObjectiveFS</rx:bookmark-label></rx:bookmark><rx:bookmark internal-destination="appendix-i-wekaio-matrix"><rx:bookmark-label>Appendix I – WekaIO Matrix</rx:bookmark-label></rx:bookmark><rx:bookmark internal-destination="appendix-j-quobyte"><rx:bookmark-label>Appendix J – Quobyte</rx:bookmark-label></rx:bookmark></rx:outline><fo:layout-master-set><fo:simple-page-master master-name="cover-page" page-width="8.5in" page-height="11in"><fo:region-body margin-top="0" margin-bottom="0" margin-left="0" margin-right="0" background-color="#eeeded"/></fo:simple-page-master><fo:simple-page-master master-name="toc" page-width="8.5in" page-height="11in" margin-top="30pt" margin-bottom="30pt" margin-left="45pt" margin-right="45pt"><fo:region-body margin-top="48pt" margin-bottom="32pt" margin-left="60pt" margin-right="60pt"/><fo:region-before extent="30pt"/><fo:region-after extent="30pt"/></fo:simple-page-master><fo:simple-page-master master-name="standard-page" page-width="8.5in" page-height="11in" margin-top="30pt" margin-bottom="30pt" margin-left="45pt" margin-right="45pt"><fo:region-body margin-top="48pt" margin-bottom="32pt" margin-left="40pt" margin-right="40pt"/><fo:region-before extent="30pt"/><fo:region-after extent="30pt"/></fo:simple-page-master></fo:layout-master-set><fo:page-sequence master-reference="cover-page" font-family="STIX2" initial-page-number="1" language="en" country="gb"><fo:flow flow-name="xsl-region-body"><fo:block-container absolute-position="absolute" top="0mm" right="0mm" width="2in" height="2in"><fo:block text-align="right"><fo:external-graphic src="url(/Users/sjt/Projects/kx/github/StephenTaylor-Kx/mkdocs2pdf/img/diamond-white.png)" content-width="40mm" scaling="uniform"/></fo:block></fo:block-container><fo:block-container absolute-position="absolute" bottom="0mm" left="0mm" width="2.5in" height="30mm"><fo:block><fo:external-graphic src="url(/Users/sjt/Projects/kx/github/StephenTaylor-Kx/mkdocs2pdf/img/diamond-bottom-left-white.png)" content-width="50mm" scaling="uniform"/></fo:block></fo:block-container><fo:block-container absolute-position="absolute" bottom="0mm" right="0mm" width="4in" height="50mm"><fo:block text-align="right"><fo:external-graphic src="url(/Users/sjt/Projects/kx/github/StephenTaylor-Kx/mkdocs2pdf/img/kx-cover.png)" content-width="75mm"/></fo:block></fo:block-container><fo:block-container absolute-position="absolute" top="35mm" left="15mm"><fo:block color="#0070cd" font-family="Proxima Nova" font-size="60pt" font-weight="bold" letter-spacing="-4pt">kx</fo:block></fo:block-container><fo:block-container absolute-position="absolute" top="40mm" left="40mm"><fo:block><fo:external-graphic src="url(/Users/sjt/Projects/kx/github/StephenTaylor-Kx/mkdocs2pdf/img/its-about-time.png)"/></fo:block></fo:block-container><fo:block-container absolute-position="absolute" top="80mm" left="35mm"><fo:block font-family="Proxima Nova" color="#0070cd" font-size="18pt" font-weight="400">
							Technical Whitepaper
						</fo:block><fo:block margin-top="9pt" margin-right="30mm" line-height="1.4" font-size="24pt" font-weight="400">Migrating a kdb+ HDB to Amazon EC2</fo:block></fo:block-container><fo:block-container absolute-position="absolute" top="170mm" left="35mm"><fo:block line-height="16pt" font-weight="bold">Date</fo:block></fo:block-container><fo:block-container absolute-position="absolute" top="170mm" left="55mm" width="100mm"><fo:block line-height="16pt" text-align="left">March 2018</fo:block></fo:block-container><fo:block-container absolute-position="absolute" top="180mm" left="35mm"><fo:block line-height="16pt" font-weight="bold">Author</fo:block></fo:block-container><fo:block-container absolute-position="absolute" top="180mm" left="55mm" width="100mm"><fo:block line-height="16pt" text-align="left"><fo:block line-height="16pt" space-after="9pt" text-align="justify">
      Glenn Wright, Systems Architect, Kx Systems, has 30+ years of
      experience within the high-performance computing industry. He has
      worked for several software and systems vendors where he has
      focused on the architecture, design and implementation of extreme
      performance solutions. At Kx, Glenn supports partners and
      solutions vendors to further exploit the industry- leading
      performance and enterprise aspects of kdb+.
    </fo:block></fo:block></fo:block-container><fo:block> </fo:block></fo:flow></fo:page-sequence><fo:page-sequence master-reference="toc" font-family="STIX2" language="en" country="gb"><fo:static-content flow-name="xsl-region-before"><fo:block text-align-last="justify" font-family="Proxima Nova" color="gray"><fo:inline font-size="9pt">Migrating a kdb+ HDB to Amazon EC2</fo:inline><fo:leader leader-pattern="space"/><fo:inline color="#0070cd" font-size="18pt" font-weight="bold" letter-spacing="-2pt">kx</fo:inline></fo:block></fo:static-content><fo:static-content flow-name="xsl-region-after"><fo:block color="gray" text-align="right"><fo:inline font-family="Proxima Nova" font-size="9pt" letter-spacing="2pt"><fo:page-number/></fo:inline></fo:block></fo:static-content><fo:flow flow-name="xsl-region-body"><fo:block break-before="page"><fo:block font-size="14pt" text-align="center" margin-top="36pt" margin-bottom="36pt">Contents</fo:block><fo:block margin-bottom="6pt" text-align-last="justify"><fo:basic-link internal-destination="idm220867411024">Migrating a kdb+ HDB to Amazon EC2 <fo:leader leader-pattern="dots"/> <fo:page-number-citation ref-id="idm220867411024"/></fo:basic-link></fo:block><fo:block margin-bottom="6pt" text-align-last="justify"><fo:basic-link internal-destination="idm220867377808">In-house vs EC2 <fo:leader leader-pattern="dots"/> <fo:page-number-citation ref-id="idm220867377808"/></fo:basic-link></fo:block><fo:block margin-bottom="6pt" text-align-last="justify"><fo:basic-link internal-destination="idm220867325296">Historical data layouts and performance testing <fo:leader leader-pattern="dots"/> <fo:page-number-citation ref-id="idm220867325296"/></fo:basic-link></fo:block><fo:block margin-bottom="6pt" text-align-last="justify"><fo:basic-link internal-destination="idm220867316400">Data locality <fo:leader leader-pattern="dots"/> <fo:page-number-citation ref-id="idm220867316400"/></fo:basic-link></fo:block><fo:block margin-bottom="6pt" text-align-last="justify"><fo:basic-link internal-destination="idm220867313456">Getting your data into EC2 <fo:leader leader-pattern="dots"/> <fo:page-number-citation ref-id="idm220867313456"/></fo:basic-link></fo:block><fo:block margin-bottom="6pt" text-align-last="justify"><fo:basic-link internal-destination="idm220867302336">Security of your data and secure access <fo:leader leader-pattern="dots"/> <fo:page-number-citation ref-id="idm220867302336"/></fo:basic-link></fo:block><fo:block margin-bottom="6pt" text-align-last="justify"><fo:basic-link internal-destination="idm220867290480">Getting your data out of EC2 <fo:leader leader-pattern="dots"/> <fo:page-number-citation ref-id="idm220867290480"/></fo:basic-link></fo:block><fo:block margin-bottom="6pt" text-align-last="justify"><fo:basic-link internal-destination="idm220867284144">Storing your HDB in S3 <fo:leader leader-pattern="dots"/> <fo:page-number-citation ref-id="idm220867284144"/></fo:basic-link></fo:block><fo:block margin-bottom="6pt" text-align-last="justify"><fo:basic-link internal-destination="idm220867226576">Disaster recovery <fo:leader leader-pattern="dots"/> <fo:page-number-citation ref-id="idm220867226576"/></fo:basic-link></fo:block><fo:block margin-bottom="6pt" text-align-last="justify"><fo:basic-link internal-destination="idm220867222800">Licensing kdb+ in the Cloud <fo:leader leader-pattern="dots"/> <fo:page-number-citation ref-id="idm220867222800"/></fo:basic-link></fo:block><fo:block margin-bottom="6pt" text-align-last="justify"><fo:basic-link internal-destination="idm220867217136">Encryption <fo:leader leader-pattern="dots"/> <fo:page-number-citation ref-id="idm220867217136"/></fo:basic-link></fo:block><fo:block margin-bottom="6pt" text-align-last="justify"><fo:basic-link internal-destination="idm220867214752">Benchmarking methodology <fo:leader leader-pattern="dots"/> <fo:page-number-citation ref-id="idm220867214752"/></fo:basic-link></fo:block><fo:block margin-bottom="6pt" text-align-last="justify"><fo:basic-link internal-destination="idm220867115184">Observations from kdb+ testing <fo:leader leader-pattern="dots"/> <fo:page-number-citation ref-id="idm220867115184"/></fo:basic-link></fo:block><fo:block margin-bottom="6pt" text-align-last="justify"><fo:basic-link internal-destination="idm220867088480">Network configuration <fo:leader leader-pattern="dots"/> <fo:page-number-citation ref-id="idm220867088480"/></fo:basic-link></fo:block><fo:block margin-bottom="6pt" text-align-last="justify"><fo:basic-link internal-destination="idm220867084048">Appendix A - Elastic Block Store (EBS) <fo:leader leader-pattern="dots"/> <fo:page-number-citation ref-id="idm220867084048"/></fo:basic-link></fo:block><fo:block margin-bottom="6pt" text-align-last="justify"><fo:basic-link internal-destination="idm220866988544">Appendix B – EFS (NFS) <fo:leader leader-pattern="dots"/> <fo:page-number-citation ref-id="idm220866988544"/></fo:basic-link></fo:block><fo:block margin-bottom="6pt" text-align-last="justify"><fo:basic-link internal-destination="idm220866943104">Appendix C – Amazon Storage Gateway (File mode) <fo:leader leader-pattern="dots"/> <fo:page-number-citation ref-id="idm220866943104"/></fo:basic-link></fo:block><fo:block margin-bottom="6pt" text-align-last="justify"><fo:basic-link internal-destination="idm220866924000">Appendix D – MapR-FS <fo:leader leader-pattern="dots"/> <fo:page-number-citation ref-id="idm220866924000"/></fo:basic-link></fo:block><fo:block margin-bottom="6pt" text-align-last="justify"><fo:basic-link internal-destination="idm220866893840">Appendix E - Goofys <fo:leader leader-pattern="dots"/> <fo:page-number-citation ref-id="idm220866893840"/></fo:basic-link></fo:block><fo:block margin-bottom="6pt" text-align-last="justify"><fo:basic-link internal-destination="idm220866872976">Appendix F - S3FS <fo:leader leader-pattern="dots"/> <fo:page-number-citation ref-id="idm220866872976"/></fo:basic-link></fo:block><fo:block margin-bottom="6pt" text-align-last="justify"><fo:basic-link internal-destination="idm220866854128">Appendix G - S3QL <fo:leader leader-pattern="dots"/> <fo:page-number-citation ref-id="idm220866854128"/></fo:basic-link></fo:block><fo:block margin-bottom="6pt" text-align-last="justify"><fo:basic-link internal-destination="idm220866849856">Appendix H - ObjectiveFS <fo:leader leader-pattern="dots"/> <fo:page-number-citation ref-id="idm220866849856"/></fo:basic-link></fo:block><fo:block margin-bottom="6pt" text-align-last="justify"><fo:basic-link internal-destination="idm220865726112">Appendix I – WekaIO Matrix <fo:leader leader-pattern="dots"/> <fo:page-number-citation ref-id="idm220865726112"/></fo:basic-link></fo:block><fo:block margin-bottom="6pt" text-align-last="justify"><fo:basic-link internal-destination="idm220865688864">Appendix J – Quobyte <fo:leader leader-pattern="dots"/> <fo:page-number-citation ref-id="idm220865688864"/></fo:basic-link></fo:block></fo:block></fo:flow></fo:page-sequence><fo:page-sequence master-reference="standard-page" font-family="STIX2" language="en" country="gb"><fo:static-content flow-name="xsl-region-before"><fo:block text-align-last="justify" font-family="Proxima Nova" color="gray"><fo:inline font-size="9pt">Migrating a kdb+ HDB to Amazon EC2</fo:inline><fo:leader leader-pattern="space"/><fo:inline color="#0070cd" font-size="18pt" font-weight="bold" letter-spacing="-2pt">kx</fo:inline></fo:block></fo:static-content><fo:static-content flow-name="xsl-footnote-separator"><fo:block><fo:leader leader-pattern="rule" rule-thickness=".5pt" leader-length="50%"/></fo:block></fo:static-content><fo:static-content flow-name="xsl-region-after"><fo:block color="gray" text-align="right"><fo:inline font-family="Proxima Nova" font-size="9pt" letter-spacing="2pt"><fo:page-number/></fo:inline></fo:block></fo:static-content><fo:flow flow-name="xsl-region-body"><fo:block id="migrating-a-kdb-hdb-to-amazon-ec2">
  <fo:block id="idm220867411024" font-size="18pt" line-height="22pt" margin-right="36pt" page-break-before="always" space-after="60pt" text-align="left">Migrating a kdb+ HDB to Amazon EC2</fo:block>
  
  <fo:block line-height="16pt" space-after="9pt" text-align="justify">
    Kx has an ongoing project of evaluating different cloud technologies
    to see how they interact with kdb+. If you are assessing migrating a
    kdb+ historical database (HDB) and analytics workloads into the
    <fo:basic-link external-destination="url('https://aws.amazon.com/ec2/')"><fo:inline color="#0070cd">Amazon Elastic
    Compute Cloud</fo:inline></fo:basic-link><fo:footnote><fo:inline font-size="8pt" alignment-baseline="hanging">1</fo:inline><fo:footnote-body><fo:block font-size="8pt">1.
									https://aws.amazon.com/ec2/</fo:block></fo:footnote-body></fo:footnote> (EC2), here are key considerations:
  </fo:block>
  <fo:list-block provisional-distance-between-starts="0.3cm" provisional-label-separation="0.15cm"><fo:list-item><fo:list-item-label end-indent="label-end()"><fo:block line-height="16pt">•</fo:block></fo:list-item-label><fo:list-item-body start-indent="body-start()">
      <fo:block line-height="16pt" space-after="9pt" text-align="justify">
        performance and functionality attributes expected from using
        kdb+, and the associated HDB, in EC2
      </fo:block>
    </fo:list-item-body></fo:list-item><fo:list-item><fo:list-item-label end-indent="label-end()"><fo:block line-height="16pt">•</fo:block></fo:list-item-label><fo:list-item-body start-indent="body-start()">
      <fo:block line-height="16pt" space-after="9pt" text-align="justify">
        capabilities of several storage solutions working in the EC2
        environment, as of March 2018
      </fo:block>
    </fo:list-item-body></fo:list-item><fo:list-item><fo:list-item-label end-indent="label-end()"><fo:block line-height="16pt">•</fo:block></fo:list-item-label><fo:list-item-body start-indent="body-start()">
      <fo:block line-height="16pt" space-after="9pt" text-align="justify">
        performance attributes of EC2, and benchmark results
      </fo:block>
    </fo:list-item-body></fo:list-item></fo:list-block>
  <fo:block line-height="16pt" space-after="9pt" text-align="justify">
    You must weigh the pros and cons of each solution. The key issues of
    each approach are discussed in the Appendices. We highlight specific
    functional constraints of each solution.
  </fo:block>
  <fo:block line-height="16pt" space-after="9pt" text-align="justify">
    We cover some of the in-house solutions supplied by Amazon Web
    Services (AWS), as well as a selection of some of the third-party
    solutions sold and supported for EC2, and a few open-source
    products. Most of these solutions are freely available for building
    and testing using Amazon Machine Images (AMI) found within the
    Amazon Marketplace.
  </fo:block>
  <fo:block id="why-amazon-ec2">
    <fo:block font-weight="bold" font-size="14pt" line-height="17pt" margin-right="36pt" page-break-after="avoid" space-after="6pt" space-before="18pt" text-align="left">Why Amazon EC2?</fo:block>
    <fo:block line-height="16pt" space-after="9pt" text-align="justify">
      <fo:basic-link external-destination="url('http://fortune.com/2017/06/15/gartner-cloud-rankings/')"><fo:inline color="#0070cd">Gartner</fo:inline></fo:basic-link><fo:footnote><fo:inline font-size="8pt" alignment-baseline="hanging">2</fo:inline><fo:footnote-body><fo:block font-size="8pt">2.
									http://fortune.com/2017/06/15/gartner-cloud-rankings/</fo:block></fo:footnote-body></fo:footnote>,
      and other sources such as
      <fo:basic-link external-destination="url('https://www.srgresearch.com/articles/microsoft-google-and-ibm-charge-public-cloud-expense-smaller-providers')"><fo:inline color="#0070cd">Synergy
      Research</fo:inline></fo:basic-link><fo:footnote><fo:inline font-size="8pt" alignment-baseline="hanging">3</fo:inline><fo:footnote-body><fo:block font-size="8pt">3.
									https://www.srgresearch.com/articles/microsoft-google-and-ibm-charge-public-cloud-expense-smaller-providers</fo:block></fo:footnote-body></fo:footnote>, rank cloud-services providers:
    </fo:block>
    <fo:list-block provisional-distance-between-starts=".7cm" provisional-label-separation="0.3cm"><fo:list-item><fo:list-item-label end-indent="label-end()"><fo:block line-height="16pt">1.</fo:block></fo:list-item-label><fo:list-item-body start-indent="body-start()">
        <fo:block line-height="16pt" space-after="9pt" text-align="justify">
          Amazon Web Services
        </fo:block>
      </fo:list-item-body></fo:list-item><fo:list-item><fo:list-item-label end-indent="label-end()"><fo:block line-height="16pt">2.</fo:block></fo:list-item-label><fo:list-item-body start-indent="body-start()">
        <fo:block line-height="16pt" space-after="9pt" text-align="justify">
          Microsoft Azure
        </fo:block>
      </fo:list-item-body></fo:list-item><fo:list-item><fo:list-item-label end-indent="label-end()"><fo:block line-height="16pt">3.</fo:block></fo:list-item-label><fo:list-item-body start-indent="body-start()">
        <fo:block line-height="16pt" space-after="9pt" text-align="justify">
          Google Cloud Platform
        </fo:block>
      </fo:list-item-body></fo:list-item></fo:list-block>
    <fo:block line-height="16pt" space-after="9pt" text-align="justify">
      This is partly due to the fact that Amazon was first to market,
      and partly because of their strong global data-center presence and
      rich sets of APIs and tools.
    </fo:block>
    <fo:block line-height="16pt" space-after="9pt" text-align="justify">
      Amazon EC2 is one of many services available to AWS users, and is
      managed via the AWS console. EC2 is typically used to host public
      estates of Web and mobile-based applications. Many of these are
      ubiquitous and familiar to the public. EC2 forms a significant
      part of the 
		‘Web 2.0/Semantic Web’
	 applications
      available for mobile and desktop computing.
    </fo:block>
    <fo:block line-height="16pt" space-after="9pt" text-align="justify">
      Kdb+ is a high-performance technology. It is often assumed the
      Cloud cannot provide a level of performance, storage and memory
      access commensurate with dedicated or custom hardware
      implementations. Porting to EC2 requires careful assessment of the
      functional performance constraints both in EC2 compute and in the
      supporting storage layers.
    </fo:block>
    <fo:block line-height="16pt" space-after="9pt" text-align="justify">
      Kdb+ users are sensitive to database performance. Many have
      significant amounts of market data – sometimes hundreds of
      petabytes – hosted in data centers. Understanding the issues is
      critical to a successful migration.
    </fo:block>
    <fo:block line-height="16pt" space-after="9pt" text-align="justify">
      Consider the following scenarios:
    </fo:block>
    <fo:list-block provisional-distance-between-starts="0.3cm" provisional-label-separation="0.15cm"><fo:list-item><fo:list-item-label end-indent="label-end()"><fo:block line-height="16pt">•</fo:block></fo:list-item-label><fo:list-item-body start-indent="body-start()">
        <fo:block line-height="16pt" space-after="9pt" text-align="justify">
          Your internal IT data services team is moving from an in-house
          data center to a cloud-services offering. This could be in
          order to move the IT costs of the internal data center from a
          capital expense line to an operating expense line.
        </fo:block>
      </fo:list-item-body></fo:list-item><fo:list-item><fo:list-item-label end-indent="label-end()"><fo:block line-height="16pt">•</fo:block></fo:list-item-label><fo:list-item-body start-indent="body-start()">
        <fo:block line-height="16pt" space-after="9pt" text-align="justify">
          You need your data analytics processing and/or storage
          capacity to be scaled up <fo:inline font-style="italic">instantly</fo:inline>,
          <fo:inline font-style="italic">on-demand</fo:inline>, and without the need to
          provide extra hardware in your own data center.
        </fo:block>
      </fo:list-item-body></fo:list-item><fo:list-item><fo:list-item-label end-indent="label-end()"><fo:block line-height="16pt">•</fo:block></fo:list-item-label><fo:list-item-body start-indent="body-start()">
        <fo:block line-height="16pt" space-after="9pt" text-align="justify">
          You believe the Cloud may be ideal for burst processing of
          your compute load. For example, you may need to run 100s of
          cores for just 30 minutes in a day for a specific
          risk-calculation workload.
        </fo:block>
      </fo:list-item-body></fo:list-item><fo:list-item><fo:list-item-label end-indent="label-end()"><fo:block line-height="16pt">•</fo:block></fo:list-item-label><fo:list-item-body start-indent="body-start()">
        <fo:block line-height="16pt" space-after="9pt" text-align="justify">
          Your quants and developers might want to work on kdb+, but
          only for a few hours in the day during the work week, a
          suitable model for an on-demand or a spot-pricing service.
        </fo:block>
      </fo:list-item-body></fo:list-item><fo:list-item><fo:list-item-label end-indent="label-end()"><fo:block line-height="16pt">•</fo:block></fo:list-item-label><fo:list-item-body start-indent="body-start()">
        <fo:block line-height="16pt" space-after="9pt" text-align="justify">
          You want to drive warm backups of data from in-house to EC2,
          or across instances/regions in EC2 – spun up for backups, then
          shut down.
        </fo:block>
      </fo:list-item-body></fo:list-item><fo:list-item><fo:list-item-label end-indent="label-end()"><fo:block line-height="16pt">•</fo:block></fo:list-item-label><fo:list-item-body start-indent="body-start()">
        <fo:block line-height="16pt" space-after="9pt" text-align="justify">
          Development/UAT/Prod life-cycles can be hosted on their own
          instances and then spun down after each phase finishes. Small
          memory/core instances can cost less and can be increased or
          decreased on demand.
        </fo:block>
      </fo:list-item-body></fo:list-item></fo:list-block>
    <fo:block line-height="16pt" space-after="9pt" text-align="justify">
      Hosting both the compute workload and the historical market data
      on EC2 can achieve the best of both worlds:
    </fo:block>
    <fo:list-block provisional-distance-between-starts="0.3cm" provisional-label-separation="0.15cm"><fo:list-item><fo:list-item-label end-indent="label-end()"><fo:block line-height="16pt">•</fo:block></fo:list-item-label><fo:list-item-body start-indent="body-start()">
        <fo:block line-height="16pt" space-after="9pt" text-align="justify">
          reduce overall costs for hosting the market data pool
        </fo:block>
      </fo:list-item-body></fo:list-item><fo:list-item><fo:list-item-label end-indent="label-end()"><fo:block line-height="16pt">•</fo:block></fo:list-item-label><fo:list-item-body start-indent="body-start()">
        <fo:block line-height="16pt" space-after="9pt" text-align="justify">
          flex to the desired performance levels
        </fo:block>
      </fo:list-item-body></fo:list-item></fo:list-block>
    <fo:block line-height="16pt" space-after="9pt" text-align="justify">
      As long as the speed of deployment and ease of use is coupled with
      similar or <fo:inline font-style="italic">good enough</fo:inline> runtime performance,
      EC2 can be a serious contender for hosting your market data.
    </fo:block>
  </fo:block>
  
</fo:block><fo:block id="in-house-vs-ec2">
  <fo:block id="idm220867377808" font-size="18pt" line-height="22pt" margin-right="36pt" page-break-before="always" space-after="60pt" text-align="left">In-house vs EC2</fo:block>
  <fo:block line-height="16pt" space-after="9pt" text-align="justify">
    Kdb+ is used to support
  </fo:block>
  <fo:list-block provisional-distance-between-starts="0.3cm" provisional-label-separation="0.15cm"><fo:list-item><fo:list-item-label end-indent="label-end()"><fo:block line-height="16pt">•</fo:block></fo:list-item-label><fo:list-item-body start-indent="body-start()">
      <fo:block line-height="16pt" space-after="9pt" text-align="justify">
        real-time data analytics
      </fo:block>
    </fo:list-item-body></fo:list-item><fo:list-item><fo:list-item-label end-indent="label-end()"><fo:block line-height="16pt">•</fo:block></fo:list-item-label><fo:list-item-body start-indent="body-start()">
      <fo:block line-height="16pt" space-after="9pt" text-align="justify">
        streaming data analytics
      </fo:block>
    </fo:list-item-body></fo:list-item><fo:list-item><fo:list-item-label end-indent="label-end()"><fo:block line-height="16pt">•</fo:block></fo:list-item-label><fo:list-item-body start-indent="body-start()">
      <fo:block line-height="16pt" space-after="9pt" text-align="justify">
        historical data analytics
      </fo:block>
    </fo:list-item-body></fo:list-item></fo:list-block>
  <fo:block line-height="16pt" space-after="9pt" text-align="justify">
    The historical database in a kdb+ solution is typically kept on a
    non-volatile persistent storage medium (a.k.a.
    <fo:inline font-style="italic">disks</fo:inline>). In financial services this data is kept
    for research (quant analytics or back-testing), algorithmic trading
    and for regulatory and compliance requirements.
  </fo:block>
  <fo:block background-color="#FFE" font-size="10.5pt" line-height="13pt" margin-bottom="9pt" margin-left="0mm" margin-right="0mm" page-break-inside="avoid" padding-left="3mm" padding-right="3mm" padding-top="3mm"><fo:block margin-bottom="6pt"><fo:inline vertical-align="sub"><fo:inline font-family="Material Icons" font-size="14pt"></fo:inline></fo:inline> <fo:inline font-weight="bold">Low latency and the Cloud </fo:inline></fo:block><fo:block line-height="16pt" space-after="9pt" text-align="justify">
      In the current state of cloud infrastructure, Kx does not
      recommend keeping the high-performance, low-latency part of market
      data – or streaming data collection – applications in the Cloud.
    </fo:block><fo:block line-height="16pt" space-after="9pt" text-align="justify">
      When speed translates to competitive advantage, using AWS (or
      cloud in general) needs to be considered carefully.
    </fo:block></fo:block>
  <fo:block line-height="16pt" space-after="9pt" text-align="justify">
    Carefully-architected cloud solutions are acceptable for parts of
    the application that are removed from from the cutting-edge
    performance and data-capture requirements often imposed on kdb+. For
    example, using parallel transfers with a proven simple technology
    such as <fo:inline font-family="Pragmata Pro">rsync</fo:inline>, that can take advantage of the
    kdb+ data structures (distinct columns that can safely be
    transferred in parallel) and the innate compressibility of some of
    the data types to transfer data to historical storage in a cloud
    environment at end of day.
  </fo:block>
  <fo:block line-height="16pt" space-after="9pt" text-align="justify">
    Storage and management of historical data can be a non-trivial
    undertaking for many organizations:
  </fo:block>
  <fo:list-block provisional-distance-between-starts="0.3cm" provisional-label-separation="0.15cm"><fo:list-item><fo:list-item-label end-indent="label-end()"><fo:block line-height="16pt">•</fo:block></fo:list-item-label><fo:list-item-body start-indent="body-start()">
      <fo:block line-height="16pt" space-after="9pt" text-align="justify">
        capital and running costs
      </fo:block>
    </fo:list-item-body></fo:list-item><fo:list-item><fo:list-item-label end-indent="label-end()"><fo:block line-height="16pt">•</fo:block></fo:list-item-label><fo:list-item-body start-indent="body-start()">
      <fo:block line-height="16pt" space-after="9pt" text-align="justify">
        overhead of maintaining security policies
      </fo:block>
    </fo:list-item-body></fo:list-item><fo:list-item><fo:list-item-label end-indent="label-end()"><fo:block line-height="16pt">•</fo:block></fo:list-item-label><fo:list-item-body start-indent="body-start()">
      <fo:block line-height="16pt" space-after="9pt" text-align="justify">
        roles and technologies required
      </fo:block>
    </fo:list-item-body></fo:list-item><fo:list-item><fo:list-item-label end-indent="label-end()"><fo:block line-height="16pt">•</fo:block></fo:list-item-label><fo:list-item-body start-indent="body-start()">
      <fo:block line-height="16pt" space-after="9pt" text-align="justify">
        planning for data growth and disaster recovery
      </fo:block>
    </fo:list-item-body></fo:list-item></fo:list-block>
  <fo:block line-height="16pt" space-after="9pt" text-align="justify">
    AWS uses tried-and-tested infrastructure, which includes excellent
    policies and processes for handling such production issues.
  </fo:block>
  <fo:block line-height="16pt" space-after="9pt" text-align="justify">
    Before we get to the analysis of the storage options, it is
    important to take a quick look at the performance you might expect
    from compute and memory in your EC2 instances.
  </fo:block>
  <fo:block id="cpu-cores">
    <fo:block font-weight="bold" font-size="14pt" line-height="17pt" margin-right="36pt" page-break-after="avoid" space-after="6pt" space-before="18pt" text-align="left">CPU cores</fo:block>
    <fo:block line-height="16pt" space-after="9pt" text-align="justify">
      We assume you require the same number of cores and memory
      quantities as you use on your in-house bare-metal servers. The
      chipset used by the instance of your choice will list the number
      of cores offered by that instance. The definition used by AWS to
      describe cores is vCPUs. It is important to note that with very
      few exceptions, the vCPU represents a hyper-threaded core, not a
      physical core. This is normally run at a ratio of 2 hyper-threaded
      cores to one physical core. There is no easy way to eliminate this
      setting. Some of the very large instances do deploy on two
      sockets. For example, <fo:inline font-family="Pragmata Pro">r4.16xlarge</fo:inline> uses two
      sockets.
    </fo:block>
    <fo:block line-height="16pt" space-after="9pt" text-align="justify">
      If your sizing calculations depend on getting one q process to run
      only on one physical core and not share itself with other q
      processes, or threads, you need to either
    </fo:block>
    <fo:list-block provisional-distance-between-starts="0.3cm" provisional-label-separation="0.15cm"><fo:list-item><fo:list-item-label end-indent="label-end()"><fo:block line-height="16pt">•</fo:block></fo:list-item-label><fo:list-item-body start-indent="body-start()">
        <fo:block line-height="16pt" space-after="9pt" text-align="justify">
          use CPU binding on q execution
        </fo:block>
      </fo:list-item-body></fo:list-item><fo:list-item><fo:list-item-label end-indent="label-end()"><fo:block line-height="16pt">•</fo:block></fo:list-item-label><fo:list-item-body start-indent="body-start()">
        <fo:block line-height="16pt" space-after="9pt" text-align="justify">
          invalidate the execution on even, or odd, core counts
        </fo:block>
      </fo:list-item-body></fo:list-item></fo:list-block>
    <fo:block line-height="16pt" space-after="9pt" text-align="justify">
      Or you can run on instances that have more vCPUs than there will
      be instances running. For the purposes of these benchmarks, we
      have focused our testing on single socket instances, with a limit
      of 16 vCPUs, meaning eight physical cores, thus:
    </fo:block>
    <fo:block font-family="Pragmata Pro" font-size="10pt" margin-bottom="12pt" margin-left="10pt" page-break-inside="avoid" white-space="pre">
[centos@nano-client1 ~]$ lscpu
Architecture: x86_64
CPU op-mode(s): 32-bit, 64-bit
Byte Order: Little Endian
CPU(s): 16
On-line CPU(s) list: 0-15
Thread(s) per core: 2
Core(s) per socket: 8
Socket(s): 1
NUMA node(s): 1
Vendor ID: GenuineIntel
CPU family: 6
Model: 79
Model name: Intel(R) Xeon(R) CPU E5-2686 v4 @ 2.30GHz
</fo:block>
  </fo:block>
  <fo:block id="system-memory">
    <fo:block font-weight="bold" font-size="14pt" line-height="17pt" margin-right="36pt" page-break-after="avoid" space-after="6pt" space-before="18pt" text-align="left">System memory</fo:block>
    <fo:block line-height="16pt" space-after="9pt" text-align="justify">
      Memory sizes vary by the instance chosen.
    </fo:block>
    <fo:block background-color="#FFE" font-size="10.5pt" line-height="13pt" margin-bottom="9pt" margin-left="0mm" margin-right="0mm" page-break-inside="avoid" padding-left="3mm" padding-right="3mm" padding-top="3mm"><fo:block margin-bottom="6pt"><fo:inline vertical-align="sub"><fo:inline font-family="Material Icons" font-size="14pt"></fo:inline></fo:inline> <fo:inline font-weight="bold">Memory lost to hypervisor </fo:inline></fo:block><fo:block line-height="16pt" space-after="9pt" text-align="justify">
        Memory is reduced from the nominal 
		‘power of two’
	
        RAM sizing, as some is set aside for the Xen hypervisor. For
        example, a nominal 128 GB of RAM gets sized to approximately 120
        GB.
      </fo:block><fo:block line-height="16pt" space-after="9pt" text-align="justify">
        Take account of this in your memory sizing exercises.
      </fo:block></fo:block>
  </fo:block>
  <fo:block id="compute-and-memory-performance">
    <fo:block font-weight="bold" font-size="14pt" line-height="17pt" margin-right="36pt" page-break-after="avoid" space-after="6pt" space-before="18pt" text-align="left">Compute and memory performance</fo:block>
    <fo:block line-height="16pt" space-after="9pt" text-align="justify">
      For CPU and memory, the EC2 performance matches that seen on
      physical systems, when correlated to the memory specifications. So
      the default HVM mode of an AMI under Xen seems to work efficiently
      when compared to a native/physical server.
    </fo:block>
    <fo:block line-height="16pt" space-after="9pt" text-align="justify">
      There is one caveat to this, in testing kdb+ list creation speeds
      we observe a degradation of memory list creation times when the
      number of q processes running exceeds the number of vCPUs in the
      virtual machine. This is because the vCPU in EC2 is actually a
      single hyperthreaded core, and not a physical core. In this
      example, we see competition on the physical cores. For a 16 vCPU
      instance we notice this only when running above 8 q processes:
    </fo:block>
    <fo:block line-height="16pt" space-after="9pt" text-align="justify">
      
        <fo:external-graphic content-width="scale-down-to-fit" src="url(img/media/image4.png)" width="100%"/>
      
    </fo:block>
    <fo:block background-color="#EEE" font-size="10.5pt" line-height="13pt" margin-bottom="9pt" margin-left="0mm" margin-right="0mm" page-break-inside="avoid" padding-left="3mm" padding-right="3mm" padding-top="3mm"><fo:block margin-bottom="6pt"><fo:inline vertical-align="sub"><fo:inline font-family="Material Icons" font-size="14pt"></fo:inline></fo:inline> <fo:inline font-weight="bold">Megabytes and mebibytes </fo:inline></fo:block><fo:block line-height="16pt" space-after="9pt" text-align="justify">
        Throughout this paper, MB and GB are used to refer to
        <fo:basic-link external-destination="url('https://en.wikipedia.org/wiki/Mebibyte')"><fo:inline color="#0070cd">MiBytes</fo:inline></fo:basic-link><fo:footnote><fo:inline font-size="8pt" alignment-baseline="hanging">4</fo:inline><fo:footnote-body><fo:block font-size="8pt">4.
									https://en.wikipedia.org/wiki/Mebibyte</fo:block></fo:footnote-body></fo:footnote>
        and GiBytes respectively.
      </fo:block></fo:block>
  </fo:block>
  <fo:block id="network-and-storage-performance">
    <fo:block font-weight="bold" font-size="14pt" line-height="17pt" margin-right="36pt" page-break-after="avoid" space-after="6pt" space-before="18pt" text-align="left">Network and storage performance</fo:block>
    <fo:block line-height="16pt" space-after="9pt" text-align="justify">
      As expected, we see more noticeable performance variations with
      the aspects of the system that are virtualized and shared in EC2,
      especially those which in principle are shared amongst others on
      the platform. For kdb+ users, the storage (I/O) and the networking
      access are virtualized/shared, being separated from the bare metal
      by the Xen hypervisor. Most of the AMIs deployed into EC2 today
      are based on the Hardware Virtual Machine layer (HVM). It seems
      that in recent instantiations of HVM, the performance for I/O
      aspects of the guest have improved. For the best performance, AWS
      recommends current-generation instance types and HVM AMIs when you
      launch your instances. Any storage solution that hosts historical
      market data must:
    </fo:block>
    <fo:list-block provisional-distance-between-starts="0.3cm" provisional-label-separation="0.15cm"><fo:list-item><fo:list-item-label end-indent="label-end()"><fo:block line-height="16pt">•</fo:block></fo:list-item-label><fo:list-item-body start-indent="body-start()">
        <fo:block line-height="16pt" space-after="9pt" text-align="justify">
          support the Linux-hosted
          <fo:basic-link external-destination="url('https://en.wikipedia.org/wiki/POSIX')"><fo:inline color="#0070cd">POSIX
          file system</fo:inline></fo:basic-link><fo:footnote><fo:inline font-size="8pt" alignment-baseline="hanging">5</fo:inline><fo:footnote-body><fo:block font-size="8pt">5.
									https://en.wikipedia.org/wiki/POSIX</fo:block></fo:footnote-body></fo:footnote> interfaces
        </fo:block>
      </fo:list-item-body></fo:list-item><fo:list-item><fo:list-item-label end-indent="label-end()"><fo:block line-height="16pt">•</fo:block></fo:list-item-label><fo:list-item-body start-indent="body-start()">
        <fo:block line-height="16pt" space-after="9pt" text-align="justify">
          offer suitable performance for streaming and random I/O mapped
          read rates
        </fo:block>
      </fo:list-item-body></fo:list-item><fo:list-item><fo:list-item-label end-indent="label-end()"><fo:block line-height="16pt">•</fo:block></fo:list-item-label><fo:list-item-body start-indent="body-start()">
        <fo:block line-height="16pt" space-after="9pt" text-align="justify">
          offer acceptable performance for random-region reads of a
          table (splayed) columns, constituting large record reads from
          random regions of the file
        </fo:block>
      </fo:list-item-body></fo:list-item></fo:list-block>
    <fo:block line-height="16pt" space-after="9pt" text-align="justify">
      These aspects, and inspection of metadata performance, are
      summarized in the tests. The term <fo:inline font-style="italic">metadata</fo:inline> is
      used to refer to file operations such as listing files in a
      directory, gathering file size of a file, appending, finding
      modification dates, and so on.
    </fo:block>
    <fo:block background-color="#FFE" font-size="10.5pt" line-height="13pt" margin-bottom="9pt" margin-left="0mm" margin-right="0mm" page-break-inside="avoid" padding-left="3mm" padding-right="3mm" padding-top="3mm"><fo:block margin-bottom="6pt"><fo:inline vertical-align="sub"><fo:inline font-family="Material Icons" font-size="14pt"></fo:inline></fo:inline> <fo:inline font-weight="bold">Using Amazon S3 as a data store </fo:inline></fo:block><fo:block line-height="16pt" space-after="9pt" text-align="justify">
        Because kdb+ does not directly support the use of an object
        store for its stored data, it cannot support direct use of an
        object-store model such as the Amazon S3. If you wish to use
        Amazon S3 as a data store, kdb+ historical data must be hosted
        on a POSIX-based file system layer fronting S3.
      </fo:block><fo:block line-height="16pt" space-after="9pt" text-align="justify">
        Several solutions offer a POSIX interface layered over an
        underlying S3 storage bucket. These can be included alongside
        native file-system support that can also be hosted on EC2.
      </fo:block></fo:block>
    <fo:block line-height="16pt" space-after="9pt" text-align="justify">
      Although EC2 offers both physical systems and virtual systems
      within the Elastic Cloud, it is most likely customers will opt for
      a virtualized environment. There is also a choice in EC2 between
      spot pricing of an EC2, and deployed virtual instances. We focus
      here on the attribute and results achieved with the deployed
      virtual instance model. These are represented by instances that
      are tested in one availability zone and one placement group.
    </fo:block>
    <fo:block line-height="16pt" space-after="9pt" text-align="justify">
      A <fo:inline font-style="italic">placement group</fo:inline> is a logical grouping of
      instances within a single availability zone. Nodes in a placement
      group should gain better network latency figures when compared to
      nodes scattered anywhere within an availability zone. Think of
      this as placement subnets or racks with a data center, as opposed
      to the datacenter itself. All of our tests use one placement
      group, unless otherwise stated.
    </fo:block>
    <fo:block line-height="16pt" space-after="9pt" text-align="justify">
      Kdb+ is supported on most mainstream Linux distributions, and by
      extension we support standard Linux distributions deployed under
      the AWS model.
    </fo:block>
    <fo:block line-height="16pt" space-after="9pt" text-align="justify">
      Testing within this report was carried out typically on CentOS 7.3
      or 7.4 distributions, but all other mainstream Linux distributions
      are expected to work equally well, with no noticeable performance
      differences seen in spot testing on RHEL, Ubuntu and SuSe running
      on EC2.
    </fo:block>
  </fo:block>
  <fo:block id="does-kdb-work-in-the-same-way-under-ec2">
    <fo:block font-weight="bold" font-size="14pt" line-height="17pt" margin-right="36pt" page-break-after="avoid" space-after="6pt" space-before="18pt" text-align="left">Does kdb+ work in the same way under EC2?</fo:block>
    <fo:block line-height="16pt" space-after="9pt" text-align="justify">
      Yes – mostly.
    </fo:block>
    <fo:block line-height="16pt" space-after="9pt" text-align="justify">
      When porting or hosting the HDB data to EC2, we expect our
      customers to:
    </fo:block>
    <fo:list-block provisional-distance-between-starts=".7cm" provisional-label-separation="0.3cm"><fo:list-item><fo:list-item-label end-indent="label-end()"><fo:block line-height="16pt">1.</fo:block></fo:list-item-label><fo:list-item-body start-indent="body-start()">
        <fo:block line-height="16pt" space-after="9pt" text-align="justify">
          Use one of the many POSIX-based file systems solutions
          available under EC2.
        </fo:block>
      </fo:list-item-body></fo:list-item><fo:list-item><fo:list-item-label end-indent="label-end()"><fo:block line-height="16pt">2.</fo:block></fo:list-item-label><fo:list-item-body start-indent="body-start()">
        <fo:block line-height="16pt" space-after="9pt" text-align="justify">
          Use (partly or fully) the lower-cost object storage via a
          POSIX or POSIX-like access method.
        </fo:block>
      </fo:list-item-body></fo:list-item><fo:list-item><fo:list-item-label end-indent="label-end()"><fo:block line-height="16pt">3.</fo:block></fo:list-item-label><fo:list-item-body start-indent="body-start()">
        <fo:block line-height="16pt" space-after="9pt" text-align="justify">
          Not store the historical data on Hadoop HDFS file systems.
        </fo:block>
      </fo:list-item-body></fo:list-item></fo:list-block>
    <fo:block line-height="16pt" space-after="9pt" text-align="justify">
      If kdb+ runs alongside one of the solutions reviewed here, your
      HDB will function identically to any internally-hosted, bare-metal
      system. You can use this report as input to determine the
      performance and the relative costs for an HDB solution on EC2.
    </fo:block>
  </fo:block>
</fo:block><fo:block id="historical-data-layouts-and-performance-testing">
  <fo:block id="idm220867325296" font-size="18pt" line-height="22pt" margin-right="36pt" page-break-before="always" space-after="60pt" text-align="left">Historical data layouts and performance testing</fo:block>
  <fo:block line-height="16pt" space-after="9pt" text-align="justify">
    The typical kdb+ database layout for a stock tick-based system is
    partitioned by date, although integer partitioning is also possible.
    Partitioning allows for quicker lookup and increases the ability to
    parallelize queries. Kdb+ splays in-memory table spaces into
    representative directories and files for long-term retention. Here
    is an example of an on-disk layout for quote and trade tables, with
    date partitions:
  </fo:block>
  <fo:block page-break-after="avoid" text-align="center"><fo:external-graphic content-width="scale-down-to-fit" src="url(img/media/image5.jpg)" width="100%"/></fo:block><fo:block font-size="9pt" font-style="italic" margin-bottom="15pt">On-disk layout for quote and trade tables with
      date partitions</fo:block>
  <fo:block line-height="16pt" space-after="9pt" text-align="justify">
    Usually, updates to the HDB are made by writing today’s or the last
    day’s in-memory columns of data to a new HDB partition. Q
    programmers can use a utility built into q for this which creates
    the files and directories organized as in the table above. Kdb+
    requires the support of a POSIX-compliant file system in order to
    access and process HDB data.
  </fo:block>
  <fo:block line-height="16pt" space-after="9pt" text-align="justify">
    Kdb+ maps the entire HDB into the runtime address space of kdb+.
    This means the Linux kernel is responsible for fetching HDB data.
    If, for example, you are expecting a query that scans an entire
    day’s trade price for a specific stock symbol range, the file system
    will load this data into the host memory as required. So, for
    porting this to EC2, if you expect it to match the performance you
    see on your in-house infrastructure you will need to look into the
    timing differences between this and EC2.
  </fo:block>
  <fo:block line-height="16pt" space-after="9pt" text-align="justify">
    Our testing measured the time to load and unload data from arrays,
    ignoring the details of structuring columns, partitions and segments
    – we focused on just the raw throughput measurements.
  </fo:block>
  <fo:block line-height="16pt" space-after="9pt" text-align="justify">
    All of these measurements will directly correlate to the final
    operational latencies for your full analytics use-case, written in
    q. In other words, if a solution reported here shows throughput of
    100 MB/sec for solution A, and shows 200 MB/sec for solution B, this
    will reflect the difference in time to complete the data fetch from
    backing store. Of course, as with any solution, you get what you pay
    for, but the interesting question is: how much more could you get
    within the constraints of one solution?
  </fo:block>
  <fo:block line-height="16pt" space-after="9pt" text-align="justify">
    To give an example: assuming a retrieval on solution A takes 50 ms
    for a query comprised of 10 ms to compute against the data, and 40
    ms to fetch the data, with half the throughput rates, it might take
    90 ms (10+80) to complete on solution B. Variations may be seen
    depending on metadata and random read values.
  </fo:block>
  <fo:block line-height="16pt" space-after="9pt" text-align="justify">
    This is especially important for solutions that use networked file
    systems to access a single namespace that contains your HDB. This
    may well exhibit a significantly different behavior when run at
    scale.
  </fo:block>
</fo:block><fo:block id="data-locality">
  <fo:block id="idm220867316400" font-size="18pt" line-height="22pt" margin-right="36pt" page-break-before="always" space-after="60pt" text-align="left">Data locality</fo:block>
  <fo:block line-height="16pt" space-after="9pt" text-align="justify">
    Data locality is the basic architectural decision.
  </fo:block>
  <fo:block line-height="16pt" space-after="9pt" text-align="justify">
    You will get the best storage performance in EC2 by localizing the
    data to be as close to the compute workload as is possible.
  </fo:block>
  <fo:block line-height="16pt" space-after="9pt" text-align="justify">
    EC2 is divided into various zones. Compute, storage and support
    software can all be placed in pre-defined availability zones.
    Typically these reflect the timezone location of the data center, as
    well as a further subdivision into a physical instance of the data
    center within one region or time zone. Kdb+ will achieve the lowest
    latency and highest bandwidth in the network by using nodes and
    storage hosted in the same availability zone.
  </fo:block>
</fo:block><fo:block id="getting-your-data-into-ec2">
  <fo:block id="idm220867313456" font-size="18pt" line-height="22pt" margin-right="36pt" page-break-before="always" space-after="60pt" text-align="left">Getting your data into EC2</fo:block>
  <fo:block line-height="16pt" space-after="9pt" text-align="justify">
    Let’s suppose you already have a lot of data for your historical
    database (HDB). You will need to know the achievable bandwidth for
    data loading, and note that you will be charged by the amount of
    data ingested. The mechanics of loading a large data set from your
    data center which hosts the HDB into EC2 involves the use of at
    least one of the two methods described below.
  </fo:block>
  <fo:block id="ec2-virtual-private-cloud">
    <fo:block font-weight="bold" font-size="14pt" line-height="17pt" margin-right="36pt" page-break-after="avoid" space-after="6pt" space-before="18pt" text-align="left">EC2 Virtual Private Cloud</fo:block>
    <fo:block line-height="16pt" space-after="9pt" text-align="justify">
      We would expect kdb+ customers to use the EC2 Virtual Private
      Cloud (VPC) network structure. Within the VPC you can use either
      an anonymous IP address, using EC2 DHCP address ranges, or a
      permanently-allocated IP address range. The anonymous DHCP IP
      address range is free of charge. Typically you would deploy both
      the front and backend domains (subnets) within the same VPC,
      provisioned and associated with each new instance in EC2.
      Typically, an entire VPC allocates an entire class-C subnet. You
      may provision up to 200 class-C subnets in EC2, as one account.
      Public IP addresses are reachable from the internet and are either
      dynamically allocated on start, or use the same pre-defined
      elastic IP address on each start of the instance.
    </fo:block>
    <fo:block line-height="16pt" space-after="9pt" text-align="justify">
      Private IP addresses refer to the locally defined IP addresses
      only visible to your cluster (e.g. the front/backend in diagram
      below). Private IP addresses are retained by that instance until
      the instance is terminated. Public access may be direct to either
      of these domains, or you may prefer to set up a classic
      
						‘
		‘demilitarized
      zone’
	’
					 for kdb+ access.
    </fo:block>
    <fo:block line-height="16pt" space-after="9pt" text-align="justify">
      An elastic IP address is usually your public IPv4 address, known
      to your quants/users/applications, and is reachable from the
      Internet and registered permanently in DNS, until you terminate
      the instance or elastic IP. AWS has added support for IPv6 in most
      of their regions. An elastic IP address can mask the failure of an
      instance or software by remapping the address to another instance
      in your estate. That is handy for things such as GUIs and
      dashboards, though you should be aware of this capability and use
      it. You are charged for the elastic IP address if you close down
      the instance associated with it, otherwise one IP address is free
      when associated. As of January 2018 the cost is, $0.12 per Elastic
      IP address/day when not associated with a running instance.
      Additional IP addresses per instance are charged.
    </fo:block>
    <fo:block line-height="16pt" space-after="9pt" text-align="justify">
      Ingesting data can be via the public/elastic IP address. In this
      case, routing to that connection is via undefined routers. The
      ingest rate to this instance using this elastic IP address would
      depend on the availability zone chosen. But in all cases, this
      would be a shared pubic routed IP model, so transfer rates may be
      outside your control.
    </fo:block>
    <fo:block line-height="16pt" space-after="9pt" text-align="justify">
      In theory this uses publicly routed connections, so you may wish
      to consider encryption of the data over the wire, prior to
      decryption.
    </fo:block>
  </fo:block>
  <fo:block id="direct-connect">
    <fo:block font-weight="bold" font-size="14pt" line-height="17pt" margin-right="36pt" page-break-after="avoid" space-after="6pt" space-before="18pt" text-align="left">Direct Connect</fo:block>
    <fo:block line-height="16pt" space-after="9pt" text-align="justify">
      Direct Connect is a dedicated network connection between an access
      point to your existing IP network and one of the AWS Direct
      Connect locations. This is a dedicated physical connection offered
      as a VLAN, using industry standard 802.1q VLAN protocol. You can
      use AWS Direct Connect instead of establishing your own VPN
      connection over the internet to VPC. Specifically, it can connect
      through to a VPC domain using a private IP space. It also gives a
      dedicated service level for bandwidth. There is an additional
      charge for this service.
    </fo:block>
  </fo:block>
</fo:block><fo:block id="security-of-your-data-and-secure-access">
  <fo:block id="idm220867302336" font-size="18pt" line-height="22pt" margin-right="36pt" page-break-before="always" space-after="60pt" text-align="left">Security of your data and secure access</fo:block>
  <fo:block line-height="16pt" space-after="9pt" text-align="justify">
    The EC2 application machine image model (AMI) has tight security
    models in place. You would have to work very hard to remove these.
  </fo:block>
  <fo:block line-height="16pt" space-after="9pt" text-align="justify">
    The following diagram is a typical scenario for authenticating
    access to kdb+ and restricting networking access. The frontend and
    backend private subnets are provisioned by default with one Virtual
    Private Cloud (VPC) managed by EC2. Typically, this allocates an
    entire class-C subnet. You may provision up to 200 class-C subnets
    in EC2. The public access may be direct to either of these domains,
    or you may prefer to setup a classic 
		‘demilitarized
    zone’
	:
  </fo:block>
  <fo:block page-break-after="avoid" text-align="center"><fo:external-graphic content-width="scale-down-to-fit" src="url(img/media/image6.png)" width="100%"/></fo:block><fo:block font-size="9pt" font-style="italic" margin-bottom="15pt">Typical scenario for authenticating
      access</fo:block>
  <fo:block line-height="16pt" space-after="9pt" text-align="justify">
    Amazon has spent a lot of time developing
    <fo:basic-link external-destination="url('https://aws.amazon.com/security/')"><fo:inline color="#0070cd">security
    features for EC2</fo:inline></fo:basic-link><fo:footnote><fo:inline font-size="8pt" alignment-baseline="hanging">7</fo:inline><fo:footnote-body><fo:block font-size="8pt">7.
									https://aws.amazon.com/security/</fo:block></fo:footnote-body></fo:footnote>. Key issues:
  </fo:block>
  <fo:list-block provisional-distance-between-starts="0.3cm" provisional-label-separation="0.15cm"><fo:list-item><fo:list-item-label end-indent="label-end()"><fo:block line-height="16pt">•</fo:block></fo:list-item-label><fo:list-item-body start-indent="body-start()">
      <fo:block line-height="16pt" space-after="9pt" text-align="justify">
        A newly-provisioned node comes from a trusted build image, for
        example, one found in the AWS Marketplace.
      </fo:block>
    </fo:list-item-body></fo:list-item><fo:list-item><fo:list-item-label end-indent="label-end()"><fo:block line-height="16pt">•</fo:block></fo:list-item-label><fo:list-item-body start-indent="body-start()">
      <fo:block line-height="16pt" space-after="9pt" text-align="justify">
        The Amazon Linux AMI Security Center provides patch and fix
        lists, and these can be automatically inlaid by the AMI. The
        Amazon Linux AMI is a supported and maintained Linux image
        provided by AWS for use on EC2.
      </fo:block>
    </fo:list-item-body></fo:list-item><fo:list-item><fo:list-item-label end-indent="label-end()"><fo:block line-height="16pt">•</fo:block></fo:list-item-label><fo:list-item-body start-indent="body-start()">
      <fo:block line-height="16pt" space-after="9pt" text-align="justify">
        Encryption at rest is offered by many of the storage interfaces
        covered in this report.
      </fo:block>
    </fo:list-item-body></fo:list-item></fo:list-block>
  <fo:block line-height="16pt" space-after="9pt" text-align="justify">
    <fo:inline font-family="Material Icons" font-size="14pt"></fo:inline>
    <fo:basic-link external-destination="url('https://aws.amazon.com/blogs/security/')"><fo:inline color="#0070cd">Amazon
    Security</fo:inline></fo:basic-link><fo:footnote><fo:inline font-size="8pt" alignment-baseline="hanging">8</fo:inline><fo:footnote-body><fo:block font-size="8pt">8.
									https://aws.amazon.com/blogs/security/</fo:block></fo:footnote-body></fo:footnote>
  </fo:block>
</fo:block><fo:block id="getting-your-data-out-of-ec2">
  <fo:block id="idm220867290480" font-size="18pt" line-height="22pt" margin-right="36pt" page-break-before="always" space-after="60pt" text-align="left">Getting your data out of EC2</fo:block>
  <fo:block line-height="16pt" space-after="9pt" text-align="justify">
    Storing billions and billions of records under kdb+ in EC2 is easily
    achievable. Pushing the data into EC2 can be easily done and in
    doing so incurs no data transfer charges from AWS. But AWS will
    charge you to extract this information from EC2. For example,
    network charges may apply if you wish to extract data to place into
    other visualization tools/GUIs, outside the domain of kdb+ toolsets.
  </fo:block>
  <fo:block id="replication">
    <fo:block font-weight="bold" font-size="14pt" line-height="17pt" margin-right="36pt" page-break-after="avoid" space-after="6pt" space-before="18pt" text-align="left">Replication</fo:block>
    <fo:block line-height="16pt" space-after="9pt" text-align="justify">
      Or you may be replicating data from one region or availability
      zone, to another. For this, there is a cost involved. At time of
      writing, the charges are $.09/GB ($92/TB), or $94,200 for 1 PB
      transferred out to the Internet via EC2 public IP addresses. That
      is raw throughput measurements, not the raw GBs of kdb+ columnar
      data itself. This is billed by AWS at a pro-rated monthly rate.
      The rate declines as the amount of data transferred increases.
      This rate also applies for all general traffic over a VPN to your
      own data center. Note that normal Internet connections carry no
      specific service-level agreements for bandwidth.
    </fo:block>
  </fo:block>
  <fo:block id="network-direct">
    <fo:block font-weight="bold" font-size="14pt" line-height="17pt" margin-right="36pt" page-break-after="avoid" space-after="6pt" space-before="18pt" text-align="left">Network Direct</fo:block>
    <fo:block line-height="16pt" space-after="9pt" text-align="justify">
      If you use the Network Direct option from EC2, you get a dedicated
      network with guaranteed bandwidth. You then pay for the dedicated
      link, plus the same outbound data transfer rates. For example, as
      of January 2018 the standard charge for a dedicated 1 GB/sec link
      to EC2 would cost $220/month plus $90/month for a transfer fee per
      TB.
    </fo:block>
    <fo:block line-height="16pt" space-after="9pt" text-align="justify">
      Consider these costs when planning to replicate HDB data between
      regions, and when exporting your data continually back to your own
      data center for visualization or other purposes. Consider the
      migration of these tools to coexist with kdb+ in the AWS estate,
      and if you do not, consider the time to export the data.
    </fo:block>
  </fo:block>
</fo:block><fo:block id="storing-your-hdb-in-s3">
  <fo:block id="idm220867284144" font-size="18pt" line-height="22pt" margin-right="36pt" page-break-before="always" space-after="60pt" text-align="left">Storing your HDB in S3</fo:block>
  <fo:block line-height="16pt" space-after="9pt" text-align="justify">
    S3 might be something you are seriously considering for storage of
    some, or all, of your HDB data in EC2. Here is how S3 fits into the
    landscape of all of the storage options in EC2.
  </fo:block>
  <fo:block id="locally-attached-drives">
    <fo:block font-weight="bold" font-size="14pt" line-height="17pt" margin-right="36pt" page-break-after="avoid" space-after="6pt" space-before="18pt" text-align="left">Locally-attached drives</fo:block>
    <fo:block line-height="16pt" space-after="9pt" text-align="justify">
      You can store your HDB on locally-attached drives, as you might do
      today on your own physical hardware on your own premises.
    </fo:block>
    <fo:block line-height="16pt" space-after="9pt" text-align="justify">
      EC2 offers the capability of bringing up an instance with internal
      NVMe or SAS/SATA disk drives, although this is not expected to be
      used for anything other than caching data, as this storage is
      referred to as ephemeral data by AWS, and might not persist after
      system shutdowns. This is due to the on-demand nature of the
      compute instances: they could be instantiated on any available
      hardware within the availability zone selected by your instance
      configuration.
    </fo:block>
  </fo:block>
  <fo:block id="ebs-volumes">
    <fo:block font-weight="bold" font-size="14pt" line-height="17pt" margin-right="36pt" page-break-after="avoid" space-after="6pt" space-before="18pt" text-align="left">EBS volumes</fo:block>
    <fo:block line-height="16pt" space-after="9pt" text-align="justify">
      You can store your HDB on
      <fo:basic-link external-destination="url('http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/RootDeviceStorage.html')"><fo:inline color="#0070cd">EBS
      volumes</fo:inline></fo:basic-link><fo:footnote><fo:inline font-size="8pt" alignment-baseline="hanging">9</fo:inline><fo:footnote-body><fo:block font-size="8pt">9.
									http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/RootDeviceStorage.html</fo:block></fo:footnote-body></fo:footnote>. These appear like persistent block-level storage.
      Because the EC2 instances are virtualized, the storage is
      separated at birth from all compute instances.
    </fo:block>
    <fo:block line-height="16pt" space-after="9pt" text-align="justify">
      By doing this, it allows you to start instances on demand, without
      the need to co-locate the HDB data alongside those nodes. This
      separation is always via the networking infrastructure built into
      EC2. In other words, your virtualized compute instance can be
      attached to a real physical instance of the storage via the EC2
      network, and thereafter appears as block storage. This is referred
      to as <fo:inline font-style="italic">network attached storage</fo:inline> (Elastic Block
      Storage).
    </fo:block>
    <fo:block line-height="16pt" space-after="9pt" text-align="justify">
      Alternatively, you can place the files on a remote independent
      file system, which in turn is typically supported by EC2 instances
      stored on EBS or S3.
    </fo:block>
  </fo:block>
  <fo:block id="amazon-s3-object-store">
    <fo:block font-weight="bold" font-size="14pt" line-height="17pt" margin-right="36pt" page-break-after="avoid" space-after="6pt" space-before="18pt" text-align="left">Amazon S3 object store</fo:block>
    <fo:block line-height="16pt" space-after="9pt" text-align="justify">
      Finally, there is the ubiquitous Amazon S3 object store, available
      in all regions and zones of EC2. Amazon uses S3 to run its own
      global network of websites, and many high-visibility web-based
      services store their key data under S3. With S3 you can create and
      deploy your HDB data in buckets of S3 objects.
    </fo:block>
    <fo:list-block provisional-distance-between-starts="0.3cm" provisional-label-separation="0.15cm"><fo:list-item><fo:list-item-label end-indent="label-end()"><fo:block line-height="16pt">•</fo:block></fo:list-item-label><fo:list-item-body start-indent="body-start()">
        <fo:block line-height="16pt" space-after="9pt" text-align="justify">
          <fo:inline font-style="italic">Storage prices</fo:inline> are lower
          (as of January 2018): typically 10% of the costs of the Amazon
          EBS model.
        </fo:block>
      </fo:list-item-body></fo:list-item><fo:list-item><fo:list-item-label end-indent="label-end()"><fo:block line-height="16pt">•</fo:block></fo:list-item-label><fo:list-item-body start-indent="body-start()">
        <fo:block line-height="16pt" space-after="9pt" text-align="justify">
          S3 can be configured to offer
          <fo:inline font-style="italic">redundancy and replication</fo:inline>
          of object data, regionally and globally.
        </fo:block>
      </fo:list-item-body></fo:list-item></fo:list-block>
    <fo:block line-height="16pt" space-after="9pt" text-align="justify">
      Amazon can be configured to duplicate your uploaded data across
      multiple geographically diverse repositories, according to the
      replication service selected at bucket-creation time. S3 promises
      <fo:basic-link external-destination="url('https://aws.amazon.com/s3/faqs/')"><fo:inline color="#0070cd">99.999999999%</fo:inline></fo:basic-link><fo:footnote><fo:inline font-size="8pt" alignment-baseline="hanging">10</fo:inline><fo:footnote-body><fo:block font-size="8pt">10.
									https://aws.amazon.com/s3/faqs/</fo:block></fo:footnote-body></fo:footnote>
      durability.
    </fo:block>
    <fo:block line-height="16pt" space-after="9pt" text-align="justify">
      <fo:inline font-family="Material Icons" font-size="14pt"></fo:inline>
      <fo:basic-link external-destination="url('https://docs.aws.amazon.com/AmazonS3/latest/dev/crr.html')"><fo:inline color="#0070cd">AWS
      S3 replication</fo:inline></fo:basic-link><fo:footnote><fo:inline font-size="8pt" alignment-baseline="hanging">11</fo:inline><fo:footnote-body><fo:block font-size="8pt">11.
									https://docs.aws.amazon.com/AmazonS3/latest/dev/crr.html</fo:block></fo:footnote-body></fo:footnote>
    </fo:block>
    <fo:block line-height="16pt" space-after="9pt" text-align="justify">
      However, there are severe limitations on using S3 when it comes to
      kdb+. The main limitation is the API.
    </fo:block>
    <fo:block id="api-limitations">
      <fo:block font-size="12pt" font-weight="bold" line-height="17pt" margin-right="36pt" page-break-after="avoid" space-after="6pt" space-before="18pt" text-align="left">API limitations</fo:block>
      <fo:block line-height="16pt" space-after="9pt" text-align="justify">
        An S3 object store is organized differently from a POSIX file
        system.
      </fo:block>
      <fo:block line-height="16pt" space-after="9pt" text-align="justify">
        
          <fo:external-graphic content-width="scale-down-to-fit" src="url(img/media/image7.png)" width="100%"/>
        
      </fo:block>
      <fo:block line-height="16pt" space-after="9pt" text-align="justify">
        S3 uses a web-style
        <fo:basic-link external-destination="url('https://en.m.wikipedia.org/wiki/Representational_state_transfer')"><fo:inline color="#0070cd">RESTful
        interface</fo:inline></fo:basic-link><fo:footnote><fo:inline font-size="8pt" alignment-baseline="hanging">13</fo:inline><fo:footnote-body><fo:block font-size="8pt">13.
									https://en.m.wikipedia.org/wiki/Representational_state_transfer</fo:block></fo:footnote-body></fo:footnote> HTTP-style interface with
        <fo:basic-link external-destination="url('https://en.wikipedia.org/wiki/Eventual_consistency')"><fo:inline color="#0070cd">eventual-​consistency</fo:inline></fo:basic-link><fo:footnote><fo:inline font-size="8pt" alignment-baseline="hanging">14</fo:inline><fo:footnote-body><fo:block font-size="8pt">14.
									https://en.wikipedia.org/wiki/Eventual_consistency</fo:block></fo:footnote-body></fo:footnote>
        semantics of put and change. This will always represent an
        additional level of abstraction for an application like kdb+
        that directly manages its virtual memory. S3 therefore exhibits
        slower per–process/thread performance than is usual for kdb+.
        The lack of POSIX interface and the semantics of RESTful
        interfaces prevents kdb+ and other high-performance databases
        from using S3 directly.
      </fo:block>
      <fo:block line-height="16pt" space-after="9pt" text-align="justify">
        However, S3’s low cost, and its ability to scale performance
        horizontally when additional kdb+ instances use the same S3
        buckets, make it a candidate for some customers.
      </fo:block>
    </fo:block>
    <fo:block id="performance-limitations">
      <fo:block font-size="12pt" font-weight="bold" line-height="17pt" margin-right="36pt" page-break-after="avoid" space-after="6pt" space-before="18pt" text-align="left">Performance limitations</fo:block>
      <fo:block line-height="16pt" space-after="9pt" text-align="justify">
        The second limitation is S3’s performance, as measured by the
        time taken to populate vectors in memory.
      </fo:block>
      <fo:block line-height="16pt" space-after="9pt" text-align="justify">
        Kdb+ uses POSIX file-system semantics to manage HDB structure
        directly on disk. It exploits this feature to gain very
        high-performance memory management through Linux-based memory
        mapping functions built into the kernel, from the very inception
        of Linux.
      </fo:block>
      <fo:block line-height="16pt" space-after="9pt" text-align="justify">
        S3 uses none of this.
      </fo:block>
      <fo:block line-height="16pt" space-after="9pt" text-align="justify">
        On EC2, kdb+ performance stacks up in this order (from slowest
        to faster):
      </fo:block>
      <fo:list-block provisional-distance-between-starts=".7cm" provisional-label-separation="0.3cm"><fo:list-item><fo:list-item-label end-indent="label-end()"><fo:block line-height="16pt">1.</fo:block></fo:list-item-label><fo:list-item-body start-indent="body-start()">
          <fo:block line-height="16pt" space-after="9pt" text-align="justify">
            S3
          </fo:block>
        </fo:list-item-body></fo:list-item><fo:list-item><fo:list-item-label end-indent="label-end()"><fo:block line-height="16pt">2.</fo:block></fo:list-item-label><fo:list-item-body start-indent="body-start()">
          <fo:block line-height="16pt" space-after="9pt" text-align="justify">
            EBS
          </fo:block>
        </fo:list-item-body></fo:list-item><fo:list-item><fo:list-item-label end-indent="label-end()"><fo:block line-height="16pt">3.</fo:block></fo:list-item-label><fo:list-item-body start-indent="body-start()">
          <fo:block line-height="16pt" space-after="9pt" text-align="justify">
            Third-party distributed or managed file system
          </fo:block>
        </fo:list-item-body></fo:list-item><fo:list-item><fo:list-item-label end-indent="label-end()"><fo:block line-height="16pt">4.</fo:block></fo:list-item-label><fo:list-item-body start-indent="body-start()">
          <fo:block line-height="16pt" space-after="9pt" text-align="justify">
            Local drives to the instance (typically cache only)
          </fo:block>
        </fo:list-item-body></fo:list-item></fo:list-block>
      <fo:block line-height="16pt" space-after="9pt" text-align="justify">
        Although the performance of S3 as measured from one node is not
        fast, S3 retains comparative performance for each new instance
        added to an HDB workload in each availability zone. Because of
        this, S3 can scale up its throughput when used across multiple
        nodes within one availability zone. This is useful if you are
        positioning large numbers of business functions against common
        sets of market data, or if you are widely distributing the
        workload of a single set of business queries. This is not so for
        EBS as, when deployed, the storage becomes owned by one, and
        only one, instance at a time.
      </fo:block>
    </fo:block>
    <fo:block id="replication-limitations">
      <fo:block font-size="12pt" font-weight="bold" line-height="17pt" margin-right="36pt" page-break-after="avoid" space-after="6pt" space-before="18pt" text-align="left">Replication limitations</fo:block>
      <fo:block line-height="16pt" space-after="9pt" text-align="justify">
        A nice feature of S3 is its built-in replication model between
        regions and/or time zones.
      </fo:block>
      <fo:block line-height="16pt" space-after="9pt" text-align="justify">
        Note you have to choose a replication option; none is chosen by
        default.
      </fo:block>
      <fo:block line-height="16pt" space-after="9pt" text-align="justify">
        The replication process may well duplicate incorrect behavior
        from one region to another. In other words, this is not a
        backup.
      </fo:block>
      <fo:block line-height="16pt" space-after="9pt" text-align="justify">
        However, the data at the replica site can be used for production
        purposes, if required. Replication is only for cross-region
        propagation (e.g. US-East to US-West). But, given that the kdb+
        user can design this into the solution (i.e. end-of-day copies
        to replica sites, or multiple pub-sub systems), you may choose
        to deploy a custom solution within kdb+, across region, rather
        than relying on S3 or the file system itself.
      </fo:block>
    </fo:block>
    <fo:block id="summary">
      <fo:block font-size="12pt" font-weight="bold" line-height="17pt" margin-right="36pt" page-break-after="avoid" space-after="6pt" space-before="18pt" text-align="left">Summary</fo:block>
      <fo:list-block provisional-distance-between-starts="0.3cm" provisional-label-separation="0.15cm"><fo:list-item><fo:list-item-label end-indent="label-end()"><fo:block line-height="16pt">•</fo:block></fo:list-item-label><fo:list-item-body start-indent="body-start()">
          <fo:block line-height="16pt" space-after="9pt" text-align="justify">
            The <fo:inline font-style="italic">POSIX file system
            interface</fo:inline> allows the Linux kernel to move data
            from the blocks of the underlying physical hardware,
            directly into memory mapped space of the user process. This
            concept has been tuned and honed by over 20 years of Linux
            kernel refinement. In our case, the recipient user process
            is kdb+. S3, by comparison, requires the application to bind
            to an HTTP-based RESTful (get, wait, receive) protocol,
            which is typically transferred over TCP/IP LAN or WAN
            connection. Clearly, this is not directly suitable for a
            high-performance in-memory analytics engine such as kdb+.
            However, all of the file-system plug-ins and middleware
            packages reviewed in this paper help mitigate this issue.
            The appendices list the main comparisons of all of the
            reviewed solutions.
          </fo:block>
        </fo:list-item-body></fo:list-item><fo:list-item><fo:list-item-label end-indent="label-end()"><fo:block line-height="16pt">•</fo:block></fo:list-item-label><fo:list-item-body start-indent="body-start()">
          <fo:block line-height="16pt" space-after="9pt" text-align="justify">
            Neither Kdb+, nor any other high-performance database, makes
            use of the <fo:inline font-style="italic">RESTful object-store
            interface</fo:inline>.
          </fo:block>
        </fo:list-item-body></fo:list-item><fo:list-item><fo:list-item-label end-indent="label-end()"><fo:block line-height="16pt">•</fo:block></fo:list-item-label><fo:list-item-body start-indent="body-start()">
          <fo:block line-height="16pt" space-after="9pt" text-align="justify">
            There is no notion of <fo:inline font-style="italic">vectors,
            lists, memory mapping</fo:inline> or optimized placement of
            objects in memory regions.
          </fo:block>
        </fo:list-item-body></fo:list-item><fo:list-item><fo:list-item-label end-indent="label-end()"><fo:block line-height="16pt">•</fo:block></fo:list-item-label><fo:list-item-body start-indent="body-start()">
          <fo:block line-height="16pt" space-after="9pt" text-align="justify">
            S3 employs an
            <fo:inline font-style="italic">eventual-consistency</fo:inline>
            model, meaning there is no guaranteed service time for
            placement of the object, or replication of the object, for
            access by other processes or threads.
          </fo:block>
        </fo:list-item-body></fo:list-item><fo:list-item><fo:list-item-label end-indent="label-end()"><fo:block line-height="16pt">•</fo:block></fo:list-item-label><fo:list-item-body start-indent="body-start()">
          <fo:block line-height="16pt" space-after="9pt" text-align="justify">
            S3 exhibits relatively low
            <fo:inline font-style="italic">streaming-read
            performance</fo:inline>. A RESTful, single S3 reader process
            is limited to a
            <fo:basic-link external-destination="url('http://blog.zachbjornson.com/2015/12/29/cloud-storage-performance.html')"><fo:inline color="#0070cd">read
            throughput</fo:inline></fo:basic-link><fo:footnote><fo:inline font-size="8pt" alignment-baseline="hanging">15</fo:inline><fo:footnote-body><fo:block font-size="8pt">15.
									http://blog.zachbjornson.com/2015/12/29/cloud-storage-performance.html</fo:block></fo:footnote-body></fo:footnote> of circa 0.07 GB/sec. Some of the
            solutions reviewed in this paper use strategies to improve
            these numbers within one instance (e.g. raising that figure
            to the 100s MB/sec – GB/sec range). There is also throughput
            scalability gained by reading the same bucket across
            multiple nodes. There is no theoretical limit on this
            bandwidth, but this has not been exhaustively tested by Kx.
          </fo:block>
        </fo:list-item-body></fo:list-item><fo:list-item><fo:list-item-label end-indent="label-end()"><fo:block line-height="16pt">•</fo:block></fo:list-item-label><fo:list-item-body start-indent="body-start()">
          <fo:block line-height="16pt" space-after="9pt" text-align="justify">
            Certain <fo:inline font-style="italic">metadata
            operations</fo:inline>, such as kdb+’s append function, cause
            significant latency vs that observed on EBS or local
            attached storage, and your mileage depends on the file
            system under review.
          </fo:block>
        </fo:list-item-body></fo:list-item></fo:list-block>
      <fo:block line-height="16pt" space-after="9pt" text-align="justify">
        Performance enhancements, some of which are bundled into
        <fo:inline font-style="italic">third-party solutions</fo:inline> that
        layer between S3 and the POSIX file system layer, are based
        around a combination of: multithreading read requests to the S3
        bucket; separation of large sequential regions of a file into
        individual objects within the bucket and read-ahead and caching
        strategies.
      </fo:block>
      <fo:block line-height="16pt" space-after="9pt" text-align="justify">
        There are some areas of synergy. Kdb+ HDB data typically stores
        billions and billions of time-series entries in an immutable
        read-only mode. Only updated new data that lands in the HDB
        needs to be written. S3 is a
        <fo:basic-link external-destination="url('https://en.wikipedia.org/wiki/Shared-nothing_architecture')"><fo:inline color="#0070cd">shared
        nothing</fo:inline></fo:basic-link><fo:footnote><fo:inline font-size="8pt" alignment-baseline="hanging">16</fo:inline><fo:footnote-body><fo:block font-size="8pt">16.
									https://en.wikipedia.org/wiki/Shared-nothing_architecture</fo:block></fo:footnote-body></fo:footnote> model. Therefore, splitting a single segment or
        partitioned column of data into one file, which in turn is
        segmented into a few objects of say 1 MB, should be a
        lightweight operation, as there is no shared/locking required
        for previously written HDB data. So the HDB can easily tolerate
        this eventual consistency model. This does not apply to all
        use-cases for kdb+. For example, S3, with or without a file
        system layer, cannot be used to store a reliable ticker-plant
        log.
      </fo:block>
      <fo:block line-height="16pt" space-after="9pt" text-align="justify">
        Where S3 definitely plays to its strengths, is that it can be
        considered for an <fo:inline font-style="italic">off-line deep
        archive</fo:inline> of your kdb+ formatted market data.
      </fo:block>
      <fo:block line-height="16pt" space-after="9pt" text-align="justify">
        Kx does not make recommendations with respect to the merits, or
        otherwise, of storing kdb+ HDB market data in a data retention
        type 
		‘WORM’
	 model, as required by the regulations
        <fo:basic-link external-destination="url('https://en.wikipedia.org/wiki/SEC_Rule_17a-4')"><fo:inline color="#0070cd">SEC
        17-a4</fo:inline></fo:basic-link><fo:footnote><fo:inline font-size="8pt" alignment-baseline="hanging">17</fo:inline><fo:footnote-body><fo:block font-size="8pt">17.
									https://en.wikipedia.org/wiki/SEC_Rule_17a-4</fo:block></fo:footnote-body></fo:footnote>.
      </fo:block>
    </fo:block>
  </fo:block>
</fo:block><fo:block id="disaster-recovery">
  <fo:block id="idm220867226576" font-size="18pt" line-height="22pt" margin-right="36pt" page-break-before="always" space-after="60pt" text-align="left">Disaster recovery</fo:block>
  <fo:block line-height="16pt" space-after="9pt" text-align="justify">
    In addition to EC2’s built-in disaster-recovery features, when you
    use kdb+ on EC2, your disaster recovery process is eased by kdb+’s
    simple, elegant design.
  </fo:block>
  <fo:block line-height="16pt" space-after="9pt" text-align="justify">
    Kdb+ databases are stored as a series of files and directories on
    disk. This makes administering databases extremely easy because
    database files can be manipulated as operating-system files. Backing
    up a kdb+ database can be implemented using any standard file-system
    backup utility. This is a key difference from traditional databases,
    which have to have their own cumbersome backup utilities and do not
    allow direct access to the database files and structure.
  </fo:block>
  <fo:block line-height="16pt" space-after="9pt" text-align="justify">
    Kdb+’s use of the native file system is also reflected in the way it
    uses standard operating-system features for accessing data
    (memory-mapped files), whereas traditional databases use proprietary
    techniques in an effort to speed up the reading and writing
    processes. The typical kdb+ database layout for time-series data is
    to partition by date.
  </fo:block>
</fo:block><fo:block id="licensing-kdb-in-the-cloud">
  <fo:block id="idm220867222800" font-size="18pt" line-height="22pt" margin-right="36pt" page-break-before="always" space-after="60pt" text-align="left">Licensing kdb+ in the Cloud</fo:block>
  <fo:block line-height="16pt" space-after="9pt" text-align="justify">
    Existing kdb+ users have a couple of options for supporting their
    kdb+ licenses in the Cloud:
  </fo:block>
  <fo:block id="existing-license">
    <fo:block font-weight="bold" font-size="14pt" line-height="17pt" margin-right="36pt" page-break-after="avoid" space-after="6pt" space-before="18pt" text-align="left">Existing license</fo:block>
    <fo:block line-height="16pt" space-after="9pt" text-align="justify">
      You can use your existing license entitlement but must transfer or
      register coverage in the Cloud service. This would consume the
      specified number of cores from your license pool. An enterprise
      license can be freely used in EC2 instance(s). This might apply in
      the situation where the Cloud environment is intended to be a
      permanent static instance. Typically, this will be associated with
      a virtual private cloud (VPC) service. For example, AWS lets you
      provision a logically isolated section of the Cloud where you can
      launch AWS resources in a virtual network. The virtual network is
      controlled by your business, including the choice of IP, subnet,
      DNS, names, security, access, etc.
    </fo:block>
  </fo:block>
  <fo:block id="on-demand-licensing">
    <fo:block font-weight="bold" font-size="14pt" line-height="17pt" margin-right="36pt" page-break-after="avoid" space-after="6pt" space-before="18pt" text-align="left">On-demand licensing</fo:block>
    <fo:block line-height="16pt" space-after="9pt" text-align="justify">
      You can sign up for an on-demand license, and use it to enable
      kdb+ on each of the on-demand EC2 nodes. Kdb+ on-demand usage
      registers by core and by minutes of execution.
    </fo:block>
  </fo:block>
</fo:block><fo:block id="encryption">
  <fo:block id="idm220867217136" font-size="18pt" line-height="22pt" margin-right="36pt" page-break-before="always" space-after="60pt" text-align="left">Encryption</fo:block>
  <fo:block line-height="16pt" space-after="9pt" text-align="justify">
    Consider the need for access to any keys used to encrypt and store
    data. Although this is not specific to AWS, do not assume you have
    automatic rights to private keys employed to encrypt the data.
  </fo:block>
  <fo:block line-height="16pt" space-after="9pt" text-align="justify">
    Where a third-party provider supplies or uses encryption or
    compression to store the market data on S3, you will need to check
    the public and private keys are either made available to you, or
    held by some form of external service.
  </fo:block>
</fo:block><fo:block id="benchmarking-methodology">
  <fo:block id="idm220867214752" font-size="18pt" line-height="22pt" margin-right="36pt" page-break-before="always" space-after="60pt" text-align="left">Benchmarking methodology</fo:block>
  <fo:block line-height="16pt" space-after="9pt" text-align="justify">
    For testing raw storage performance, we used a lightweight test
    script developed by Kx, called <fo:inline font-family="Pragmata Pro">nano</fo:inline>, based on the
    script <fo:inline font-family="Pragmata Pro">io.q</fo:inline> written by Kx’s Chief Customer
    Officer, Simon Garland. The scripts used for this benchmarking are
    freely available for use and are published on Github at
    
    <fo:basic-link external-destination="url('https://github.com/KxSystems/nano')"><fo:inline color="#0070cd">KxSystems/nano</fo:inline></fo:basic-link><fo:footnote><fo:inline font-size="8pt" alignment-baseline="hanging">18</fo:inline><fo:footnote-body><fo:block font-size="8pt">18.
									https://github.com/KxSystems/nano</fo:block></fo:footnote-body></fo:footnote>
  </fo:block>
  <fo:block line-height="16pt" space-after="9pt" text-align="justify">
    These sets of scripts are designed to focus on the relative
    performance of distinct I/O functions typically expected by a HDB.
    The measurements are taken from the perspective of the primitive IO
    operations, namely:
  </fo:block>
  <fo:table-and-caption margin-right="36pt" margin-bottom="9pt"><fo:table font-size="9pt"><fo:table-header border-bottom-style="solid" border-bottom-width=".5pt" page-break-after="avoid"><fo:table-row><fo:table-cell font-style="italic" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block linefeed-treatment="preserve" text-align="left">
            test
          </fo:block></fo:table-cell><fo:table-cell font-style="italic" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block linefeed-treatment="preserve" text-align="left">
            what happens
          </fo:block></fo:table-cell></fo:table-row></fo:table-header><fo:table-body><fo:table-row page-break-inside="avoid"><fo:table-cell padding-top="3pt" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block text-align="left">
            Streaming reads
          </fo:block></fo:table-cell><fo:table-cell padding-top="3pt" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block text-align="left">
            One list (e.g. one column) is read sequentially into memory.
            We read the entire space of the list into RAM, and the list
            is memory-mapped into the address space of kdb+.
          </fo:block></fo:table-cell></fo:table-row><fo:table-row page-break-inside="avoid"><fo:table-cell padding-top="3pt" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block text-align="left">
            Large Random Reads
(one mapped read and map/unmapped)
          </fo:block></fo:table-cell><fo:table-cell padding-top="3pt" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block text-align="left">
            100 random-region reads of 1 MB of a single column of data
            are indexed and fetched into memory. Both single mappings
            into memory, and individual map/fetch/unmap sequences.
            Mapped reads are triggered by a page fault from the kernel
            into <fo:inline font-family="Pragmata Pro">mmap</fo:inline>’d user space of kdb+. This is
            representative of a query that requires to read through 100
            large regions of a column of data for one or more dates
            (partitions).
          </fo:block></fo:table-cell></fo:table-row><fo:table-row page-break-inside="avoid"><fo:table-cell padding-top="3pt" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block text-align="left">
            Small Random Reads
(mapped/unmapped sequences)
          </fo:block></fo:table-cell><fo:table-cell padding-top="3pt" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block text-align="left">
            1600 random-region reads of 64 KB of a single column of data
            are indexed and fetched into memory. Both single mappings
            into memory, and individual map/fetch/unmap sequences. Reads
            are triggered by a page fault from the kernel into
            <fo:inline font-family="Pragmata Pro">mmap</fo:inline>’d user space of kdb+. We run both
            fully-mapped tests and tests with map/unmap sequences for
            each read.
          </fo:block></fo:table-cell></fo:table-row><fo:table-row page-break-inside="avoid"><fo:table-cell padding-top="3pt" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block text-align="left">
            Write
          </fo:block></fo:table-cell><fo:table-cell padding-top="3pt" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block text-align="left">
            Write rate is of less interest for this testing, but is
            reported nonetheless.
          </fo:block></fo:table-cell></fo:table-row><fo:table-row page-break-inside="avoid"><fo:table-cell padding-top="3pt" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block text-align="left">
            Metadata:
(<fo:inline font-family="Pragmata Pro">hclose</fo:inline>
            <fo:inline font-family="Pragmata Pro">hopen</fo:inline>)
          </fo:block></fo:table-cell><fo:table-cell padding-top="3pt" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block text-align="left">
            Average time for a typical open/seek to end/close loop. Used
            by TP log as an 
		‘append to’
	 and whenever the
            database is being checked. Can be used to append data to an
            existing HDB column.
          </fo:block></fo:table-cell></fo:table-row><fo:table-row page-break-inside="avoid"><fo:table-cell padding-top="3pt" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block text-align="left">
            Metadata:
<fo:inline font-family="Pragmata Pro">(();,;2 3)</fo:inline>
          </fo:block></fo:table-cell><fo:table-cell padding-top="3pt" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block text-align="left">
            Append data to a modest list of 128 KB, will
            open/stat/seek/write/close. Similar to ticker plant write
            down.
          </fo:block></fo:table-cell></fo:table-row><fo:table-row page-break-inside="avoid"><fo:table-cell padding-top="3pt" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block text-align="left">
            Metadata:
<fo:inline font-family="Pragmata Pro">(();:;2 3)</fo:inline>
          </fo:block></fo:table-cell><fo:table-cell padding-top="3pt" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block text-align="left">
            Assign bytes to a list of 128 KB, stat/seek/write/link.
            Similar to initial creation of a column.
          </fo:block></fo:table-cell></fo:table-row><fo:table-row page-break-inside="avoid"><fo:table-cell padding-top="3pt" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block text-align="left">
            Metadata:
(<fo:inline font-family="Pragmata Pro">hcount</fo:inline>)
          </fo:block></fo:table-cell><fo:table-cell padding-top="3pt" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block text-align="left">
            Typical open/stat/close sequence on a modest list of 128 KB.
            Determine size. e.g. included in <fo:inline font-family="Pragmata Pro">read1</fo:inline>.
          </fo:block></fo:table-cell></fo:table-row><fo:table-row page-break-inside="avoid"><fo:table-cell padding-top="3pt" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block text-align="left">
            Metadata:
(<fo:inline font-family="Pragmata Pro">read1</fo:inline>)
          </fo:block></fo:table-cell><fo:table-cell padding-top="3pt" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block text-align="left">
            An atomic mapped map/read/unmap sequence
            open/stat/seek/read/close sequence. Test on a modest list of
            128 KB.
          </fo:block></fo:table-cell></fo:table-row></fo:table-body></fo:table></fo:table-and-caption>
  <fo:block line-height="16pt" space-after="9pt" text-align="justify">
    This test suite ensures we cover several of the operational tasks
    undertaken during an HDB lifecycle.
  </fo:block>
  <fo:block line-height="16pt" space-after="9pt" text-align="justify">
    For example, one broad comparison between direct-attached storage
    and a networked/shared file system is that the networked file-system
    timings might reflect higher operational overheads vs. a Linux
    kernel block-based direct file system. Note that a shared file
    system will scale up in-line with the implementation of horizontally
    distributed compute, which the block file systems will not easily
    do, if at all. Also note the networked file system may be able to
    leverage 100s or 1000s of storage targets, meaning it can sustain
    high levels of throughput even for a single reader thread.
  </fo:block>
  <fo:block id="baseline-result-using-a-physical-server">
    <fo:block font-weight="bold" font-size="14pt" line-height="17pt" margin-right="36pt" page-break-after="avoid" space-after="6pt" space-before="18pt" text-align="left">Baseline result – using a physical server</fo:block>
    <fo:block line-height="16pt" space-after="9pt" text-align="justify">
      All the appendices refer to tests on AWS.
    </fo:block>
    <fo:block line-height="16pt" space-after="9pt" text-align="justify">
      To see how EC2 nodes compare to a physical server, we show the
      results of running the same set of benchmarks on a server running
      natively, bare metal, instead of on a virtualized server on the
      Cloud.
    </fo:block>
    <fo:block line-height="16pt" space-after="9pt" text-align="justify">
      For the physical server, we benchmarked a two-socket Broadwell
      E5-2620 v4 @ 2.10 GHz; 128 GB DDR4 2133 MHz. This used one Micron
      PCIe NVMe drive, with CentOS 7.3. For the block device settings,
      we set the device read-ahead settings to 32 KB and the queue
      depths to 64. It is important to note this is just a reference
      point and not a full solution for a typical HDB. This is because
      the number of target drives at your disposal here will limited by
      the number of slots in the server.
    </fo:block>
    <fo:block line-height="16pt" space-after="9pt" text-align="justify">
      Highlights:
    </fo:block>
    <fo:block id="creating-a-memory-list">
      <fo:block font-size="12pt" font-weight="bold" line-height="17pt" margin-right="36pt" page-break-after="avoid" space-after="6pt" space-before="18pt" text-align="left">Creating a memory list</fo:block>
      <fo:block line-height="16pt" space-after="9pt" text-align="justify">
        The MB/sec that can be laid out in a simple list
        allocation/creation in kdb+. Here we create a list of longs of
        approximately half the size of available RAM in the server.
      </fo:block>
      <fo:block page-break-after="avoid" text-align="center"><fo:external-graphic content-width="scale-down-to-fit" src="url(img/media/image8.png)" width="100%"/></fo:block><fo:block font-size="9pt" font-style="italic" margin-bottom="15pt">Creating a memory
          list</fo:block>
      <fo:block line-height="16pt" space-after="9pt" text-align="justify">
        Shows the capability of the server when laying out lists in
        memory; reflects the combination of memory speeds alongside the
        CPU.
      </fo:block>
    </fo:block>
    <fo:block id="re-read-from-cache">
      <fo:block font-size="12pt" font-weight="bold" line-height="17pt" margin-right="36pt" page-break-after="avoid" space-after="6pt" space-before="18pt" text-align="left">Re-read from cache</fo:block>
      <fo:block line-height="16pt" space-after="9pt" text-align="justify">
        The MB/sec that can be re-read when the data is already held by
        the kernel buffer cache (or file-system cache, if kernel buffer
        not used). It includes the time to map the pages back into the
        memory space of kdb+ as we effectively restart the instance here
        without flushing the buffer cache or file system cache.
      </fo:block>
      <fo:block page-break-after="avoid" text-align="center"><fo:external-graphic content-width="scale-down-to-fit" src="url(img/media/image9.png)" width="100%"/></fo:block><fo:block font-size="9pt" font-style="italic" margin-bottom="15pt">Re-read from cache</fo:block>
      <fo:block line-height="16pt" space-after="9pt" text-align="justify">
        Shows if there are any unexpected glitches with the file-system
        caching subsystem. This may not affect your product kdb+ code
        per-se, but may be of interest in your research.
      </fo:block>
    </fo:block>
    <fo:block id="streaming-reads">
      <fo:block font-size="12pt" font-weight="bold" line-height="17pt" margin-right="36pt" page-break-after="avoid" space-after="6pt" space-before="18pt" text-align="left">Streaming reads</fo:block>
      <fo:block line-height="16pt" space-after="9pt" text-align="justify">
        Where complex queries demand wide time periods or symbol ranges.
        An example of this might be a VWAP trading calculation. These
        types of queries are most impacted by the throughput rate i.e.,
        the slower the rate, the higher the query wait time.
      </fo:block>
      <fo:block page-break-after="avoid" text-align="center"><fo:external-graphic content-width="scale-down-to-fit" src="url(img/media/image10.png)" width="100%"/></fo:block><fo:block font-size="9pt" font-style="italic" margin-bottom="15pt">Streaming reads</fo:block>
      <fo:block line-height="16pt" space-after="9pt" text-align="justify">
        Shows that a single q process can ingest at 1900 MB/sec with
        data hosted on a single drive, into kdb+’s memory space, mapped.
        Theoretical maximum for the device is approximately 2800 MB/sec
        and we achieve 2689 MB/sec. Note that with 16 reader processes,
        this throughput continues to scale up to the device limit,
        meaning kdb+ can drive the device harder, as more processes are
        added.
      </fo:block>
    </fo:block>
    <fo:block id="random-reads">
      <fo:block font-size="12pt" font-weight="bold" line-height="17pt" margin-right="36pt" page-break-after="avoid" space-after="6pt" space-before="18pt" text-align="left">Random reads</fo:block>
      <fo:block line-height="16pt" space-after="9pt" text-align="justify">
        We compare the throughputs for random 1 MB-sized reads. This
        simulates more precise data queries spanning smaller periods of
        time or symbol ranges.
      </fo:block>
      <fo:block line-height="16pt" space-after="9pt" text-align="justify">
        In all random-read benchmarks, the term <fo:inline font-style="italic">full
        map</fo:inline> refers to reading pages from the storage target
        straight into regions of memory that are pre-mapped.
      </fo:block>
      <fo:block page-break-after="avoid" text-align="center"><fo:external-graphic content-width="scale-down-to-fit" src="url(img/media/image11.png)" width="100%"/></fo:block><fo:block font-size="9pt" font-style="italic" margin-bottom="15pt">Random 1 MB read</fo:block>
      <fo:block page-break-after="avoid" text-align="center"><fo:external-graphic content-width="scale-down-to-fit" src="url(img/media/image12.png)" width="100%"/></fo:block><fo:block font-size="9pt" font-style="italic" margin-bottom="15pt">Random 64 KB reads</fo:block>
      <fo:block line-height="16pt" space-after="9pt" text-align="justify">
        Simulates queries that are searching around broadly different
        times or symbol regions. This shows that a typical NVMe device
        under kdb+ trends very well when we are reading smaller/random
        regions one or more columns at the same time. This shows that
        the device actually gets similar throughput when under high
        parallel load as threads increase, meaning more requests are
        queuing to the device and the latency per request sustains.
      </fo:block>
    </fo:block>
    <fo:block id="metadata-function-response-times">
      <fo:block font-size="12pt" font-weight="bold" line-height="17pt" margin-right="36pt" page-break-after="avoid" space-after="6pt" space-before="18pt" text-align="left">Metadata function response times</fo:block>
      <fo:block line-height="16pt" space-after="9pt" text-align="justify">
        We also look at metadata function response times for the file
        system. In the baseline results below, you can see what a
        theoretical lowest figure might be.
      </fo:block>
      <fo:block line-height="16pt" space-after="9pt" text-align="justify">
        We deliberately did not run metadata tests using very large data
        sets/files, so that they better represent just the overhead of
        the file system, the Linux kernel and target device.
      </fo:block>
      <fo:table-and-caption margin-right="36pt" margin-bottom="9pt"><fo:table font-size="9pt"><fo:table-header border-bottom-style="solid" border-bottom-width=".5pt" page-break-after="avoid"><fo:table-row><fo:table-cell font-style="italic" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block linefeed-treatment="preserve" text-align="left">
                function
              </fo:block></fo:table-cell><fo:table-cell font-style="italic" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block linefeed-treatment="preserve" text-align="left">
                latency (mSec)
              </fo:block></fo:table-cell><fo:table-cell font-style="italic" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block linefeed-treatment="preserve" text-align="left">
                function
              </fo:block></fo:table-cell><fo:table-cell font-style="italic" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block linefeed-treatment="preserve" text-align="left">
                latency (mSec)
              </fo:block></fo:table-cell></fo:table-row></fo:table-header><fo:table-body><fo:table-row page-break-inside="avoid"><fo:table-cell padding-top="3pt" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block text-align="left">
                <fo:inline font-family="Pragmata Pro">hclose hopen</fo:inline>
              </fo:block></fo:table-cell><fo:table-cell padding-top="3pt" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block text-align="left">
                0.006
              </fo:block></fo:table-cell><fo:table-cell padding-top="3pt" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block text-align="left">
                <fo:inline font-family="Pragmata Pro">();,;2 3</fo:inline>
              </fo:block></fo:table-cell><fo:table-cell padding-top="3pt" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block text-align="left">
                0.01
              </fo:block></fo:table-cell></fo:table-row><fo:table-row page-break-inside="avoid"><fo:table-cell padding-top="3pt" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block text-align="left">
                <fo:inline font-family="Pragmata Pro">hcount</fo:inline>
              </fo:block></fo:table-cell><fo:table-cell padding-top="3pt" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block text-align="left">
                0.003
              </fo:block></fo:table-cell><fo:table-cell padding-top="3pt" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block text-align="left">
                <fo:inline font-family="Pragmata Pro">read1</fo:inline>
              </fo:block></fo:table-cell><fo:table-cell padding-top="3pt" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block text-align="left">
                0.022
              </fo:block></fo:table-cell></fo:table-row></fo:table-body></fo:table></fo:table-and-caption>
      <fo:block line-height="16pt" space-after="9pt" text-align="justify">
        <fo:inline font-size="10pt"><fo:inline font-style="italic">Physical server, metadata operational latencies
        - mSecs (headlines)</fo:inline></fo:inline>
      </fo:block>
      <fo:block page-break-after="avoid" text-align="center"><fo:external-graphic content-width="scale-down-to-fit" src="url(img/media/image13.png)" width="100%"/></fo:block><fo:block font-size="9pt" font-style="italic" margin-bottom="15pt">Metadata latency</fo:block>
      <fo:block line-height="16pt" space-after="9pt" text-align="justify">
        This appears to be sustained for multiple q processes, and on
        the whole is below the multiple μSecs range. Kdb+ sustains good
        metrics.
      </fo:block>
    </fo:block>
  </fo:block>
  <fo:block id="aws-instance-local-ssdnvme">
    <fo:block font-weight="bold" font-size="14pt" line-height="17pt" margin-right="36pt" page-break-after="avoid" space-after="6pt" space-before="18pt" text-align="left">AWS instance local SSD/NVMe</fo:block>
    <fo:block line-height="16pt" space-after="9pt" text-align="justify">
      We separate this specific test from other storage tests, as these
      devices are contained within the EC2 instance itself, unlike every
      other solution reviewed in 
						‘Appendix
      A’
					. Note that some of the solutions reviewed in the
      appendixes do actually leverage instances containing these
      devices.
    </fo:block>
    <fo:block line-height="16pt" space-after="9pt" text-align="justify">
      An instance-local store provides temporary block-level storage for
      your instance. This storage is located on disks that are
      physically attached to the host computer.
    </fo:block>
    <fo:block line-height="16pt" space-after="9pt" text-align="justify">
      This is available in a few predefined regions (e.g. US-East-1),
      and for a selected list of specific instances. In each case, the
      instance local storage is provisioned for you when created and
      started. The size and quantity of drives is preordained and fixed
      in both size and quantity. This differs from EBS, where you can
      select your own.
    </fo:block>
    <fo:block line-height="16pt" space-after="9pt" text-align="justify">
      For this test we selected the <fo:inline font-family="Pragmata Pro">i3.8xlarge</fo:inline> as the
      instance under
      test.
      <fo:inline font-family="Pragmata Pro">i3</fo:inline> instance definitions will provision local
      NVMe or SATA SSD drives for local attached storage, without the
      need for networked EBS.
    </fo:block>
    <fo:block line-height="16pt" space-after="9pt" text-align="justify">
      Locally provisioned SSD and NVMe are supported by kdb+. The
      results from these two represent the highest performance per
      device available for read rates from any non-volatile storage in
      EC2.
    </fo:block>
    <fo:block line-height="16pt" space-after="9pt" text-align="justify">
      However, note that this data is ephemeral. That is, whenever you
      stop an instance, EC2 is at liberty to reassign that space to
      another instance and it will scrub the original data. When the
      instance is restarted, the storage will be available but scrubbed.
      This is because the instance is physically associated with the
      drives, and you do not know where the physical instance will be
      assigned at start time. The only exception to this is if the
      instance crashes or reboots without an operational stop of the
      instance, then the same storage will recur on the same instance.
    </fo:block>
    <fo:block line-height="16pt" space-after="9pt" text-align="justify">
      The cost of instance-local SSD is embedded in the fixed price of
      the instance, so this pricing model needs to be considered. By
      contrast, the cost of EBS is fixed per GB per month, pro-rated.
      The data held on instance local SSD is not natively sharable. If
      this needs to be shared, this will require a shared file-system to
      be layered on top, i.e. demoting this node to be a file system
      server node. For the above reasons, these storage types have been
      used by solutions such as
      
						‘WekaIO’
					, for their
      local instance of the erasure coded data cache.
    </fo:block>
    <fo:table-and-caption margin-right="36pt" margin-bottom="9pt"><fo:table font-size="9pt"><fo:table-header border-bottom-style="solid" border-bottom-width=".5pt" page-break-after="avoid"><fo:table-row><fo:table-cell font-style="italic" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block linefeed-treatment="preserve" text-align="left">
              function
            </fo:block></fo:table-cell><fo:table-cell font-style="italic" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block linefeed-treatment="preserve" text-align="center">
              instance-local NVMe
(4 × 1.9 TB)
            </fo:block></fo:table-cell><fo:table-cell font-style="italic" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block linefeed-treatment="preserve" text-align="center">
              physical node
(1 NVMe)
            </fo:block></fo:table-cell></fo:table-row></fo:table-header><fo:table-body><fo:table-row page-break-inside="avoid"><fo:table-cell padding-top="3pt" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block text-align="left">
              streaming read (MB/sec)
            </fo:block></fo:table-cell><fo:table-cell padding-top="3pt" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block text-align="center">
              7006
            </fo:block></fo:table-cell><fo:table-cell padding-top="3pt" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block text-align="center">
              2624
            </fo:block></fo:table-cell></fo:table-row><fo:table-row page-break-inside="avoid"><fo:table-cell padding-top="3pt" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block text-align="left">
              random 1-MB read (MB/sec)
            </fo:block></fo:table-cell><fo:table-cell padding-top="3pt" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block text-align="center">
              6422
            </fo:block></fo:table-cell><fo:table-cell padding-top="3pt" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block text-align="center">
              2750
            </fo:block></fo:table-cell></fo:table-row><fo:table-row page-break-inside="avoid"><fo:table-cell padding-top="3pt" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block text-align="left">
              random 64-KB read (MB/sec)
            </fo:block></fo:table-cell><fo:table-cell padding-top="3pt" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block text-align="center">
              1493
            </fo:block></fo:table-cell><fo:table-cell padding-top="3pt" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block text-align="center">
              1182
            </fo:block></fo:table-cell></fo:table-row><fo:table-row page-break-inside="avoid"><fo:table-cell padding-top="3pt" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block text-align="left">
              metadata (<fo:inline font-family="Pragmata Pro">hclose</fo:inline>,
              <fo:inline font-family="Pragmata Pro">hopen</fo:inline>)
            </fo:block></fo:table-cell><fo:table-cell padding-top="3pt" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block text-align="center">
              0.0038 mSec
            </fo:block></fo:table-cell><fo:table-cell padding-top="3pt" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block text-align="center">
              0.0068 mSec
            </fo:block></fo:table-cell></fo:table-row></fo:table-body></fo:table></fo:table-and-caption>
    <fo:block line-height="16pt" space-after="9pt" text-align="justify">
      The variation of absolute streaming rates is reflective of the
      device itself. These results are equivalent to the results seen on
      physical servers. What is interesting is that at high parallelism,
      the targets work quicker with random reads and for metadata
      service times than the physical server. These instances can be
      deployed as a high-performance persistent cache for some of the
      AWS-based file system solutions, such as used in ObjectiveFS and
      WekaIO Matrix and Quobyte.
    </fo:block>
  </fo:block>
</fo:block><fo:block id="observations-from-kdb-testing">
  <fo:block id="idm220867115184" font-size="18pt" line-height="22pt" margin-right="36pt" page-break-before="always" space-after="60pt" text-align="left">Observations from kdb+ testing</fo:block>
  <fo:block id="cpu-and-memory-speed">
    <fo:block font-weight="bold" font-size="14pt" line-height="17pt" margin-right="36pt" page-break-after="avoid" space-after="6pt" space-before="18pt" text-align="left">CPU and memory speed</fo:block>
    <fo:block line-height="16pt" space-after="9pt" text-align="justify">
      For CPU and memory speed/latencies with kdb+, EC2 compute nodes
      performance for CPU/memory mirrors the capability of logically
      equivalent bare-metal servers. At time of writing, your main
      decision here is the selection of system instance. CPUs range from
      older generation Intel up to Haswell and Broadwell, and from 1
      core up to 128 vcores (vCPU). Memory ranges from 1 GB up to
      1952 GB RAM.
    </fo:block>
  </fo:block>
  <fo:block id="storage-performance">
    <fo:block font-weight="bold" font-size="14pt" line-height="17pt" margin-right="36pt" page-break-after="avoid" space-after="6pt" space-before="18pt" text-align="left">Storage performance</fo:block>
    <fo:block line-height="16pt" space-after="9pt" text-align="justify">
      The best storage performance was, as expected, achieved with
      locally-attached ephemeral NVMe storage. This matched, or
      exceeded, EBS as that storage is virtualized and will have higher
      latency figures. As data kept on this device cannot be easily
      shared, we anticipate this being considered for a super cache for
      hot data (recent dates). Data stored here would have to be
      replicated at some point as this data could be lost if the
      instance is shut down by the operator.
    </fo:block>
  </fo:block>
  <fo:block id="wire-speeds">
    <fo:block font-weight="bold" font-size="14pt" line-height="17pt" margin-right="36pt" page-break-after="avoid" space-after="6pt" space-before="18pt" text-align="left">Wire speeds</fo:block>
    <fo:block line-height="16pt" space-after="9pt" text-align="justify">
      Kdb+ reaches wire speeds on most streaming read tests to
      networked/shared storage, under kdb+, and in several cases we can
      reach wire speeds for random 1-MB reads using standard mapped
      reads into standard q abstractions, such as lists.
    </fo:block>
  </fo:block>
  <fo:block id="gp2-vs-io1">
    <fo:block font-weight="bold" font-size="14pt" line-height="17pt" margin-right="36pt" page-break-after="avoid" space-after="6pt" space-before="18pt" text-align="left">gp2 vs io1</fo:block>
    <fo:block line-height="16pt" space-after="9pt" text-align="justify">
      EBS was tested for both <fo:inline font-family="Pragmata Pro">gp2</fo:inline> and its brethren
      the <fo:inline font-family="Pragmata Pro">io1</fo:inline> flash variation. Kdb+ achieved wire
      speed bandwidth for both of these. When used for larger
      capacities, we saw no significant advantages of
      <fo:inline font-family="Pragmata Pro">io1</fo:inline> for the HDB store use case, so the
      additional charges applied there need to be considered.
    </fo:block>
  </fo:block>
  <fo:block id="st1">
    <fo:block font-weight="bold" font-size="14pt" line-height="17pt" margin-right="36pt" page-break-after="avoid" space-after="6pt" space-before="18pt" text-align="left">st1</fo:block>
    <fo:block line-height="16pt" space-after="9pt" text-align="justify">
      EBS results for the <fo:inline font-family="Pragmata Pro">st1</fo:inline> devices (low cost
      traditional disk drives, lower cost per GB) show good
      (90th-percentile) results for streaming and random 1-MB reads,
      but, as expected, significantly slower results for random 64-KB
      and 1-MB reads, and 4× the latencies for metadata ops. Consider
      these as a good candidate for storing longer term, older HDB data
      to reduce costs for owned EBS storage.
    </fo:block>
  </fo:block>
  <fo:block id="objectivefs-and-wekaio-matrix">
    <fo:block font-weight="bold" font-size="14pt" line-height="17pt" margin-right="36pt" page-break-after="avoid" space-after="6pt" space-before="18pt" text-align="left">ObjectiveFS and WekaIO Matrix</fo:block>
    <fo:block line-height="16pt" space-after="9pt" text-align="justify">
      ObjectiveFS and WekaIO Matrix are commercial products that offer
      full operational functionality for the POSIX interface, when
      compared to open-source S3 gateway products. These can be used to
      store and read your data from/to S3 buckets.
    </fo:block>
    <fo:block line-height="16pt" space-after="9pt" text-align="justify">
      WekaIO Matrix offers an erasure-encoded clustered file-system,
      which works by sharing out pieces of the data around each of the
      members of the Matrix cluster.
    </fo:block>
    <fo:block line-height="16pt" space-after="9pt" text-align="justify">
      ObjectiveFS works between kdb+ and S3 with a per-instance buffer
      cache plus distributed eventual consistency. It also allows you to
      cache files locally in RAM cache and/or on ephemeral drives within
      the instance. Caching to locally provisioned drives is likely to
      be more attractive vs. caching to another RAM cache.
    </fo:block>
  </fo:block>
  <fo:block id="posix-file-systems">
    <fo:block font-weight="bold" font-size="14pt" line-height="17pt" margin-right="36pt" page-break-after="avoid" space-after="6pt" space-before="18pt" text-align="left">POSIX file systems</fo:block>
    <fo:block line-height="16pt" space-after="9pt" text-align="justify">
      Standalone file systems such as MapR-FS and Quobyte support POSIX
      fully. Other distributed file systems designed from the offset to
      support POSIX should fare equally well, as to some degree, the
      networking infrastructure is consistent when measured within one
      availability zone or placement group. Although these file system
      services are encapsulated in the AWS marketplace as AMI’s, you are
      obliged to run this estate alongside your HDB compute estate, as
      you would own and manage the HDB just the same as if it were
      in-house. Although the vendors supply AWS marketplace instances,
      you would own and running your own instances required for the file
      system.
    </fo:block>
  </fo:block>
  <fo:block id="wekaio-and-quobyte">
    <fo:block font-weight="bold" font-size="14pt" line-height="17pt" margin-right="36pt" page-break-after="avoid" space-after="6pt" space-before="18pt" text-align="left">WekaIO and Quobyte</fo:block>
    <fo:block line-height="16pt" space-after="9pt" text-align="justify">
      WekaIO and Quobyte use a distributed file-system based on
      erasure-coding distribution of data amongst their quorum of nodes
      in the cluster. This may be appealing to customers wanting to
      provision the HDB data alongside the compute nodes. If, for
      example, you anticipate using eight or nine nodes in production
      these nodes could also be configured to fully own and manage the
      file system in a reliable way, and would not mandate the creation
      of distinct file-system services to be created in other AWS
      instances in the VPC.
    </fo:block>
    <fo:block line-height="16pt" space-after="9pt" text-align="justify">
      What might not be immediately apparent is that for this style of
      product, they will scavenge at least one core on every
      participating node in order to run their erasure-coding algorithm
      most efficiently. This core will load at 100% CPU.
    </fo:block>
  </fo:block>
  <fo:block id="efs-and-aws-gateway">
    <fo:block font-weight="bold" font-size="14pt" line-height="17pt" margin-right="36pt" page-break-after="avoid" space-after="6pt" space-before="18pt" text-align="left">EFS and AWS Gateway</fo:block>
    <fo:block line-height="16pt" space-after="9pt" text-align="justify">
      Avoid
      <fo:basic-link external-destination="url('http://docs.aws.amazon.com/efs/latest/ug/performance.html')"><fo:inline color="#0070cd">EFS</fo:inline></fo:basic-link><fo:footnote><fo:inline font-size="8pt" alignment-baseline="hanging">21</fo:inline><fo:footnote-body><fo:block font-size="8pt">21.
									http://docs.aws.amazon.com/efs/latest/ug/performance.html</fo:block></fo:footnote-body></fo:footnote>
      and AWS Gateway for HDB storage. They both exhibit very high
      latencies of operation in addition to the network-bandwidth
      constraints. They appear to impact further on the overall
      performance degradations seen in generic NFS builds in Linux. This
      stems from the latency between a customer-owned S3 bucket (AWS
      Gateway), and an availability zone wide distribution of S3 buckets
      managed privately by AWS.
    </fo:block>
  </fo:block>
  <fo:block id="open-source-products">
    <fo:block font-weight="bold" font-size="14pt" line-height="17pt" margin-right="36pt" page-break-after="avoid" space-after="6pt" space-before="18pt" text-align="left">Open-source products</fo:block>
    <fo:block line-height="16pt" space-after="9pt" text-align="justify">
      Although the open source products that front an S3 store (S3FS,
      S3QL and Goofys) do offer POSIX, they all fail to offer full POSIX
      semantics such as symbolic linking, hard linking and file locking.
      Although these may not be crucial for your use case, it needs
      consideration.
    </fo:block>
    <fo:block line-height="16pt" space-after="9pt" text-align="justify">
      You might also want to avoid these, as performance of them is at
      best average, partly because they both employ user-level FUSE code
      for POSIX support.
    </fo:block>
  </fo:block>
</fo:block><fo:block id="network-configuration">
  <fo:block id="idm220867088480" font-size="18pt" line-height="22pt" margin-right="36pt" page-break-before="always" space-after="60pt" text-align="left">Network configuration</fo:block>
  <fo:block line-height="16pt" space-after="9pt" text-align="justify">
    The network configuration used in the tests:
  </fo:block>
  <fo:block line-height="16pt" space-after="9pt" text-align="justify">
    The host build was CentOS 7.4, with Kernel 3.10.0-693.el7.x86_64.
    The ENS module was installed but not configured. The default
    instance used in these test reports was
    <fo:inline font-family="Pragmata Pro">r4.4xlarge</fo:inline>.
  </fo:block>
  <fo:block line-height="16pt" space-after="9pt" text-align="justify">
    Total network bandwidth on this model is 
		‘up-to’
	
    10 Gbps.
  </fo:block>
  <fo:block line-height="16pt" space-after="9pt" text-align="justify">
    For storage, this is documented by AWS as provisioning up to
    3,500 Mbps, equivalent to 437 MB/sec of EBS bandwidth, per node,
    bi-directional. We met these discrete values as seen in most of our
    individual kdb+ tests.
  </fo:block>
</fo:block><fo:block id="appendix-a---elastic-block-store-ebs">
  <fo:block id="idm220867084048" font-size="18pt" line-height="22pt" margin-right="36pt" page-break-before="always" space-after="60pt" text-align="left">Appendix A - Elastic Block Store (EBS)</fo:block>
  <fo:block background-color="#EEE" font-size="10.5pt" line-height="13pt" margin-bottom="9pt" margin-left="0mm" margin-right="0mm" page-break-inside="avoid" padding-left="3mm" padding-right="3mm" padding-top="3mm"><fo:block margin-bottom="6pt"><fo:inline vertical-align="sub"><fo:inline font-family="Material Icons" font-size="14pt"></fo:inline></fo:inline> <fo:inline font-weight="bold">EBS can be used to store HDB data, and is fully compliant with kdb+. </fo:inline></fo:block><fo:block line-height="16pt" space-after="9pt" text-align="justify">
      It supports all of the POSIX semantics required.
    </fo:block></fo:block>
  <fo:block line-height="16pt" space-after="9pt" text-align="justify">
    Three variants of the
    <fo:basic-link external-destination="url('http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/RootDeviceStorage.html')"><fo:inline color="#0070cd">Elastic
    Block Service</fo:inline></fo:basic-link><fo:footnote><fo:inline font-size="8pt" alignment-baseline="hanging">22</fo:inline><fo:footnote-body><fo:block font-size="8pt">22.
									http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/RootDeviceStorage.html</fo:block></fo:footnote-body></fo:footnote> (EBS) are all qualified by kdb+:
    <fo:inline font-family="Pragmata Pro">gp2</fo:inline> and <fo:inline font-family="Pragmata Pro">io1</fo:inline> are both NAND
    Flash, but offer different price/performance points, and
    <fo:inline font-family="Pragmata Pro">st1</fo:inline> is comprised of traditional drives. Unlike
    ephemeral SSD storage, EBS-based storage can be dynamically
    provisioned to any other EC2 instance via operator control. So this
    is a candidate for on-demand HDB storage. Assign the storage to an
    instance in build scripts and then spin them up. (Ref: Amazon EBS)
  </fo:block>
  <fo:block page-break-after="avoid" text-align="center"><fo:external-graphic content-width="scale-down-to-fit" src="url(img/media/image14.png)" width="100%"/></fo:block><fo:block font-size="9pt" font-style="italic" margin-bottom="15pt">Amazon EC2 instance</fo:block>
  <fo:block line-height="16pt" space-after="9pt" text-align="justify">
    A disadvantage of EBS is that even if the data is read-only
    (immutable) a specific volume cannot be simultaneously mounted and
    shared between two or more EC2 instances. Furthermore, the elastic
    volume would have to be migrated from one instance ownership to
    another, either manually, or with launch scripts. EBS Snapshots can
    be used for regenerating an elastic volume to be copied across to
    other freshly created EBS volumes, which are subsequently shared
    around under EBS with a new instance being deployed on-demand.
  </fo:block>
  <fo:block line-height="16pt" space-after="9pt" text-align="justify">
    Therefore, users of EBS or direct attach containing significant
    volumes of historical data, may need to replicate the data to avoid
    constraining it to just one node. You could also shard the data
    manually, perhaps thence accessing nodes attached via a kdb+ UI
    gateway.
  </fo:block>
  <fo:block line-height="16pt" space-after="9pt" text-align="justify">
    EBS is carried over the local network within one availability zone.
    Between availability zones there would be IP L3 routing protocols
    involved in moving the data between zones, and so the latencies
    would be increased.
  </fo:block>
  <fo:block line-height="16pt" space-after="9pt" text-align="justify">
    EBS may look like a disk, act like a disk, and walk like a disk, but
    it doesn’t behave like a disk in the traditional sense.
  </fo:block>
  <fo:block line-height="16pt" space-after="9pt" text-align="justify">
    There are constraints on calculating the throughput gained from EBS:
  </fo:block>
  <fo:list-block provisional-distance-between-starts="0.3cm" provisional-label-separation="0.15cm"><fo:list-item><fo:list-item-label end-indent="label-end()"><fo:block line-height="16pt">•</fo:block></fo:list-item-label><fo:list-item-body start-indent="body-start()">
      <fo:block line-height="16pt" space-after="9pt" text-align="justify">
        There is a max throughput to/from each physical EBS volume. This
        is set to 500 MB/sec for io1 and 160 MB/sec for
        <fo:inline font-family="Pragmata Pro">gp2</fo:inline>. A <fo:inline font-family="Pragmata Pro">gp2</fo:inline> volume can
        range in size from 1 GB to 16 TB. You can use multiple volumes
        per instance (and we would expect to see that in place with a
        HDB).
      </fo:block>
    </fo:list-item-body></fo:list-item><fo:list-item><fo:list-item-label end-indent="label-end()"><fo:block line-height="16pt">•</fo:block></fo:list-item-label><fo:list-item-body start-indent="body-start()">
      <fo:block line-height="16pt" space-after="9pt" text-align="justify">
        There is a further limit to the volume throughput applied, based
        on its size at creation time. For example, a GP2 volume provides
        a baseline rate of IOPs geared up from the size of the volume
        and calculated on the basis of 3 IOPs/per GB. For 200 GB of
        volume, we get 600 IOPS and @ 1 MB that exceeds the above number
        in (1), so the lower value would remain the cap. The burst peak
        IOPS figure is more meaningful for random, small reads of kdb+
        data.
      </fo:block>
    </fo:list-item-body></fo:list-item><fo:list-item><fo:list-item-label end-indent="label-end()"><fo:block line-height="16pt">•</fo:block></fo:list-item-label><fo:list-item-body start-indent="body-start()">
      <fo:block line-height="16pt" space-after="9pt" text-align="justify">
        For <fo:inline font-family="Pragmata Pro">gp2</fo:inline> volumes there is a burst level cap,
        but this increases as the volume gets larger. This burst level
        peaks at 1 TB, and is 3000 IOPS. that would be 384 MB/sec at
        128 KB records, which, again is in excess of the cap of
        160 MB/sec.
      </fo:block>
    </fo:list-item-body></fo:list-item><fo:list-item><fo:list-item-label end-indent="label-end()"><fo:block line-height="16pt">•</fo:block></fo:list-item-label><fo:list-item-body start-indent="body-start()">
      <fo:block line-height="16pt" space-after="9pt" text-align="justify">
        There is a maximum network bandwidth per instance. In the case
        of the unit under test here we used
        <fo:inline font-family="Pragmata Pro">r4.4xlarge</fo:inline>, which constrains the throughput
        to the instance at 3500 Mbps, or a wire speed of 430 MB/sec,
        capped. This would be elevated with larger instances, up to a
        maximum value of 25 Gbps for a large instance, such as for
        <fo:inline font-family="Pragmata Pro">r4.16xlarge</fo:inline>.
      </fo:block>
    </fo:list-item-body></fo:list-item><fo:list-item><fo:list-item-label end-indent="label-end()"><fo:block line-height="16pt">•</fo:block></fo:list-item-label><fo:list-item-body start-indent="body-start()">
      <fo:block line-height="16pt" space-after="9pt" text-align="justify">
        It is important note that EBS scaled linearly across an entire
        estate (e.g. parallel peach queries). There should be no
        constraints if you are accessing your data, splayed across
        different physical across distinct instances. e.g. 10 nodes of
        <fo:inline font-family="Pragmata Pro">r4.4xlarge</fo:inline> is capable of reading 4300 MB/sec.
      </fo:block>
    </fo:list-item-body></fo:list-item></fo:list-block>
  <fo:block line-height="16pt" space-after="9pt" text-align="justify">
    Kdb+ achieves or meets all of these advertised figures. So the EBS
    network bandwidth algorithms become the dominating factor in any
    final calculations for your environment.
  </fo:block>
  <fo:block line-height="16pt" space-after="9pt" text-align="justify">
    For consistency in all of these evaluations, we tested with a common
    baseline using an <fo:inline font-family="Pragmata Pro">r4.4xlarge</fo:inline> instance with four
    200-GB volumes, each with one xfs file system per volume, therefore
    using four mount points (four partitions). To show the scale to
    higher throughputs we used an <fo:inline font-family="Pragmata Pro">r4.16xlarge</fo:inline>
    instance with more volumes: eight 500-GB targets, (host max
    bandwidth there of 20 Gbps, compared with max EBS bandwidth of
    1280 MB/sec) and we ran the comparison on <fo:inline font-family="Pragmata Pro">gp2</fo:inline> and
    <fo:inline font-family="Pragmata Pro">io1</fo:inline> versions of EBS storage. For the testing of
    <fo:inline font-family="Pragmata Pro">st1</fo:inline> storage, we used four 6-TB volumes, as each
    of these could burst between 240-500 MB/sec. We then compared the
    delta between two instance sizes.
  </fo:block>
  <fo:block id="ebs-gp2">
    <fo:block font-weight="bold" font-size="14pt" line-height="17pt" margin-right="36pt" page-break-after="avoid" space-after="6pt" space-before="18pt" text-align="left">EBS-GP2</fo:block>
    <fo:block line-height="16pt" space-after="9pt" text-align="justify">
      
        <fo:external-graphic content-width="scale-down-to-fit" src="url(img/media/image15.png)" width="100%"/>
      
    </fo:block>
    <fo:block line-height="16pt" space-after="9pt" text-align="justify">
      
        <fo:external-graphic content-width="scale-down-to-fit" src="url(img/media/image16.png)" width="100%"/>
      
    </fo:block>
    <fo:block line-height="16pt" space-after="9pt" text-align="justify">
      
        <fo:external-graphic content-width="scale-down-to-fit" src="url(img/media/image17.png)" width="100%"/>
      
    </fo:block>
    <fo:block line-height="16pt" space-after="9pt" text-align="justify">
      
        <fo:external-graphic content-width="scale-down-to-fit" src="url(img/media/image18.png)" width="100%"/>
      
    </fo:block>
    <fo:table-and-caption margin-right="36pt" margin-bottom="9pt"><fo:table font-size="9pt"><fo:table-header border-bottom-style="solid" border-bottom-width=".5pt" page-break-after="avoid"><fo:table-row><fo:table-cell font-style="italic" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block linefeed-treatment="preserve" text-align="left">
              function
            </fo:block></fo:table-cell><fo:table-cell font-style="italic" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block linefeed-treatment="preserve" text-align="left">
              latency (mSec)
            </fo:block></fo:table-cell><fo:table-cell font-style="italic" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block linefeed-treatment="preserve" text-align="left">
              function
            </fo:block></fo:table-cell><fo:table-cell font-style="italic" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block linefeed-treatment="preserve" text-align="left">
              latency (mSec)
            </fo:block></fo:table-cell></fo:table-row></fo:table-header><fo:table-body><fo:table-row page-break-inside="avoid"><fo:table-cell padding-top="3pt" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block text-align="left">
              <fo:inline font-family="Pragmata Pro">hclose hopen</fo:inline>
            </fo:block></fo:table-cell><fo:table-cell padding-top="3pt" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block text-align="left">
              0.004
            </fo:block></fo:table-cell><fo:table-cell padding-top="3pt" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block text-align="left">
              <fo:inline font-family="Pragmata Pro">();,;2 3</fo:inline>
            </fo:block></fo:table-cell><fo:table-cell padding-top="3pt" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block text-align="left">
              0.006
            </fo:block></fo:table-cell></fo:table-row><fo:table-row page-break-inside="avoid"><fo:table-cell padding-top="3pt" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block text-align="left">
              <fo:inline font-family="Pragmata Pro">hcount</fo:inline>
            </fo:block></fo:table-cell><fo:table-cell padding-top="3pt" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block text-align="left">
              0.002
            </fo:block></fo:table-cell><fo:table-cell padding-top="3pt" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block text-align="left">
              <fo:inline font-family="Pragmata Pro">read1</fo:inline>
            </fo:block></fo:table-cell><fo:table-cell padding-top="3pt" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block text-align="left">
              0.018
            </fo:block></fo:table-cell></fo:table-row></fo:table-body></fo:table></fo:table-and-caption>
    <fo:block line-height="16pt" space-after="9pt" text-align="justify">
      <fo:inline font-size="10pt"><fo:inline font-style="italic">EBS GP2 metadata operational latencies - mSecs
      (headlines)</fo:inline></fo:inline>
    </fo:block>
  </fo:block>
  <fo:block id="ebs-io1">
    <fo:block font-weight="bold" font-size="14pt" line-height="17pt" margin-right="36pt" page-break-after="avoid" space-after="6pt" space-before="18pt" text-align="left">EBS-IO1</fo:block>
    <fo:block line-height="16pt" space-after="9pt" text-align="justify">
      
        <fo:external-graphic content-width="scale-down-to-fit" src="url(img/media/image19.png)" width="100%"/>
      
    </fo:block>
    <fo:block line-height="16pt" space-after="9pt" text-align="justify">
      
        <fo:external-graphic content-width="scale-down-to-fit" src="url(img/media/image20.png)" width="100%"/>
      
    </fo:block>
    <fo:block line-height="16pt" space-after="9pt" text-align="justify">
      
        <fo:external-graphic content-width="scale-down-to-fit" src="url(img/media/image21.png)" width="100%"/>
      
    </fo:block>
    <fo:block line-height="16pt" space-after="9pt" text-align="justify">
      
        <fo:external-graphic content-width="scale-down-to-fit" src="url(img/media/image22.png)" width="100%"/>
      
    </fo:block>
    <fo:table-and-caption margin-right="36pt" margin-bottom="9pt"><fo:table font-size="9pt"><fo:table-header border-bottom-style="solid" border-bottom-width=".5pt" page-break-after="avoid"><fo:table-row><fo:table-cell font-style="italic" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block linefeed-treatment="preserve" text-align="left">
              function
            </fo:block></fo:table-cell><fo:table-cell font-style="italic" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block linefeed-treatment="preserve" text-align="left">
              latency (mSec)
            </fo:block></fo:table-cell><fo:table-cell font-style="italic" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block linefeed-treatment="preserve" text-align="left">
              function
            </fo:block></fo:table-cell><fo:table-cell font-style="italic" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block linefeed-treatment="preserve" text-align="left">
              latency (mSec)
            </fo:block></fo:table-cell></fo:table-row></fo:table-header><fo:table-body><fo:table-row page-break-inside="avoid"><fo:table-cell padding-top="3pt" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block text-align="left">
              <fo:inline font-family="Pragmata Pro">hclose hopen</fo:inline>
            </fo:block></fo:table-cell><fo:table-cell padding-top="3pt" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block text-align="left">
              0.003
            </fo:block></fo:table-cell><fo:table-cell padding-top="3pt" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block text-align="left">
              <fo:inline font-family="Pragmata Pro">();,;2 3</fo:inline>
            </fo:block></fo:table-cell><fo:table-cell padding-top="3pt" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block text-align="left">
              0.006
            </fo:block></fo:table-cell></fo:table-row><fo:table-row page-break-inside="avoid"><fo:table-cell padding-top="3pt" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block text-align="left">
              <fo:inline font-family="Pragmata Pro">hcount</fo:inline>
            </fo:block></fo:table-cell><fo:table-cell padding-top="3pt" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block text-align="left">
              0.002
            </fo:block></fo:table-cell><fo:table-cell padding-top="3pt" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block text-align="left">
              <fo:inline font-family="Pragmata Pro">read1</fo:inline>
            </fo:block></fo:table-cell><fo:table-cell padding-top="3pt" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block text-align="left">
              0.017
            </fo:block></fo:table-cell></fo:table-row></fo:table-body></fo:table></fo:table-and-caption>
    <fo:block line-height="16pt" space-after="9pt" text-align="justify">
      <fo:inline font-size="10pt"><fo:inline font-style="italic">EBS-IO1 metadata operational latencies - mSecs
      (headlines)</fo:inline></fo:inline>
    </fo:block>
  </fo:block>
  <fo:block id="ebs-st1">
    <fo:block font-weight="bold" font-size="14pt" line-height="17pt" margin-right="36pt" page-break-after="avoid" space-after="6pt" space-before="18pt" text-align="left">EBS-ST1</fo:block>
    <fo:block line-height="16pt" space-after="9pt" text-align="justify">
      
        <fo:external-graphic content-width="scale-down-to-fit" src="url(img/media/image23.png)" width="100%"/>
      
    </fo:block>
    <fo:block line-height="16pt" space-after="9pt" text-align="justify">
      
        <fo:external-graphic content-width="scale-down-to-fit" src="url(img/media/image24.png)" width="100%"/>
      
    </fo:block>
    <fo:block line-height="16pt" space-after="9pt" text-align="justify">
      
        <fo:external-graphic content-width="scale-down-to-fit" src="url(img/media/image25.png)" width="100%"/>
      
    </fo:block>
    <fo:block line-height="16pt" space-after="9pt" text-align="justify">
      
        <fo:external-graphic content-width="scale-down-to-fit" src="url(img/media/image26.png)" width="100%"/>
      
    </fo:block>
    <fo:table-and-caption margin-right="36pt" margin-bottom="9pt"><fo:table font-size="9pt"><fo:table-header border-bottom-style="solid" border-bottom-width=".5pt" page-break-after="avoid"><fo:table-row><fo:table-cell font-style="italic" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block linefeed-treatment="preserve" text-align="left">
              function
            </fo:block></fo:table-cell><fo:table-cell font-style="italic" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block linefeed-treatment="preserve" text-align="left">
              latency (mSec)
            </fo:block></fo:table-cell><fo:table-cell font-style="italic" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block linefeed-treatment="preserve" text-align="left">
              function
            </fo:block></fo:table-cell><fo:table-cell font-style="italic" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block linefeed-treatment="preserve" text-align="left">
              latency (mSec)
            </fo:block></fo:table-cell></fo:table-row></fo:table-header><fo:table-body><fo:table-row page-break-inside="avoid"><fo:table-cell padding-top="3pt" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block text-align="left">
              <fo:inline font-family="Pragmata Pro">hclose hopen</fo:inline>
            </fo:block></fo:table-cell><fo:table-cell padding-top="3pt" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block text-align="left">
              0.003
            </fo:block></fo:table-cell><fo:table-cell padding-top="3pt" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block text-align="left">
              <fo:inline font-family="Pragmata Pro">();,;2 3</fo:inline>
            </fo:block></fo:table-cell><fo:table-cell padding-top="3pt" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block text-align="left">
              0.04
            </fo:block></fo:table-cell></fo:table-row><fo:table-row page-break-inside="avoid"><fo:table-cell padding-top="3pt" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block text-align="left">
              <fo:inline font-family="Pragmata Pro">hcount</fo:inline>
            </fo:block></fo:table-cell><fo:table-cell padding-top="3pt" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block text-align="left">
              0.002
            </fo:block></fo:table-cell><fo:table-cell padding-top="3pt" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block text-align="left">
              <fo:inline font-family="Pragmata Pro">read1</fo:inline>
            </fo:block></fo:table-cell><fo:table-cell padding-top="3pt" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block text-align="left">
              0.02
            </fo:block></fo:table-cell></fo:table-row></fo:table-body></fo:table></fo:table-and-caption>
    <fo:block line-height="16pt" space-after="9pt" text-align="justify">
      <fo:inline font-size="10pt"><fo:inline font-style="italic">EBS-ST1 metadata operational latencies - mSecs
      (headlines)</fo:inline></fo:inline>
    </fo:block>
  </fo:block>
  <fo:block id="summary-1">
    <fo:block font-weight="bold" font-size="14pt" line-height="17pt" margin-right="36pt" page-break-after="avoid" space-after="6pt" space-before="18pt" text-align="left">Summary</fo:block>
    <fo:block line-height="16pt" space-after="9pt" text-align="justify">
      Kdb+ matches the expected throughput of EBS for all
      classifications, with no major deviations across all classes of
      read patterns required. EBS-IO1 achieves slightly higher
      throughput metrics over GP2, but achieves this at a guaranteed
      IOPS rate. Its operational latency is very slightly lower for meta
      data and random reads. When considering EBS for kdb+, take the
      following into consideration:
    </fo:block>
    <fo:list-block provisional-distance-between-starts="0.3cm" provisional-label-separation="0.15cm"><fo:list-item><fo:list-item-label end-indent="label-end()"><fo:block line-height="16pt">•</fo:block></fo:list-item-label><fo:list-item-body start-indent="body-start()">
        <fo:block line-height="16pt" space-after="9pt" text-align="justify">
          Due to private-only presentations of EBS volumes, you may wish
          to consider EBS for solutions that shard/segment their HDB
          data between physical nodes in a cluster/gateway architecture.
          Or you may choose to use EBS for locally cached historical
          data, with other file-systems backing EBS with full or partial
          copies of the entire HDB.
        </fo:block>
      </fo:list-item-body></fo:list-item><fo:list-item><fo:list-item-label end-indent="label-end()"><fo:block line-height="16pt">•</fo:block></fo:list-item-label><fo:list-item-body start-indent="body-start()">
        <fo:block line-height="16pt" space-after="9pt" text-align="justify">
          Fixed bandwidth per node: in our testing cases, the instance
          throughput limit of circa 430 MB/sec for
          <fo:inline font-family="Pragmata Pro">r4.4xlarge</fo:inline> is easily achieved with these
          tests. Contrast that with the increased throughput gained with
          the larger <fo:inline font-family="Pragmata Pro">r4.16xlarge</fo:inline> instance. Use this
          precept in your calculations.
        </fo:block>
      </fo:list-item-body></fo:list-item><fo:list-item><fo:list-item-label end-indent="label-end()"><fo:block line-height="16pt">•</fo:block></fo:list-item-label><fo:list-item-body start-indent="body-start()">
        <fo:block line-height="16pt" space-after="9pt" text-align="justify">
          There is a fixed throughput per GP2 volume, maxing at 160 MB/
          sec. But multiple volumes will increment that value up until
          the peak achievable in the instance definition. Kdb+ achieves
          that instance peak throughput.
        </fo:block>
      </fo:list-item-body></fo:list-item><fo:list-item><fo:list-item-label end-indent="label-end()"><fo:block line-height="16pt">•</fo:block></fo:list-item-label><fo:list-item-body start-indent="body-start()">
        <fo:block line-height="16pt" space-after="9pt" text-align="justify">
          Server-side kdb+ in-line compression works very well for
          streaming and random 1-MB read throughputs, whereby the CPU
          essentially keeps up with the lower level of compressed data
          ingest from EBS, and for random reads with many processes, due
          to read-ahead and decompression running in-parallel being able
          to magnify the input bandwidth, pretty much in line with the
          compression rate.
        </fo:block>
      </fo:list-item-body></fo:list-item><fo:list-item><fo:list-item-label end-indent="label-end()"><fo:block line-height="16pt">•</fo:block></fo:list-item-label><fo:list-item-body start-indent="body-start()">
        <fo:block line-height="16pt" space-after="9pt" text-align="justify">
          <fo:inline font-family="Pragmata Pro">st1</fo:inline> works well at streaming reads, but will
          suffer from high latencies for any form of random searching.
          Due to the lower capacity cost of <fo:inline font-family="Pragmata Pro">st1</fo:inline>, you
          may wish to consider this for data that is considered for
          streaming reads only, e.g. older data.
        </fo:block>
      </fo:list-item-body></fo:list-item></fo:list-block>
  </fo:block>
</fo:block><fo:block id="appendix-b-efs-nfs">
  <fo:block id="idm220866988544" font-size="18pt" line-height="22pt" margin-right="36pt" page-break-before="always" space-after="60pt" text-align="left">Appendix B – EFS (NFS)</fo:block>
  <fo:block line-height="16pt" space-after="9pt" text-align="justify">
    EFS is an NFS service owned and run by AWS that offers NFS service
    for nodes in the same availability zone, and can run across zones,
    or can be exposed externally. The location of where the storage is
    kept is owned by Amazon and is not made transparent to the user. The
    only access to the data is via using the service by name (NFS
    service), and there is no block or object access to said data.
  </fo:block>
  <fo:table-and-caption margin-right="36pt" margin-bottom="9pt"><fo:table font-size="9pt"><fo:table-header border-bottom-style="solid" border-bottom-width=".5pt" page-break-after="avoid"><fo:table-row><fo:table-cell font-style="italic" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block linefeed-treatment="preserve" text-align="left">
             
          </fo:block></fo:table-cell><fo:table-cell font-style="italic" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block linefeed-treatment="preserve" text-align="left">
            Amazon EFS
          </fo:block></fo:table-cell><fo:table-cell font-style="italic" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block linefeed-treatment="preserve" text-align="left">
            Amazon EBS Provisioned IOPS
          </fo:block></fo:table-cell></fo:table-row></fo:table-header><fo:table-body><fo:table-row page-break-inside="avoid"><fo:table-cell padding-top="3pt" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block text-align="left">
            Availability and durability
          </fo:block></fo:table-cell><fo:table-cell padding-top="3pt" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block text-align="left">
            Data is stored independently across multiple AZs.
          </fo:block></fo:table-cell><fo:table-cell padding-top="3pt" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block text-align="left">
            Data is stored redundantly in a single AZ.
          </fo:block></fo:table-cell></fo:table-row><fo:table-row page-break-inside="avoid"><fo:table-cell padding-top="3pt" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block text-align="left">
            Access
          </fo:block></fo:table-cell><fo:table-cell padding-top="3pt" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block text-align="left">
            Up to thousands of Amazon EC2 instances, from multiple AZs,
            can connect concurrently to a file system.
          </fo:block></fo:table-cell><fo:table-cell padding-top="3pt" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block text-align="left">
            A single Amazon EC2 instance can connect to a file system.
          </fo:block></fo:table-cell></fo:table-row><fo:table-row page-break-inside="avoid"><fo:table-cell padding-top="3pt" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block text-align="left">
            Use cases
          </fo:block></fo:table-cell><fo:table-cell padding-top="3pt" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block text-align="left">
            Big data and analytics, media processing workflows, content
            management, web serving, and home directories.
          </fo:block></fo:table-cell><fo:table-cell padding-top="3pt" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block text-align="left">
            Boot volumes, transactional and NoSQL databases, data
            warehousing, and ETL.
          </fo:block></fo:table-cell></fo:table-row></fo:table-body></fo:table></fo:table-and-caption>
  <fo:block line-height="16pt" space-after="9pt" text-align="justify">
    One way to think about EFS is that it is a service deployed in some
    regions (not all) of the AWS estate. It does indeed leverage S3 as a
    persistent storage, but the EFS users have no visibility of a single
    instance of the server, as the service itself is ephemeral and is
    deployed throughout all availability zones.
  </fo:block>
  <fo:block line-height="16pt" space-after="9pt" text-align="justify">
    This is different from running your own NFS service, whereby you
    would define and own the instance by name, and then connect it to an
    S3 bucket that you also own and define.
  </fo:block>
  <fo:block line-height="16pt" space-after="9pt" text-align="justify">
    A constraint of EFS for kdb+ is that performance is limited by a
    predefined burst limit, which is based on the file-system size:
  </fo:block>
  <fo:table-and-caption margin-right="36pt" margin-bottom="9pt"><fo:table font-size="9pt"><fo:table-header border-bottom-style="solid" border-bottom-width=".5pt" page-break-after="avoid"><fo:table-row><fo:table-cell font-style="italic" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block linefeed-treatment="preserve" text-align="left">
            file-system size
          </fo:block></fo:table-cell><fo:table-cell font-style="italic" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block linefeed-treatment="preserve" text-align="left">
            aggregate read/write throughput
          </fo:block></fo:table-cell></fo:table-row></fo:table-header><fo:table-body><fo:table-row page-break-inside="avoid"><fo:table-cell padding-top="3pt" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block text-align="left">
            100 GiB
          </fo:block></fo:table-cell><fo:table-cell padding-top="3pt" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block text-align="left">
            • burst to 100 MiB/s for up to 72 min a day
• drive up
            to 5 MiB/s continuously
          </fo:block></fo:table-cell></fo:table-row><fo:table-row page-break-inside="avoid"><fo:table-cell padding-top="3pt" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block text-align="left">
            1 TiB
          </fo:block></fo:table-cell><fo:table-cell padding-top="3pt" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block text-align="left">
            • burst to 100 MiB/s for 12 hours a day
• drive 50 MiB/s
            continuously
          </fo:block></fo:table-cell></fo:table-row><fo:table-row page-break-inside="avoid"><fo:table-cell padding-top="3pt" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block text-align="left">
            10 TiB
          </fo:block></fo:table-cell><fo:table-cell padding-top="3pt" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block text-align="left">
            • burst to 1 GiB/s for 12 hours a day
• drive 500 MiB/s
            continuously
          </fo:block></fo:table-cell></fo:table-row><fo:table-row page-break-inside="avoid"><fo:table-cell padding-top="3pt" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block text-align="left">
            larger
          </fo:block></fo:table-cell><fo:table-cell padding-top="3pt" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block text-align="left">
            • burst to 100 MiB/s per TiB of storage for 12 hours a
            day
• drive 50 MiB/s per TiB of storage continuously
          </fo:block></fo:table-cell></fo:table-row></fo:table-body></fo:table></fo:table-and-caption>
  <fo:block line-height="16pt" space-after="9pt" text-align="justify">
    So, the EFS solution offers a single name space for your HDB
    structure, and this can be shared around multiple instances
    including the ability for one or more nodes to be able to write to
    the space, which is useful for daily updates. We tested kdb+
    performance with a 1-TB file system. Testing was done within the
    burst limit time periods.
  </fo:block>
  <fo:block line-height="16pt" space-after="9pt" text-align="justify">
    The EFS burst performance is limited to 72 minutes per day for a
    100-GB file system. Subsequent throughput is limited to 5 MB/sec.
  </fo:block>
  <fo:block line-height="16pt" space-after="9pt" text-align="justify">
    
      <fo:external-graphic content-width="scale-down-to-fit" src="url(img/media/image29.png)" width="100%"/>
    
  </fo:block>
  <fo:block line-height="16pt" space-after="9pt" text-align="justify">
    
      <fo:external-graphic content-width="scale-down-to-fit" src="url(img/media/image30.png)" width="100%"/>
    
  </fo:block>
  <fo:table-and-caption margin-right="36pt" margin-bottom="9pt"><fo:table font-size="9pt"><fo:table-header border-bottom-style="solid" border-bottom-width=".5pt" page-break-after="avoid"><fo:table-row><fo:table-cell font-style="italic" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block linefeed-treatment="preserve" text-align="left">
            function
          </fo:block></fo:table-cell><fo:table-cell font-style="italic" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block linefeed-treatment="preserve" text-align="left">
            latency (mSec)
          </fo:block></fo:table-cell><fo:table-cell font-style="italic" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block linefeed-treatment="preserve" text-align="left">
            function
          </fo:block></fo:table-cell><fo:table-cell font-style="italic" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block linefeed-treatment="preserve" text-align="left">
            latency (mSec)
          </fo:block></fo:table-cell></fo:table-row></fo:table-header><fo:table-body><fo:table-row page-break-inside="avoid"><fo:table-cell padding-top="3pt" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block text-align="left">
            <fo:inline font-family="Pragmata Pro">hclose hopen</fo:inline>
          </fo:block></fo:table-cell><fo:table-cell padding-top="3pt" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block text-align="left">
            3.658
          </fo:block></fo:table-cell><fo:table-cell padding-top="3pt" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block text-align="left">
            <fo:inline font-family="Pragmata Pro">();,;2 3</fo:inline>
          </fo:block></fo:table-cell><fo:table-cell padding-top="3pt" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block text-align="left">
            11.64
          </fo:block></fo:table-cell></fo:table-row><fo:table-row page-break-inside="avoid"><fo:table-cell padding-top="3pt" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block text-align="left">
            <fo:inline font-family="Pragmata Pro">hcount</fo:inline>
          </fo:block></fo:table-cell><fo:table-cell padding-top="3pt" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block text-align="left">
            3.059
          </fo:block></fo:table-cell><fo:table-cell padding-top="3pt" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block text-align="left">
            <fo:inline font-family="Pragmata Pro">read1</fo:inline>
          </fo:block></fo:table-cell><fo:table-cell padding-top="3pt" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block text-align="left">
            6.85
          </fo:block></fo:table-cell></fo:table-row></fo:table-body></fo:table></fo:table-and-caption>
  <fo:block line-height="16pt" space-after="9pt" text-align="justify">
    <fo:inline font-size="10pt"><fo:inline font-style="italic">Metadata operational latencies - mSecs
    (headlines)</fo:inline></fo:inline>
  </fo:block>
  <fo:block id="summary-2">
    <fo:block font-weight="bold" font-size="14pt" line-height="17pt" margin-right="36pt" page-break-after="avoid" space-after="6pt" space-before="18pt" text-align="left">Summary</fo:block>
    <fo:block line-height="16pt" space-after="9pt" text-align="justify">
      Note the low rate of streaming read performance, combined with
      very high metadata latencies (1000× that of EBS). The increase in
      transfer rate for many-threaded compressed data indicates that
      there is a capped bandwidth number having some influence on the
      results as well as the operational latency. Consider constraining
      any use of EFS to temporary store and not for runtime data access.
    </fo:block>
  </fo:block>
</fo:block><fo:block id="appendix-c-amazon-storage-gateway-file-mode">
  <fo:block id="idm220866943104" font-size="18pt" line-height="22pt" margin-right="36pt" page-break-before="always" space-after="60pt" text-align="left">Appendix C – Amazon Storage Gateway (File mode)</fo:block>
  <fo:block line-height="16pt" space-after="9pt" text-align="justify">
    Amazon Storage Gateway is a pre-prepared AMI/instance that can be
    provisioned on-demand. It allows you to present an NFS layer to the
    application with S3 as a backing store. The difference between this
    and EFS is that the S3 bucket is owned and named by you. But
    fundamentally the drawback with this approach will be the
    operational latencies. These appear much more significant than the
    latencies gained for the EFS solution, and may reflect the
    communication between the file gateway instance and a single
    declared instance of S3. It is likely that the S3 buckets used by
    EFS are run in a more distributed fashion.
  </fo:block>
  <fo:block line-height="16pt" space-after="9pt" text-align="justify">
    One advantage of AWS Gateway is that it is managed by AWS, it can be
    deployed directly from the AWS console, and incurs no additional
    fees beyond the normal storage costs which is in line with S3.
  </fo:block>
  <fo:block line-height="16pt" space-after="9pt" text-align="justify">
    
      <fo:external-graphic content-width="scale-down-to-fit" src="url(img/media/image31.png)" width="100%"/>
    
  </fo:block>
  <fo:table-and-caption margin-right="36pt" margin-bottom="9pt"><fo:table font-size="9pt"><fo:table-header border-bottom-style="solid" border-bottom-width=".5pt" page-break-after="avoid"><fo:table-row><fo:table-cell font-style="italic" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block linefeed-treatment="preserve" text-align="left">
            function
          </fo:block></fo:table-cell><fo:table-cell font-style="italic" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block linefeed-treatment="preserve" text-align="left">
            latency (mSec)
          </fo:block></fo:table-cell><fo:table-cell font-style="italic" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block linefeed-treatment="preserve" text-align="left">
            function
          </fo:block></fo:table-cell><fo:table-cell font-style="italic" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block linefeed-treatment="preserve" text-align="left">
            latency (mSec)
          </fo:block></fo:table-cell></fo:table-row></fo:table-header><fo:table-body><fo:table-row page-break-inside="avoid"><fo:table-cell padding-top="3pt" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block text-align="left">
            <fo:inline font-family="Pragmata Pro">hclose hopen</fo:inline>
          </fo:block></fo:table-cell><fo:table-cell padding-top="3pt" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block text-align="left">
            3.892
          </fo:block></fo:table-cell><fo:table-cell padding-top="3pt" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block text-align="left">
            <fo:inline font-family="Pragmata Pro">();,;2 3</fo:inline>
          </fo:block></fo:table-cell><fo:table-cell padding-top="3pt" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block text-align="left">
            77.94
          </fo:block></fo:table-cell></fo:table-row><fo:table-row page-break-inside="avoid"><fo:table-cell padding-top="3pt" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block text-align="left">
            <fo:inline font-family="Pragmata Pro">hcount</fo:inline>
          </fo:block></fo:table-cell><fo:table-cell padding-top="3pt" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block text-align="left">
            0.911
          </fo:block></fo:table-cell><fo:table-cell padding-top="3pt" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block text-align="left">
            <fo:inline font-family="Pragmata Pro">read1</fo:inline>
          </fo:block></fo:table-cell><fo:table-cell padding-top="3pt" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block text-align="left">
            7.42
          </fo:block></fo:table-cell></fo:table-row></fo:table-body></fo:table></fo:table-and-caption>
  <fo:block line-height="16pt" space-after="9pt" text-align="justify">
    <fo:inline font-size="10pt"><fo:inline font-style="italic">Metadata operational latencies - mSecs
    (headlines)</fo:inline></fo:inline>
  </fo:block>
  <fo:block id="summary-3">
    <fo:block font-weight="bold" font-size="14pt" line-height="17pt" margin-right="36pt" page-break-after="avoid" space-after="6pt" space-before="18pt" text-align="left">Summary</fo:block>
    <fo:block line-height="16pt" space-after="9pt" text-align="justify">
      The throughput appears to run at about 50% of the line rates
      available, even when run at scale. The AWS gateway exhibits
      significantly high operational latency. This manifests as very
      long wait times when performing an interactive
      <fo:inline font-family="Pragmata Pro">ls -l</fo:inline> command from the root of the file system,
      while the file system is under load, sometimes taking several
      minutes to respond to the directory walk.
    </fo:block>
  </fo:block>
</fo:block><fo:block id="appendix-d-mapr-fs">
  <fo:block id="idm220866924000" font-size="18pt" line-height="22pt" margin-right="36pt" page-break-before="always" space-after="60pt" text-align="left">Appendix D – MapR-FS</fo:block>
  <fo:block background-color="#EEE" font-size="10.5pt" line-height="13pt" margin-bottom="9pt" margin-left="0mm" margin-right="0mm" page-break-inside="avoid" padding-left="3mm" padding-right="3mm" padding-top="3mm"><fo:block margin-bottom="6pt"><fo:inline vertical-align="sub"><fo:inline font-family="Material Icons" font-size="14pt"></fo:inline></fo:inline> <fo:inline font-weight="bold">MapR is qualified with kdb+ </fo:inline></fo:block><fo:block line-height="16pt" space-after="9pt" text-align="justify">
      It offers the full POSIX semantics, including through the NFS
      interface.
    </fo:block></fo:block>
  <fo:block line-height="16pt" space-after="9pt" text-align="justify">
    MapR is a commercial implementation of the Apache Hadoop open-source
    stack. Solutions such as MapR-FS were originally driven by the need
    to support Hadoop clusters alongside high-performance file-system
    capabilities. In this regard, MapR improved on the original HDFS
    implementation found in Hadoop distributions. MapR-FS is a core
    component of their stack. MapR AMIs are freely available on the
    Amazon marketplace.
  </fo:block>
  <fo:block line-height="16pt" space-after="9pt" text-align="justify">
    We installed version 6.0a1 of MapR, using the cloud formation
    templates published in EC2. We used the BYOL licensing model, using
    an evaluation enterprise license. We tested just the enterprise
    version of the NFS service for this test, as we were not able to
    test the POSIX fuse client at the time we went to press.
  </fo:block>
  <fo:block line-height="16pt" space-after="9pt" text-align="justify">
    The reasons for considering something like MapR include:
  </fo:block>
  <fo:list-block provisional-distance-between-starts=".7cm" provisional-label-separation="0.3cm"><fo:list-item><fo:list-item-label end-indent="label-end()"><fo:block line-height="16pt">1.</fo:block></fo:list-item-label><fo:list-item-body start-indent="body-start()">
      <fo:block line-height="16pt" space-after="9pt" text-align="justify">
        Already being familiar with and using MapR in your enterprise,
        so this may already be a candidate or use case when considering
        AWS.
      </fo:block>
    </fo:list-item-body></fo:list-item><fo:list-item><fo:list-item-label end-indent="label-end()"><fo:block line-height="16pt">2.</fo:block></fo:list-item-label><fo:list-item-body start-indent="body-start()">
      <fo:block line-height="16pt" space-after="9pt" text-align="justify">
        You would like to read and write HDB structured data into the
        same file-system service as is used to store unstructured data
        written/read using the HDFS RESTful APIs. This may offer the
        ability to consolidate or run Hadoop and kdb+ analytics
        independently of each other in your organization while sharing
        the same file-system infrastructure.
      </fo:block>
    </fo:list-item-body></fo:list-item></fo:list-block>
  <fo:block line-height="16pt" space-after="9pt" text-align="justify">
    Locking semantics on files passed muster during testing, although
    thorough testing of region or file locking on shared files across
    multiple hosts was not fully tested for the purposes of this report.
  </fo:block>
  <fo:block line-height="16pt" space-after="9pt" text-align="justify">
    
      <fo:external-graphic content-width="scale-down-to-fit" src="url(img/media/image32.png)" width="100%"/>
    
  </fo:block>
  <fo:block line-height="16pt" space-after="9pt" text-align="justify">
    
      <fo:external-graphic content-width="scale-down-to-fit" src="url(img/media/image33.png)" width="100%"/>
    
  </fo:block>
  <fo:block line-height="16pt" space-after="9pt" text-align="justify">
    
      <fo:external-graphic content-width="scale-down-to-fit" src="url(img/media/image34.png)" width="100%"/>
    
  </fo:block>
  <fo:block line-height="16pt" space-after="9pt" text-align="justify">
    
      <fo:external-graphic content-width="scale-down-to-fit" src="url(img/media/image35.png)" width="100%"/>
    
  </fo:block>
  <fo:table-and-caption margin-right="36pt" margin-bottom="9pt"><fo:table font-size="9pt"><fo:table-header border-bottom-style="solid" border-bottom-width=".5pt" page-break-after="avoid"><fo:table-row><fo:table-cell font-style="italic" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block linefeed-treatment="preserve" text-align="left">
            function
          </fo:block></fo:table-cell><fo:table-cell font-style="italic" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block linefeed-treatment="preserve" text-align="left">
            latency (mSec)
          </fo:block></fo:table-cell><fo:table-cell font-style="italic" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block linefeed-treatment="preserve" text-align="left">
            function
          </fo:block></fo:table-cell><fo:table-cell font-style="italic" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block linefeed-treatment="preserve" text-align="left">
            latency (mSec)
          </fo:block></fo:table-cell></fo:table-row></fo:table-header><fo:table-body><fo:table-row page-break-inside="avoid"><fo:table-cell padding-top="3pt" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block text-align="left">
            <fo:inline font-family="Pragmata Pro">hclose hopen</fo:inline>
          </fo:block></fo:table-cell><fo:table-cell padding-top="3pt" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block text-align="left">
            0.447
          </fo:block></fo:table-cell><fo:table-cell padding-top="3pt" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block text-align="left">
            <fo:inline font-family="Pragmata Pro">();,;2 3</fo:inline>
          </fo:block></fo:table-cell><fo:table-cell padding-top="3pt" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block text-align="left">
            6.77
          </fo:block></fo:table-cell></fo:table-row><fo:table-row page-break-inside="avoid"><fo:table-cell padding-top="3pt" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block text-align="left">
            <fo:inline font-family="Pragmata Pro">hcount</fo:inline>
          </fo:block></fo:table-cell><fo:table-cell padding-top="3pt" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block text-align="left">
            0.484
          </fo:block></fo:table-cell><fo:table-cell padding-top="3pt" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block text-align="left">
            <fo:inline font-family="Pragmata Pro">read1</fo:inline>
          </fo:block></fo:table-cell><fo:table-cell padding-top="3pt" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block text-align="left">
            0.768
          </fo:block></fo:table-cell></fo:table-row></fo:table-body></fo:table></fo:table-and-caption>
  <fo:block line-height="16pt" space-after="9pt" text-align="justify">
    <fo:inline font-size="10pt"><fo:inline font-style="italic">Metadata operational latencies - mSecs
    (headlines)</fo:inline></fo:inline>
  </fo:block>
  <fo:block id="summary-4">
    <fo:block font-weight="bold" font-size="14pt" line-height="17pt" margin-right="36pt" page-break-after="avoid" space-after="6pt" space-before="18pt" text-align="left">Summary</fo:block>
    <fo:block line-height="16pt" space-after="9pt" text-align="justify">
      The operational latency of this solution is significantly lower
      than seen with EFS and Storage Gateway, which is good for an
      underlying NFS protocol, but is beaten by WekaIO Matrix.
    </fo:block>
    <fo:block line-height="16pt" space-after="9pt" text-align="justify">
      By way of contrast however, this solution scales very well
      horizontally and vertically when looking at the accumulated
      throughput numbers. It also appears to do very well with random
      reads, however there we are likely to be hitting server-side
      caches in a significant way, so mileage will vary.
    </fo:block>
    <fo:block line-height="16pt" space-after="9pt" text-align="justify">
      We plan to look at the POSIX MapR client in the future.
    </fo:block>
  </fo:block>
</fo:block><fo:block id="appendix-e---goofys">
  <fo:block id="idm220866893840" font-size="18pt" line-height="22pt" margin-right="36pt" page-break-before="always" space-after="60pt" text-align="left">Appendix E - Goofys</fo:block>
  <fo:block line-height="16pt" space-after="9pt" text-align="justify">
    Goofys is an open-source Linux client distribution. It uses an AWS
    S3 storage backend, behind a running and a normal Linux AWS EC2
    instance. It presents a POSIX file system layer to kdb+ using the
    FUSE layer. It is distributed in binary form for RHEL/CentOS and
    others, or can be built from source.
  </fo:block>
  <fo:block line-height="16pt" space-after="9pt" text-align="justify">
    Limitations of the POSIX support are that hard links, symlinks and
    appends are not supported.
  </fo:block>
  <fo:block line-height="16pt" space-after="9pt" text-align="justify">
    
      <fo:external-graphic content-width="scale-down-to-fit" src="url(img/media/image36.png)" width="100%"/>
    
  </fo:block>
  <fo:block line-height="16pt" space-after="9pt" text-align="justify">
    
      <fo:external-graphic content-width="scale-down-to-fit" src="url(img/media/image37.png)" width="100%"/>
    
  </fo:block>
  <fo:table-and-caption margin-right="36pt" margin-bottom="9pt"><fo:table font-size="9pt"><fo:table-header border-bottom-style="solid" border-bottom-width=".5pt" page-break-after="avoid"><fo:table-row><fo:table-cell font-style="italic" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block linefeed-treatment="preserve" text-align="left">
            function
          </fo:block></fo:table-cell><fo:table-cell font-style="italic" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block linefeed-treatment="preserve" text-align="left">
            latency (mSec)
          </fo:block></fo:table-cell><fo:table-cell font-style="italic" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block linefeed-treatment="preserve" text-align="left">
            function
          </fo:block></fo:table-cell><fo:table-cell font-style="italic" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block linefeed-treatment="preserve" text-align="left">
            latency (mSec)
          </fo:block></fo:table-cell></fo:table-row></fo:table-header><fo:table-body><fo:table-row page-break-inside="avoid"><fo:table-cell padding-top="3pt" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block text-align="left">
            <fo:inline font-family="Pragmata Pro">hclose hopen</fo:inline>
          </fo:block></fo:table-cell><fo:table-cell padding-top="3pt" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block text-align="left">
            0.468
          </fo:block></fo:table-cell><fo:table-cell padding-top="3pt" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block text-align="left">
            <fo:inline font-family="Pragmata Pro">();,;2 3</fo:inline>
          </fo:block></fo:table-cell><fo:table-cell padding-top="3pt" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block text-align="left">
            DNF
          </fo:block></fo:table-cell></fo:table-row><fo:table-row page-break-inside="avoid"><fo:table-cell padding-top="3pt" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block text-align="left">
            <fo:inline font-family="Pragmata Pro">hcount</fo:inline>
          </fo:block></fo:table-cell><fo:table-cell padding-top="3pt" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block text-align="left">
            0.405
          </fo:block></fo:table-cell><fo:table-cell padding-top="3pt" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block text-align="left">
            <fo:inline font-family="Pragmata Pro">read1</fo:inline>
          </fo:block></fo:table-cell><fo:table-cell padding-top="3pt" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block text-align="left">
            0.487
          </fo:block></fo:table-cell></fo:table-row></fo:table-body></fo:table></fo:table-and-caption>
  <fo:block line-height="16pt" space-after="9pt" text-align="justify">
    <fo:inline font-size="10pt"><fo:inline font-style="italic">Metadata operational latencies - mSecs
    (headlines)</fo:inline></fo:inline>
  </fo:block>
  <fo:block id="summary-5">
    <fo:block font-weight="bold" font-size="14pt" line-height="17pt" margin-right="36pt" page-break-after="avoid" space-after="6pt" space-before="18pt" text-align="left">Summary</fo:block>
    <fo:block line-height="16pt" space-after="9pt" text-align="justify">
      Operational latency is high. The natural streaming throughput
      seems to hover around 130 MB/sec, or approximately a quarter of
      the EBS rate. The solution thrashes at 16 processes of streaming
      reads. Metadata latency figures are in the order of 100-200×
      higher that of EBS.
    </fo:block>
    <fo:block line-height="16pt" space-after="9pt" text-align="justify">
      The compressed tests show that the bottleneck is per-thread read
      speeds, as the data when decompressed rates improve a lot over the
      uncompressed model.
    </fo:block>
  </fo:block>
</fo:block><fo:block id="appendix-f---s3fs">
  <fo:block id="idm220866872976" font-size="18pt" line-height="22pt" margin-right="36pt" page-break-before="always" space-after="60pt" text-align="left">Appendix F - S3FS</fo:block>
  <fo:block line-height="16pt" space-after="9pt" text-align="justify">
    S3FS is an open-source Linux client software layer that arbitrates
    between the AWS S3 storage layer and each AWS EC2 instance. It
    presents a POSIX file system layer to kdb+.
  </fo:block>
  <fo:block line-height="16pt" space-after="9pt" text-align="justify">
    S3FS uses the Linux user-land FUSE layer. By default, it uses the
    POSIX handle mapped as an S3 object in a one-to-one map. It does not
    use the kernel cache buffer, nor does it use its own caching model
    by default.
  </fo:block>
  <fo:block line-height="16pt" space-after="9pt" text-align="justify">
    Due to S3’s eventual consistency limitations file creation with S3FS
    can occasionally fail.
  </fo:block>
  <fo:block line-height="16pt" space-after="9pt" text-align="justify">
    Metadata operations with this FS are slow. The append function,
    although supported is not usable in a production setting due to the
    massive latency involved.
  </fo:block>
  <fo:block line-height="16pt" space-after="9pt" text-align="justify">
    With multiple kdb+ processes reading, the S3FS service effectively
    stalled.
  </fo:block>
  <fo:block page-break-after="avoid" text-align="center"><fo:external-graphic content-width="scale-down-to-fit" src="url(img/media/image38.png)" width="100%"/></fo:block><fo:block font-size="9pt" font-style="italic" margin-bottom="15pt">s3fs</fo:block>
  <fo:table-and-caption margin-right="36pt" margin-bottom="9pt"><fo:table font-size="9pt"><fo:table-header border-bottom-style="solid" border-bottom-width=".5pt" page-break-after="avoid"><fo:table-row><fo:table-cell font-style="italic" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block linefeed-treatment="preserve" text-align="left">
            function
          </fo:block></fo:table-cell><fo:table-cell font-style="italic" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block linefeed-treatment="preserve" text-align="left">
            latency (mSec)
          </fo:block></fo:table-cell><fo:table-cell font-style="italic" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block linefeed-treatment="preserve" text-align="left">
            function
          </fo:block></fo:table-cell><fo:table-cell font-style="italic" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block linefeed-treatment="preserve" text-align="left">
            latency (mSec)
          </fo:block></fo:table-cell></fo:table-row></fo:table-header><fo:table-body><fo:table-row page-break-inside="avoid"><fo:table-cell padding-top="3pt" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block text-align="left">
            <fo:inline font-family="Pragmata Pro">hclose hopen</fo:inline>
          </fo:block></fo:table-cell><fo:table-cell padding-top="3pt" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block text-align="left">
            7.57
          </fo:block></fo:table-cell><fo:table-cell padding-top="3pt" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block text-align="left">
            <fo:inline font-family="Pragmata Pro">();,;2 3</fo:inline>
          </fo:block></fo:table-cell><fo:table-cell padding-top="3pt" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block text-align="left">
            91.1
          </fo:block></fo:table-cell></fo:table-row><fo:table-row page-break-inside="avoid"><fo:table-cell padding-top="3pt" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block text-align="left">
            <fo:inline font-family="Pragmata Pro">hcount</fo:inline>
          </fo:block></fo:table-cell><fo:table-cell padding-top="3pt" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block text-align="left">
            10.18
          </fo:block></fo:table-cell><fo:table-cell padding-top="3pt" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block text-align="left">
            <fo:inline font-family="Pragmata Pro">read1</fo:inline>
          </fo:block></fo:table-cell><fo:table-cell padding-top="3pt" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block text-align="left">
            12.64
          </fo:block></fo:table-cell></fo:table-row></fo:table-body></fo:table></fo:table-and-caption>
  <fo:block line-height="16pt" space-after="9pt" text-align="justify">
    <fo:inline font-size="10pt"><fo:inline font-style="italic">Metadata operational latencies - mSecs
    (headlines)</fo:inline></fo:inline>
  </fo:block>
</fo:block><fo:block id="appendix-g---s3ql">
  <fo:block id="idm220866854128" font-size="18pt" line-height="22pt" margin-right="36pt" page-break-before="always" space-after="60pt" text-align="left">Appendix G - S3QL</fo:block>
  <fo:block line-height="16pt" space-after="9pt" text-align="justify">
    The code is perhaps the least-referenced open-source S3 gateway
    package, and from a vanilla RHEL 7.3 build we had to add a
    significant number of packages to get to the utility compiled and
    installed. S3QL is written in Python. Significant additions are
    required to build S3QL namely: llfuse, Python3, Cython, Python-pip,
    EPEL and SQlite.
  </fo:block>
  <fo:block line-height="16pt" space-after="9pt" text-align="justify">
    S3QL uses the Python bindings (llfuse) to the Linux user-mode kernel
    FUSE layer. By default, it uses the POSIX handle mapped as an S3
    object in a one-to-one map. S3QL supports only one node sharing one
    subset (directory) tree of one S3 bucket. There is no sharing in
    this model.
  </fo:block>
  <fo:block line-height="16pt" space-after="9pt" text-align="justify">
    Several code exception/faults were seen in Python subroutines of the
    <fo:inline font-family="Pragmata Pro">mkfs.s3ql</fo:inline> utility during initial test so, due to
    time pressures, we will revisit this later.
  </fo:block>
  <fo:block line-height="16pt" space-after="9pt" text-align="justify">
    Although the process exceptions are probably due to a build error,
    and plainly the product does work, this does highlight that the
    build process was unusually complex, due to the nature of so many
    dependencies on other open-source components. This may play as a
    factor in the decision process for selecting solutions.
  </fo:block>
</fo:block><fo:block id="appendix-h---objectivefs">
  <fo:block id="idm220866849856" font-size="18pt" line-height="22pt" margin-right="36pt" page-break-before="always" space-after="60pt" text-align="left">Appendix H - ObjectiveFS</fo:block>
  <fo:block background-color="#EEE" font-size="10.5pt" line-height="13pt" margin-bottom="9pt" margin-left="0mm" margin-right="0mm" page-break-inside="avoid" padding-left="3mm" padding-right="3mm" padding-top="3mm"><fo:block margin-bottom="6pt"><fo:inline vertical-align="sub"><fo:inline font-family="Material Icons" font-size="14pt"></fo:inline></fo:inline> <fo:inline font-weight="bold">ObjectiveFS is qualified with kdb+. </fo:inline></fo:block></fo:block>
  <fo:block line-height="16pt" space-after="9pt" text-align="justify">
    ObjectiveFS is a commercial Linux client/kernel package. It
    arbitrates between S3 storage (each S3 bucket is presented as a FS)
    and each AWS EC2 instance running ObjectiveFS.
  </fo:block>
  <fo:block line-height="16pt" space-after="9pt" text-align="justify">
    It presents a POSIX file system layer to kdb+. This is distinct from
    the EFS NFS service from AWS, which is defined independently from
    the S3 service. With this approach, you pay storage fees only for
    the S3 element, alongside a usage fee for ObjectiveFS.
  </fo:block>
  <fo:block line-height="16pt" space-after="9pt" text-align="justify">
    ObjectiveFS contains a pluggable driver, which allows for
    multithreaded readers to be implemented in kernel mode. This gives
    an increase in the concurrency of the reading of S3 data.
    ObjectiveFS would be installed on each kdb+ node accessing the S3
    bucket containing the HDB data.
  </fo:block>
  <fo:block line-height="16pt" space-after="9pt" text-align="justify">
    ObjectiveFS V5.3.1 is qualified with kdb+. ObjectiveFS achieves
    significantly better performance than EFS. It also has significantly
    better metadata operation latency than all of the EFS and open
    source S3 gateway products. ObjectiveFS also scales aggregate
    bandwidth as more kdb+ nodes use the same S3 bucket. It scales up
    close to linearly for reads, as the number of reader nodes increase,
    since Amazon automatically partitions a bucket across service nodes,
    as needed to support higher request rates.
  </fo:block>
  <fo:block page-break-after="avoid" text-align="center"><fo:external-graphic content-width="scale-down-to-fit" src="url(img/media/image39.png)" width="100%"/></fo:block><fo:block font-size="9pt" font-style="italic" margin-bottom="15pt">ObjectiveFS</fo:block>
  <fo:block page-break-after="avoid" text-align="center"><fo:external-graphic content-width="scale-down-to-fit" src="url(img/media/image40.png)" width="100%"/></fo:block><fo:block font-size="9pt" font-style="italic" margin-bottom="15pt">ObjectiveFS</fo:block>
  <fo:block line-height="16pt" space-after="9pt" text-align="justify">
    This shows that the read rates from the S3 buckets scale well when
    the number of nodes increases. This is more noticeable than the read
    rate seen when measuring the throughput on one node with varying
    numbers of kdb+ processes. Here it remains around the 260 MB/sec
    mark irrespective of the number of kdb+ processes reading.
  </fo:block>
  <fo:block page-break-after="avoid" text-align="center"><fo:external-graphic content-width="scale-down-to-fit" src="url(img/media/image41.png)" width="100%"/></fo:block><fo:block font-size="9pt" font-style="italic" margin-bottom="15pt">ObjectiveFS</fo:block>
  <fo:block line-height="16pt" space-after="9pt" text-align="justify">
    If you select the use of instance local SSD storage as a cache, this
    can accelerate reads of recent data. The instance local cache is
    written around for writes, as these go direct to the S3 bucket. But
    any re-reads of this data would be cached on local disk, local to
    that node. In other words, the same data on multiple client nodes of
    ObjectiveFS would each be copies of the same data. The cache may be
    filled and would be expired in a form of LRU expiry based on the
    access time of a file. For a single node, the read rate from disk
    cache is:
  </fo:block>
  <fo:block page-break-after="avoid" text-align="center"><fo:external-graphic content-width="scale-down-to-fit" src="url(img/media/image42.png)" width="100%"/></fo:block><fo:block font-size="9pt" font-style="italic" margin-bottom="15pt">ObjectiveFS</fo:block>
  <fo:block page-break-after="avoid" text-align="center"><fo:external-graphic content-width="scale-down-to-fit" src="url(img/media/image43.png)" width="100%"/></fo:block><fo:block font-size="9pt" font-style="italic" margin-bottom="15pt">ObjectiveFS</fo:block>
  <fo:table-and-caption margin-right="36pt" margin-bottom="9pt"><fo:table font-size="9pt"><fo:table-header border-bottom-style="solid" border-bottom-width=".5pt" page-break-after="avoid"><fo:table-row><fo:table-cell font-style="italic" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block linefeed-treatment="preserve" text-align="left">
            function
          </fo:block></fo:table-cell><fo:table-cell font-style="italic" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block linefeed-treatment="preserve" text-align="left">
            latency (mSec)
          </fo:block></fo:table-cell><fo:table-cell font-style="italic" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block linefeed-treatment="preserve" text-align="left">
            function
          </fo:block></fo:table-cell><fo:table-cell font-style="italic" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block linefeed-treatment="preserve" text-align="left">
            latency (mSec)
          </fo:block></fo:table-cell></fo:table-row></fo:table-header><fo:table-body><fo:table-row page-break-inside="avoid"><fo:table-cell padding-top="3pt" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block text-align="left">
            <fo:inline font-family="Pragmata Pro">hclose hopen</fo:inline>
          </fo:block></fo:table-cell><fo:table-cell padding-top="3pt" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block text-align="left">
            0.162
          </fo:block></fo:table-cell><fo:table-cell padding-top="3pt" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block text-align="left">
            <fo:inline font-family="Pragmata Pro">();,;2 3</fo:inline>
          </fo:block></fo:table-cell><fo:table-cell padding-top="3pt" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block text-align="left">
            0.175
          </fo:block></fo:table-cell></fo:table-row><fo:table-row page-break-inside="avoid"><fo:table-cell padding-top="3pt" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block text-align="left">
            <fo:inline font-family="Pragmata Pro">hcount</fo:inline>
          </fo:block></fo:table-cell><fo:table-cell padding-top="3pt" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block text-align="left">
            0.088
          </fo:block></fo:table-cell><fo:table-cell padding-top="3pt" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block text-align="left">
            <fo:inline font-family="Pragmata Pro">read1</fo:inline>
          </fo:block></fo:table-cell><fo:table-cell padding-top="3pt" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block text-align="left">
            0.177
          </fo:block></fo:table-cell></fo:table-row></fo:table-body></fo:table></fo:table-and-caption>
  <fo:block line-height="16pt" space-after="9pt" text-align="justify">
    <fo:inline font-size="10pt"><fo:inline font-style="italic">ObjectiveFS metadata operational latencies - mSecs
    (headlines)</fo:inline></fo:inline>
  </fo:block>
  <fo:block line-height="16pt" space-after="9pt" text-align="justify">
    Note that ObjectiveFS encrypts and compresses the S3 objects using
    its own private keys plus your project’s public key. This will
    require a valid license and functioning software for the length of
    time you use this solution in a production setting.
  </fo:block>
  <fo:block id="summary-6">
    <fo:block font-weight="bold" font-size="14pt" line-height="17pt" margin-right="36pt" page-break-after="avoid" space-after="6pt" space-before="18pt" text-align="left">Summary</fo:block>
    <fo:block line-height="16pt" space-after="9pt" text-align="justify">
      This is a simple and elegant solution for the retention of old
      data on a slower, lower cost S3 archive, which can be replicated
      by AWS, geographically or within availability zones. It magnifies
      the generically very low S3 read rates by moving a
      
		‘parallelizing’
	 logic layer into a kernel driver, and
      away from the FUSE layer. It then multithreads the read tasks.
    </fo:block>
    <fo:block line-height="16pt" space-after="9pt" text-align="justify">
      It requires the addition of the ObjectiveFS package on each node
      running kdb+ and then the linking of that system to the target S3
      bucket. This is a very simple process to install, and very easy to
      set up.
    </fo:block>
    <fo:block line-height="16pt" space-after="9pt" text-align="justify">
      For solutions requiring higher throughput and lower latencies, you
      can consider the use of their local caching on instances with
      internal SSD drives, allowing you to reload and cache, at runtime,
      the most recent and most latency sensitive data. This cache can be
      pre-loaded according to a site-specific recipe, and could cover,
      for example, the most recent market data written back to cache,
      even through originally written to S3.
    </fo:block>
    <fo:block line-height="16pt" space-after="9pt" text-align="justify">
      Like some of the other solutions tested, ObjectiveFS does not use
      the kernel block cache. Instead it uses its own memory cache
      mechanism. The amount used by it is defined as a percent of RAM or
      as a fixed size. This allocation is made dynamically.
    </fo:block>
    <fo:block line-height="16pt" space-after="9pt" text-align="justify">
      Therefore attention should be paid to the cases where a kdb+
      writer (e.g. RDB or a TP write-down) is growing its private heap
      space dynamically, as this could extend beyond available space at
      runtime. Reducing the size of the memory cache for ObjectiveFS and
      use of disk cache would mitigate this.
    </fo:block>
  </fo:block>
</fo:block><fo:block id="appendix-i-wekaio-matrix">
  <fo:block id="idm220865726112" font-size="18pt" line-height="22pt" margin-right="36pt" page-break-before="always" space-after="60pt" text-align="left">Appendix I – WekaIO Matrix</fo:block>
  <fo:block background-color="#EEE" font-size="10.5pt" line-height="13pt" margin-bottom="9pt" margin-left="0mm" margin-right="0mm" page-break-inside="avoid" padding-left="3mm" padding-right="3mm" padding-top="3mm"><fo:block margin-bottom="6pt"><fo:inline vertical-align="sub"><fo:inline font-family="Material Icons" font-size="14pt"></fo:inline></fo:inline> <fo:inline font-weight="bold">WekaIO Matrix is qualified with kdb+. </fo:inline></fo:block></fo:block>
  <fo:block line-height="16pt" space-after="9pt" text-align="justify">
    WekaIO Matrix is a commercial product from WekaIO. Version 3.1.2 was
    used for testing. Matrix uses a VFS driver, enabling Weka to support
    POSIX semantics with lockless queues for I/O. The WekaIO POSIX
    system has the same runtime semantics as a local Linux file system.
  </fo:block>
  <fo:block line-height="16pt" space-after="9pt" text-align="justify">
    Matrix provides distributed data protection based on a proprietary
    form of erasure coding. Files are broken up into chunks and spread
    across nodes (or EC2 instances) of the designated Matrix cluster
    (minimum cluster size is six nodes = four data + two parity). The
    data for each chunk of the file is mapped into an erasure-coded
    stripe/chunk that is stored on the node’s direct-attached SSD. EC2
    instances must have local SATA or NVMe based SSDs for storage.
  </fo:block>
  <fo:block line-height="16pt" space-after="9pt" text-align="justify">
    With Matrix, we would anticipate kdb+ to be run in one of two ways.
    Firstly, it can run on the server nodes of the Matrix cluster,
    sharing the same namespace and same compute components. This
    eliminates the need to create an independent file-system
    infrastructure under EC2. Secondly, the kdb+ clients can run on
    clients of the Matrix cluster, the client/server protocol elements
    being included as part of the Matrix solution, being installed on
    both server and client nodes.
  </fo:block>
  <fo:block line-height="16pt" space-after="9pt" text-align="justify">
    One nice feature is that WekaIO tiers its namespace with S3, and
    includes operator selectable tiering rules, and can be based on age
    of file and time in cache, and so on.
  </fo:block>
  <fo:block line-height="16pt" space-after="9pt" text-align="justify">
    The performance is at its best when running from the cluster’s
    erasure-coded SSD tier, exhibiting good metadata operational
    latency.
  </fo:block>
  <fo:block line-height="16pt" space-after="9pt" text-align="justify">
    This product, like others using the same design model, does require
    server and client nodes to dedicate one or more cores (vCPU) to the
    file-system function. These dedicated cores run at 100% of
    capability on that core. This needs to be catered for in your core
    sizing calculations for kdb+, if you are running directly on the
    cluster.
  </fo:block>
  <fo:block line-height="16pt" space-after="9pt" text-align="justify">
    
      <fo:external-graphic content-width="scale-down-to-fit" src="url(img/media/image44.png)" width="100%"/>
    
  </fo:block>
  <fo:block line-height="16pt" space-after="9pt" text-align="justify">
    
      <fo:external-graphic content-width="scale-down-to-fit" src="url(img/media/image45.png)" width="100%"/>
    
  </fo:block>
  <fo:block line-height="16pt" space-after="9pt" text-align="justify">
    
      <fo:external-graphic content-width="scale-down-to-fit" src="url(img/media/image46.png)" width="100%"/>
    
  </fo:block>
  <fo:block line-height="16pt" space-after="9pt" text-align="justify">
    
      <fo:external-graphic content-width="scale-down-to-fit" src="url(img/media/image47.png)" width="100%"/>
    
  </fo:block>
  <fo:block line-height="16pt" space-after="9pt" text-align="justify">
    When forcing the cluster to read from the data expired to S3, we see
    these results:
  </fo:block>
  <fo:block line-height="16pt" space-after="9pt" text-align="justify">
    
      <fo:external-graphic content-width="scale-down-to-fit" src="url(img/media/image48.png)" width="100%"/>
    
  </fo:block>
  <fo:block line-height="16pt" space-after="9pt" text-align="justify">
    
      <fo:external-graphic content-width="scale-down-to-fit" src="url(img/media/image49.png)" width="100%"/>
    
  </fo:block>
  <fo:table-and-caption margin-right="36pt" margin-bottom="9pt"><fo:table font-size="9pt"><fo:table-header border-bottom-style="solid" border-bottom-width=".5pt" page-break-after="avoid"><fo:table-row><fo:table-cell font-style="italic" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block linefeed-treatment="preserve" text-align="left">
            function
          </fo:block></fo:table-cell><fo:table-cell font-style="italic" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block linefeed-treatment="preserve" text-align="left">
            latency (mSec)
          </fo:block></fo:table-cell><fo:table-cell font-style="italic" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block linefeed-treatment="preserve" text-align="left">
            function
          </fo:block></fo:table-cell><fo:table-cell font-style="italic" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block linefeed-treatment="preserve" text-align="left">
            latency (mSec)
          </fo:block></fo:table-cell></fo:table-row></fo:table-header><fo:table-body><fo:table-row page-break-inside="avoid"><fo:table-cell padding-top="3pt" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block text-align="left">
            <fo:inline font-family="Pragmata Pro">hclose hopen</fo:inline>
          </fo:block></fo:table-cell><fo:table-cell padding-top="3pt" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block text-align="left">
            0.555
          </fo:block></fo:table-cell><fo:table-cell padding-top="3pt" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block text-align="left">
            <fo:inline font-family="Pragmata Pro">();,;2 3</fo:inline>
          </fo:block></fo:table-cell><fo:table-cell padding-top="3pt" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block text-align="left">
            3.5
          </fo:block></fo:table-cell></fo:table-row><fo:table-row page-break-inside="avoid"><fo:table-cell padding-top="3pt" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block text-align="left">
            <fo:inline font-family="Pragmata Pro">hcount</fo:inline>
          </fo:block></fo:table-cell><fo:table-cell padding-top="3pt" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block text-align="left">
            0.049
          </fo:block></fo:table-cell><fo:table-cell padding-top="3pt" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block text-align="left">
            <fo:inline font-family="Pragmata Pro">read1</fo:inline>
          </fo:block></fo:table-cell><fo:table-cell padding-top="3pt" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block text-align="left">
            0.078
          </fo:block></fo:table-cell></fo:table-row></fo:table-body></fo:table></fo:table-and-caption>
  <fo:block line-height="16pt" space-after="9pt" text-align="justify">
    <fo:inline font-size="10pt"><fo:inline font-style="italic">WekaIO Matrix metadata operational latencies -
    mSecs (headlines)</fo:inline></fo:inline>
  </fo:block>
  <fo:block id="summary-7">
    <fo:block font-weight="bold" font-size="14pt" line-height="17pt" margin-right="36pt" page-break-after="avoid" space-after="6pt" space-before="18pt" text-align="left">Summary</fo:block>
    <fo:block line-height="16pt" space-after="9pt" text-align="justify">
      Streaming reads running in concert across multiple nodes of the
      cluster achieve 4.6 GB/sec transfer rates, as measured across
      eight nodes running kdb+, and on one file system. What is
      interesting here is to observe there is no decline in scaling rate
      between one and eight nodes. This tested cluster had twelve nodes,
      running within that a 4+2 data protection across these nodes, each
      of instance type <fo:inline font-family="Pragmata Pro">r3.8xlarge</fo:inline> (based on the older
      Intel Ivy Bridge chipset), chosen for its modest SSD disks and not
      for its latest CPU/mem speeds.
    </fo:block>
    <fo:block line-height="16pt" space-after="9pt" text-align="justify">
      Streaming throughput on one client node is 1029 MB/sec
      representing wire speed when considered as a client node. This
      indicates that the data is injected to the host running kdb+ from
      all of the Matrix nodes whilst still constructing sequential data
      from the remaining active nodes in the cluster, across the same
      network.
    </fo:block>
    <fo:block line-height="16pt" space-after="9pt" text-align="justify">
      Metadata operational latency: whilst noticeably worse than EBS, is
      one or two orders of magnitude better than EFS and Storage Gateway
      and all of the open source products.
    </fo:block>
    <fo:block line-height="16pt" space-after="9pt" text-align="justify">
      For the S3 tier, a single kdb+ thread on one node will stream
      reads at 555 MB/sec. This rises to 1596 MB/sec across eight nodes,
      continuing to scale, but not linearly. For eight processes and
      eight nodes throughput maximizes at a reasonable 1251 MB/sec. In a
      real-world setting, you are likely to see a blended figure improve
      with hits coming from the SSDs. The other elements that
      distinguish this solution from others are
      
		‘block-like’
	 low operational latencies for some
      meta-data functions, and good aggregate throughputs for the small
      random reads with kdb+.
    </fo:block>
    <fo:block line-height="16pt" space-after="9pt" text-align="justify">
      For setup and installation, a configuration tool guides users
      through the cluster configuration, and it is pre-configured to
      build out a cluster of standard r3- or i3-series EC2 instances.
      The tool has options for both standard and expert users. The tool
      also provides users with performance and cost information based on
      the options that have been chosen.
    </fo:block>
  </fo:block>
</fo:block><fo:block id="appendix-j-quobyte">
  <fo:block id="idm220865688864" font-size="18pt" line-height="22pt" margin-right="36pt" page-break-before="always" space-after="60pt" text-align="left">Appendix J – Quobyte</fo:block>
  <fo:block background-color="#EEE" font-size="10.5pt" line-height="13pt" margin-bottom="9pt" margin-left="0mm" margin-right="0mm" page-break-inside="avoid" padding-left="3mm" padding-right="3mm" padding-top="3mm"><fo:block margin-bottom="6pt"><fo:inline vertical-align="sub"><fo:inline font-family="Material Icons" font-size="14pt"></fo:inline></fo:inline> <fo:inline font-weight="bold">Quobyte is functionally qualified with kdb+. </fo:inline></fo:block></fo:block>
  <fo:block line-height="16pt" space-after="9pt" text-align="justify">
    Quobyte offers a shared namespace solution based on either
    locally-provisioned or EBS-style storage. It leverages an
    erasure-coding model around nodes of a Quobyte cluster.
  </fo:block>
  <fo:table-and-caption margin-right="36pt" margin-bottom="9pt"><fo:table font-size="9pt"><fo:table-header border-bottom-style="solid" border-bottom-width=".5pt" page-break-after="avoid"><fo:table-row><fo:table-cell font-style="italic" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block linefeed-treatment="preserve" text-align="left">
            test
          </fo:block></fo:table-cell><fo:table-cell font-style="italic" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block linefeed-treatment="preserve" text-align="left">
            result
          </fo:block></fo:table-cell></fo:table-row></fo:table-header><fo:table-body><fo:table-row page-break-inside="avoid"><fo:table-cell padding-top="3pt" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block text-align="left">
            throughput
          </fo:block></fo:table-cell><fo:table-cell padding-top="3pt" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block text-align="left">
            Multiple thread read saturated the ingest bandwidth of each
            <fo:inline font-family="Pragmata Pro">r4.4xlarge</fo:inline> instance running kdb+.
          </fo:block></fo:table-cell></fo:table-row><fo:table-row page-break-inside="avoid"><fo:table-cell padding-top="3pt" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block text-align="left">
            fileops attributes
          </fo:block></fo:table-cell><fo:table-cell padding-top="3pt" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block text-align="left">
            <fo:inline font-style="italic">Test results to follow, please check back at
            code.kx.com for full results.</fo:inline>
          </fo:block></fo:table-cell></fo:table-row></fo:table-body></fo:table></fo:table-and-caption>
</fo:block></fo:flow></fo:page-sequence></fo:root>
