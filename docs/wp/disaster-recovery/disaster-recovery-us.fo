<?xml version="1.0"?>
<fo:root xmlns:fo="http://www.w3.org/1999/XSL/Format" xmlns:db="http://docbook.org/ns/docbook" xmlns:rx="http://www.renderx.com/XSL/Extensions" xmlns:xlink="http://www.w3.org/1999/xlink"><rx:meta-info><rx:meta-field name="author" value="&#10;        Stewart&#10;        Robinson&#10;      "/><rx:meta-field name="creator" value="Kx Systems"/><rx:meta-field name="title" value="Disaster-recovery planning for kdb+ tick systems"/><rx:meta-field name="keywords" value="Kx, Kx Systems, kdb+, disaster, failover, kdb+, planning, recovery, tick"/></rx:meta-info><rx:outline><rx:bookmark internal-destination="what-does-a-kdb-database-look-like-on-disk"><rx:bookmark-label>What does a kdb+ database look like on disk?</rx:bookmark-label></rx:bookmark><rx:bookmark internal-destination="what-is-failover"><rx:bookmark-label>What is failover?</rx:bookmark-label></rx:bookmark><rx:bookmark internal-destination="what-does-hot-hot-mean"><rx:bookmark-label>What does hot-hot mean?</rx:bookmark-label></rx:bookmark><rx:bookmark internal-destination="what-to-do-when"><rx:bookmark-label>What to do when…</rx:bookmark-label></rx:bookmark><rx:bookmark internal-destination="data-reconciliation-and-synchronization"><rx:bookmark-label>Data reconciliation and synchronization</rx:bookmark-label></rx:bookmark><rx:bookmark internal-destination="what-is-the-best-configuration-for-high-availability"><rx:bookmark-label>What is the best configuration for high availability?</rx:bookmark-label></rx:bookmark><rx:bookmark internal-destination="how-does-the-system-recover"><rx:bookmark-label>How does the system recover?</rx:bookmark-label></rx:bookmark><rx:bookmark internal-destination="additional-kdb-features-for-disaster-recovery"><rx:bookmark-label>Additional kdb+ features for disaster recovery</rx:bookmark-label></rx:bookmark><rx:bookmark internal-destination="control-for-kx"><rx:bookmark-label>Control for Kx</rx:bookmark-label></rx:bookmark><rx:bookmark internal-destination="conclusion"><rx:bookmark-label>Conclusion</rx:bookmark-label></rx:bookmark><rx:bookmark internal-destination="appendix"><rx:bookmark-label>Appendix</rx:bookmark-label></rx:bookmark></rx:outline><fo:layout-master-set><fo:simple-page-master master-name="cover-page" page-width="8.5in" page-height="11in"><fo:region-body margin-top="0" margin-bottom="0" margin-left="0" margin-right="0" background-color="#eeeded"/></fo:simple-page-master><fo:simple-page-master master-name="toc" page-width="8.5in" page-height="11in" margin-top="30pt" margin-bottom="30pt" margin-left="45pt" margin-right="45pt"><fo:region-body margin-top="48pt" margin-bottom="32pt" margin-left="60pt" margin-right="60pt"/><fo:region-before extent="30pt"/><fo:region-after extent="30pt"/></fo:simple-page-master><fo:simple-page-master master-name="standard-page" page-width="8.5in" page-height="11in" margin-top="30pt" margin-bottom="30pt" margin-left="45pt" margin-right="45pt"><fo:region-body margin-top="48pt" margin-bottom="32pt" margin-left="40pt" margin-right="40pt"/><fo:region-before extent="30pt"/><fo:region-after extent="30pt"/></fo:simple-page-master></fo:layout-master-set><fo:page-sequence master-reference="cover-page" font-family="STIX2" initial-page-number="1" language="en" country="gb"><fo:flow flow-name="xsl-region-body"><fo:block-container absolute-position="absolute" top="0mm" right="0mm" width="2in" height="2in"><fo:block text-align="right"><fo:external-graphic src="url(/Users/sjt/Projects/kx/github/StephenTaylor-Kx/mkdocs2pdf/img/diamond-white.png)" content-width="40mm" scaling="uniform"/></fo:block></fo:block-container><fo:block-container absolute-position="absolute" bottom="0mm" left="0mm" width="2.5in" height="30mm"><fo:block><fo:external-graphic src="url(/Users/sjt/Projects/kx/github/StephenTaylor-Kx/mkdocs2pdf/img/diamond-bottom-left-white.png)" content-width="50mm" scaling="uniform"/></fo:block></fo:block-container><fo:block-container absolute-position="absolute" bottom="0mm" right="0mm" width="4in" height="50mm"><fo:block text-align="right"><fo:external-graphic src="url(/Users/sjt/Projects/kx/github/StephenTaylor-Kx/mkdocs2pdf/img/kx-cover.png)" content-width="75mm"/></fo:block></fo:block-container><fo:block-container absolute-position="absolute" top="35mm" left="15mm"><fo:block color="#0070cd" font-family="Proxima Nova" font-size="60pt" font-weight="bold" letter-spacing="-4pt">kx</fo:block></fo:block-container><fo:block-container absolute-position="absolute" top="40mm" left="40mm"><fo:block><fo:external-graphic src="url(/Users/sjt/Projects/kx/github/StephenTaylor-Kx/mkdocs2pdf/img/its-about-time.png)"/></fo:block></fo:block-container><fo:block-container absolute-position="absolute" top="80mm" left="35mm"><fo:block font-family="Proxima Nova" color="#0070cd" font-size="18pt" font-weight="400">
							Technical Whitepaper
						</fo:block><fo:block margin-top="9pt" margin-right="30mm" line-height="1.4" font-size="24pt" font-weight="400">Disaster-recovery planning for kdb+ tick systems</fo:block></fo:block-container><fo:block-container absolute-position="absolute" top="170mm" left="35mm"><fo:block line-height="16pt" font-weight="bold">Date</fo:block></fo:block-container><fo:block-container absolute-position="absolute" top="170mm" left="55mm" width="100mm"><fo:block line-height="16pt" text-align="left"/></fo:block-container><fo:block-container absolute-position="absolute" top="180mm" left="35mm"><fo:block line-height="16pt" font-weight="bold">Author</fo:block></fo:block-container><fo:block-container absolute-position="absolute" top="180mm" left="55mm" width="100mm"><fo:block line-height="16pt" text-align="left"><fo:block line-height="16pt" space-after="9pt" text-align="justify">
        Stewart Robinson is a kdb+ consultant currently based in
        Belfast. He has developed various kdb+ tick applications,
        focusing on market surveillance, for some of the world’s largest
        financial institutions.
      </fo:block></fo:block></fo:block-container><fo:block> </fo:block></fo:flow></fo:page-sequence><fo:page-sequence master-reference="toc" font-family="STIX2" language="en" country="gb"><fo:static-content flow-name="xsl-region-before"><fo:block text-align-last="justify" font-family="Proxima Nova" color="gray"><fo:inline font-size="9pt">Disaster-recovery planning for kdb+ tick systems</fo:inline><fo:leader leader-pattern="space"/><fo:inline color="#0070cd" font-size="18pt" font-weight="bold" letter-spacing="-2pt">kx</fo:inline></fo:block></fo:static-content><fo:static-content flow-name="xsl-region-after"><fo:block color="gray" text-align="right"><fo:inline font-family="Proxima Nova" font-size="9pt" letter-spacing="2pt"><fo:page-number/></fo:inline></fo:block></fo:static-content><fo:flow flow-name="xsl-region-body"><fo:block break-before="page"><fo:block font-size="14pt" text-align="center" margin-top="36pt" margin-bottom="36pt">Contents</fo:block><fo:block margin-bottom="6pt" text-align-last="justify"><fo:basic-link internal-destination="idm106820806464">What does a kdb+ database look like on disk? <fo:leader leader-pattern="dots"/> <fo:page-number-citation ref-id="idm106820806464"/></fo:basic-link></fo:block><fo:block margin-bottom="6pt" text-align-last="justify"><fo:basic-link internal-destination="idm106820785232">What is failover? <fo:leader leader-pattern="dots"/> <fo:page-number-citation ref-id="idm106820785232"/></fo:basic-link></fo:block><fo:block margin-bottom="6pt" text-align-last="justify"><fo:basic-link internal-destination="idm106820779232">What does hot-hot mean? <fo:leader leader-pattern="dots"/> <fo:page-number-citation ref-id="idm106820779232"/></fo:basic-link></fo:block><fo:block margin-bottom="6pt" text-align-last="justify"><fo:basic-link internal-destination="idm106820759024">What to do when… <fo:leader leader-pattern="dots"/> <fo:page-number-citation ref-id="idm106820759024"/></fo:basic-link></fo:block><fo:block margin-bottom="6pt" text-align-last="justify"><fo:basic-link internal-destination="idm106820716352">Data reconciliation and synchronization <fo:leader leader-pattern="dots"/> <fo:page-number-citation ref-id="idm106820716352"/></fo:basic-link></fo:block><fo:block margin-bottom="6pt" text-align-last="justify"><fo:basic-link internal-destination="idm106820711648">What is the best configuration for high availability? <fo:leader leader-pattern="dots"/> <fo:page-number-citation ref-id="idm106820711648"/></fo:basic-link></fo:block><fo:block margin-bottom="6pt" text-align-last="justify"><fo:basic-link internal-destination="idm106820706512">How does the system recover? <fo:leader leader-pattern="dots"/> <fo:page-number-citation ref-id="idm106820706512"/></fo:basic-link></fo:block><fo:block margin-bottom="6pt" text-align-last="justify"><fo:basic-link internal-destination="idm106820701792">Additional kdb+ features for disaster recovery <fo:leader leader-pattern="dots"/> <fo:page-number-citation ref-id="idm106820701792"/></fo:basic-link></fo:block><fo:block margin-bottom="6pt" text-align-last="justify"><fo:basic-link internal-destination="idm106820697424">Control for Kx <fo:leader leader-pattern="dots"/> <fo:page-number-citation ref-id="idm106820697424"/></fo:basic-link></fo:block><fo:block margin-bottom="6pt" text-align-last="justify"><fo:basic-link internal-destination="idm106820693408">Conclusion <fo:leader leader-pattern="dots"/> <fo:page-number-citation ref-id="idm106820693408"/></fo:basic-link></fo:block><fo:block margin-bottom="6pt" text-align-last="justify"><fo:basic-link internal-destination="idm106820685728">Appendix <fo:leader leader-pattern="dots"/> <fo:page-number-citation ref-id="idm106820685728"/></fo:basic-link></fo:block></fo:block></fo:flow></fo:page-sequence><fo:page-sequence master-reference="standard-page" font-family="STIX2" language="en" country="gb"><fo:static-content flow-name="xsl-region-before"><fo:block text-align-last="justify" font-family="Proxima Nova" color="gray"><fo:inline font-size="9pt">Disaster-recovery planning for kdb+ tick systems</fo:inline><fo:leader leader-pattern="space"/><fo:inline color="#0070cd" font-size="18pt" font-weight="bold" letter-spacing="-2pt">kx</fo:inline></fo:block></fo:static-content><fo:static-content flow-name="xsl-footnote-separator"><fo:block><fo:leader leader-pattern="rule" rule-thickness=".5pt" leader-length="50%"/></fo:block></fo:static-content><fo:static-content flow-name="xsl-region-after"><fo:block color="gray" text-align="right"><fo:inline font-family="Proxima Nova" font-size="9pt" letter-spacing="2pt"><fo:page-number/></fo:inline></fo:block></fo:static-content><fo:flow flow-name="xsl-region-body"><fo:block id="disaster-recovery-planning-for-kdb-tick-systems">
  <fo:block id="idm106820824624" font-size="18pt" line-height="22pt" margin-right="36pt" page-break-before="always" space-after="60pt" text-align="left">Disaster-recovery planning for kdb+ tick systems</fo:block>
  <fo:block line-height="16pt" space-after="9pt" text-align="justify">
    Disasters are inevitable; hardware failure, network problems, and
    data corruption are all events that could play havoc with a system.
    If these events are not fully understood and planned for they may
    lead to significant downtime and potentially severe business impact,
    including revenue loss, legal and financial implications, and impact
    to business reputation and brand. Recent high-profile systems
    failures of a large cloud computing provider and an international
    airline highlight the importance for IT systems to have a
    comprehensive disaster recovery plan in place.
  </fo:block>
  <fo:block line-height="16pt" space-after="9pt" text-align="justify">
    This whitepaper discusses disaster recovery (DR) and failover
    concepts from the perspective of the gateway layer accessing a
    typical kdb+ tick system used in capital-markets applications. The
    end goal of constructing this plan is to ensure high availability of
    the application via the gateway where possible, considering all
    conceivable failure scenarios and outlining any actions required to
    prevent data loss, minimize any downtime and keep the application
    accessible.
  </fo:block>
  <fo:block line-height="16pt" space-after="9pt" text-align="justify">
    
      <fo:external-graphic content-width="scale-down-to-fit" src="url(img/image3.jpeg)" width="100%"/>
    
  </fo:block>
  <fo:block line-height="16pt" space-after="9pt" text-align="justify">
    As part of business-continuity and DR planning, the RTO, or Recovery
    Time Objective, is the maximum target time set for recovery of the
    systems after a disaster has struck. How quickly the system needs to
    recover can dictate the type of preparations required and determine
    the overall budget you should assign to business continuity. If for
    example, the RTO is three hours, meaning your business can only
    survive with systems down for this amount of time, then you will
    need to ensure a high level of preparation and a higher budget to
    ensure that systems can be recovered quickly. On the other hand, if
    the RTO is two weeks, then you can probably budget less and invest
    in less advanced solutions.
  </fo:block>
  <fo:block line-height="16pt" space-after="9pt" text-align="justify">
    The RPO, or Recovery Point Objective, is the maximum targeted period
    in which data might be lost from the system due to a major incident.
    Alternatively, the RPO is the maximum tolerable period in which data
    might be lost from a critical IT service or system.
  </fo:block>
  <fo:block line-height="16pt" space-after="9pt" text-align="justify">
    A disaster recovery plan is usually based on requirements from both
    the RTO and RPO specifications which can guide the design of a
    cost-effective solution. However, every system has its own unique
    requirements and challenges; therefore, this paper does not propose
    a 
		‘one-size-fits-all’
	 approach. Rather, it suggests the
    best practice methods for dealing with the various possible failures
    one needs to be aware of and plan for when building a kdb+
    tick-based system. This paper does not deal with many failure
    conditions of a kdb+ system outside of the ticker plant model used
    in capital markets.
  </fo:block>
  <fo:block line-height="16pt" space-after="9pt" text-align="justify">
    Kdb+ tick is an architecture which allows the capturing, processing
    and querying of data in real time and historically. It typically
    consists of:
  </fo:block>
  <fo:block><fo:block page-break-after="avoid" margin-bottom="6pt">
        Feedhandler/s
      </fo:block></fo:block><fo:block margin-left="10mm" page-break-inside="avoid" margin-bottom="3pt">
        <fo:block line-height="16pt" space-after="9pt" text-align="justify">
          a process used to capture external data and translate it into
          kdb+
        </fo:block>
      </fo:block><fo:block><fo:block page-break-after="avoid" margin-bottom="6pt">
        Tickerplant
      </fo:block></fo:block><fo:block margin-left="10mm" page-break-inside="avoid" margin-bottom="3pt">
        <fo:block line-height="16pt" space-after="9pt" text-align="justify">
          a specialized kdb+ process that operates as a link between the
          client’s data feed and a number of subscribers. It receives
          data from the feedhandler, appends a time stamp to it and
          saves it to a log file. It publishes this data to a real-time
          database and any clients which have subscribed to it and then
          purges its tables of data. In this way, the tickerplant uses
          very little memory, whilst a full record of intra-day data is
          maintained in the real-time database.
        </fo:block>
      </fo:block><fo:block><fo:block page-break-after="avoid" margin-bottom="6pt">
        Real-time Database (RDB)
      </fo:block></fo:block><fo:block margin-left="10mm" page-break-inside="avoid" margin-bottom="3pt">
        <fo:block line-height="16pt" space-after="9pt" text-align="justify">
          holds all the intra-day data in memory to allow for fast,
          powerful queries.
        </fo:block>
      </fo:block><fo:block><fo:block page-break-after="avoid" margin-bottom="6pt">
        Historical Database (HDB)
      </fo:block></fo:block><fo:block margin-left="10mm" page-break-inside="avoid" margin-bottom="3pt">
        <fo:block line-height="16pt" space-after="9pt" text-align="justify">
          consists of on-disk kdb+ data, typically split into date
          partitions. A q process can read this data and memory-map it,
          allowing for fast queries across a large volume of data. The
          RDB is instructed to save its data to the HDB at EOD (end of
          day).
        </fo:block>
      </fo:block>
  <fo:block line-height="16pt" space-after="9pt" text-align="justify">
    <fo:inline font-family="Material Icons" font-size="14pt"></fo:inline> Whitepaper:
    <fo:basic-link external-destination="url('http://code.kx.com/q/wp/data_recovery_for_kdb_tick.pdf')"><fo:inline color="#0070cd">Data Recovery
    for kdb+ tick</fo:inline></fo:basic-link><fo:footnote><fo:inline font-size="8pt" alignment-baseline="hanging">1</fo:inline><fo:footnote-body><fo:block font-size="8pt">1.
									http://code.kx.com/q/wp/data_recovery_for_kdb_tick.pdf</fo:block></fo:footnote-body></fo:footnote>
  </fo:block>
  <fo:block id="what-does-a-kdb-database-look-like-on-disk">
    <fo:block id="idm106820806464" font-size="18pt" line-height="22pt" margin-right="36pt" page-break-before="always" space-after="60pt" text-align="left">What does a kdb+ database look like on disk?</fo:block>
    <fo:block line-height="16pt" space-after="9pt" text-align="justify">
      Before we consider creating a disaster-recovery plan, we first
      must understand the layout of a typical kdb+ database on-disk.
      Kdb+ databases are stored as a series of files and directories on
      disk. This makes handling databases extremely easy because
      database files can be manipulated as operating system files.
      Backing up a kdb+ database can therefore be implemented by using
      any standard file-system backup utility. This is a key difference
      from traditional databases, which have their own backup utilities
      and do not allow direct access to the database files.
    </fo:block>
    <fo:block line-height="16pt" space-after="9pt" text-align="justify">
      Kdb+’s use of the native file system is also reflected in the way
      it uses standard operating-system features for accessing data
      (memory-mapped files), whereas traditional databases use
      proprietary techniques in an effort to speed up the reading and
      writing processes. The typical kdb+ database layout for a
      tick-based system is partitioned by date, although integer
      partitioning is also possible. Below is an example of an on-disk
      layout with date partitions as represented using the Linux program
      <fo:inline font-family="Pragmata Pro">tree</fo:inline>.
    </fo:block>
    <fo:block page-break-after="avoid" text-align="center"><fo:external-graphic content-width="scale-down-to-fit" src="url(img/image2.png)" width="100%"/></fo:block><fo:block font-size="9pt" font-style="italic" margin-bottom="15pt">Filesystem tree</fo:block>
    <fo:block line-height="16pt" space-after="9pt" text-align="justify">
      The example above displays the various file-system entities which
      constitute a kdb+ database. Highlighted in bold are directories,
      which can be either a partition or a splayed table directory. The
      splayed table directories contain individual column files and a
      kdb+ <fo:inline font-family="Pragmata Pro">.d</fo:inline> file which contains the column
      ordering. A file called <fo:inline font-family="Pragmata Pro">sym</fo:inline> also exists at the
      top level and is used for enumerations of table columns of symbol
      type.
    </fo:block>
    <fo:block line-height="16pt" space-after="9pt" text-align="justify">
      Splayed or flat tables can also exist in the database at the top
      level but are excluded here for simplicity.
    </fo:block>
    <fo:block line-height="16pt" space-after="9pt" text-align="justify">
      The standard approach to saving data to an on-disk database (HDB)
      is to use the <fo:inline font-family="Pragmata Pro">.Q.dpft</fo:inline> utility, usually from an
      in-memory real-time database via an end-of-day write-down function
      (see <fo:inline font-family="Pragmata Pro">.u.end</fo:inline> in <fo:inline font-family="Pragmata Pro">r.q</fo:inline>). This
      utility creates the files and directories as laid out in the table
      above, along with applying a <fo:inline font-style="italic">parted</fo:inline> attribute
      to the specified column.
    </fo:block>
    <fo:block font-family="Pragmata Pro" font-size="10pt" margin-bottom="12pt" margin-left="10pt" page-break-inside="avoid" white-space="pre">
.Q.dpft[`:/kx/hdb;2017.01.01;`sym;`trade]
</fo:block>
    <fo:block line-height="16pt" space-after="9pt" text-align="justify">
      However, the constituent parts of the utility can be run
      individually and used to construct or amend a database. The basic
      steps are:
    </fo:block>
    <fo:list-block provisional-distance-between-starts=".7cm" provisional-label-separation="0.3cm"><fo:list-item><fo:list-item-label end-indent="label-end()"><fo:block line-height="16pt">1.</fo:block></fo:list-item-label><fo:list-item-body start-indent="body-start()">
        <fo:block line-height="16pt" space-after="9pt" text-align="justify">
          Enumerate the table to be saved against the sym file
          (<fo:inline font-family="Pragmata Pro">.Q.en</fo:inline>)
        </fo:block>
      </fo:list-item-body></fo:list-item><fo:list-item><fo:list-item-label end-indent="label-end()"><fo:block line-height="16pt">2.</fo:block></fo:list-item-label><fo:list-item-body start-indent="body-start()">
        <fo:block line-height="16pt" space-after="9pt" text-align="justify">
          Save enumerated table splayed to date partition. Parent
          directories are created by set if they do not yet exist.
        </fo:block>
      </fo:list-item-body></fo:list-item></fo:list-block>
    <fo:block line-height="16pt" space-after="9pt" text-align="justify">
      Note that as part of setting a splayed table, the column
      <fo:inline font-family="Pragmata Pro">.d</fo:inline> file is automatically written by kdb+. After
      these steps, the database can now be loaded into q via a simple
      <fo:inline font-family="Pragmata Pro">\l</fo:inline> command.
    </fo:block>
    <fo:block line-height="16pt" space-after="9pt" text-align="justify">
      These step-by-step methods can become useful for fixing issues
      that may occur on specific files or date partitions. For example,
      if we need to reorder our table columns, or we have a schema
      change, then the <fo:inline font-family="Pragmata Pro">.d</fo:inline> file can be overwritten
      with a simple <fo:inline font-family="Pragmata Pro">set</fo:inline>.
    </fo:block>
    <fo:block font-family="Pragmata Pro" font-size="10pt" margin-bottom="12pt" margin-left="10pt" page-break-inside="avoid" white-space="pre">
`:/kx/hdb/2017.01.01/trade/.d set `time`sym`price`size`side`ex
</fo:block>
    <fo:block line-height="16pt" space-after="9pt" text-align="justify">
      The
      <fo:basic-link external-destination="url('https://github.com/KxSystems/kdb/blob/master/utils/dbmaint.md')"><fo:inline color="#0070cd"><fo:inline font-family="Pragmata Pro">dbmaint.q</fo:inline>
      script</fo:inline></fo:basic-link><fo:footnote><fo:inline font-size="8pt" alignment-baseline="hanging">2</fo:inline><fo:footnote-body><fo:block font-size="8pt">2.
									https://github.com/KxSystems/kdb/blob/master/utils/dbmaint.md</fo:block></fo:footnote-body></fo:footnote>) provides some useful utilities for editing and
      maintaining a historical database (HDB). Generally, these
      functions are safer and should be used in place of the raw
      commands for any database amendments.
    </fo:block>
    <fo:block line-height="16pt" space-after="9pt" text-align="justify">
      In the event that a database needs to be restored to the latest
      backup version, it is as simple as restoring the file-system
      entities from the backup location and reloading any HDB processes
      via the load command.
    </fo:block>
    <fo:block font-family="Pragmata Pro" font-size="10pt" margin-bottom="12pt" margin-left="10pt" page-break-inside="avoid" white-space="pre">
system "l ."
</fo:block>
  </fo:block>
  <fo:block id="what-is-failover">
    <fo:block id="idm106820785232" font-size="18pt" line-height="22pt" margin-right="36pt" page-break-before="always" space-after="60pt" text-align="left">What is failover?</fo:block>
    <fo:block font-size="10.5pt" margin-top="6pt" margin-bottom="6pt" margin-left="5mm">
      <fo:block line-height="16pt" space-after="9pt" text-align="justify">
        Failover is the capability to switch over to a redundant or
        standby server, system or network upon the failure or
        termination of an existing asset. Failover should happen without
        any kind of human intervention or warning. —
        <fo:basic-link external-destination="url('http://www.computerweekly.com/feature/Idea-lab')"><fo:inline color="#0070cd"><fo:inline font-style="italic">Computer
        Weekly</fo:inline></fo:inline></fo:basic-link><fo:footnote><fo:inline font-size="8pt" alignment-baseline="hanging">3</fo:inline><fo:footnote-body><fo:block font-size="8pt">3.
									http://www.computerweekly.com/feature/Idea-lab</fo:block></fo:footnote-body></fo:footnote>
      </fo:block>
    </fo:block>
    <fo:block line-height="16pt" space-after="9pt" text-align="justify">
      In a kdb+ tick setup, failover can be designed and configured at a
      whole system level or at a process-by- process level. Generally,
      if a critical process goes down and does not restart then the
      entire system will failover to a mirrored secondary or backup
      system. However, if a non-critical process goes down and does not
      restart then it may be judged unnecessary to failover the entire
      system to the secondary server. The mirrored process on the
      secondary server may be able to take over from the failed process.
    </fo:block>
    <fo:block line-height="16pt" space-after="9pt" text-align="justify">
      Alternatively, in the case where there are multiple processes in a
      group (e.g. HDBs) on the primary side for load balancing reasons
      all accessing the same data, then the remaining processes within
      the group will simply pick up the slack after the failure and no
      switching would be required. However, the issue of failure must be
      investigated and fixed to restore full functionality of the
      system.
    </fo:block>
    <fo:block line-height="16pt" space-after="9pt" text-align="justify">
      In all the various combinations of failover operations that can be
      designed, the end goal is always to maintain availability of the
      application and minimize any disruption to the business.
    </fo:block>
  </fo:block>
  <fo:block id="what-does-hot-hot-mean">
    <fo:block id="idm106820779232" font-size="18pt" line-height="22pt" margin-right="36pt" page-break-before="always" space-after="60pt" text-align="left">What does hot-hot mean?</fo:block>
    <fo:block line-height="16pt" space-after="9pt" text-align="justify">
      In a production environment, some level of redundancy is always
      required. Depending on the use case, requirements may vary but in
      nearly all instances requiring high availability, the best option
      is to have a hot-hot (or 
		‘active-active’
	)
      configuration.
    </fo:block>
    <fo:block line-height="16pt" space-after="9pt" text-align="justify">
      <fo:inline font-style="italic">Hot-hot</fo:inline> is the term for an identical mirrored
      secondary system running, separate to the primary system,
      capturing and storing data but also serving client queries. In a
      system with a secondary server available, hot-hot is the typical
      configuration as it is sensible to use all available hardware to
      maximize operational performance.
    </fo:block>
    <fo:block line-height="16pt" space-after="9pt" text-align="justify">
      Alternative configurations include:
    </fo:block>
    <fo:block><fo:block page-break-after="avoid" margin-bottom="6pt">
          Hot-warm
        </fo:block></fo:block><fo:block margin-left="10mm" page-break-inside="avoid" margin-bottom="3pt">
          <fo:block line-height="16pt" space-after="9pt" text-align="justify">
            The secondary system captures data but does not serve
            queries
          </fo:block>
        </fo:block><fo:block><fo:block page-break-after="avoid" margin-bottom="6pt">
          Hot-cold
        </fo:block></fo:block><fo:block margin-left="10mm" page-break-inside="avoid" margin-bottom="3pt">
          <fo:block line-height="16pt" space-after="9pt" text-align="justify">
            The secondary system has a complete backup or copy of the
            primary system at some previous point in time (recall that
            kdb+ databases are just a series of operating system files
            and directories) with no live processes running. A failover
            in this scenario involves restoring from this latest backup,
            with the understanding that there may be some data loss
            between the time of failover to the time the latest backup
            was made.
          </fo:block>
        </fo:block><fo:block><fo:block page-break-after="avoid" margin-bottom="6pt">
          Pilot light (or cold hot-warm)
        </fo:block></fo:block><fo:block margin-left="10mm" page-break-inside="avoid" margin-bottom="3pt">
          <fo:block line-height="16pt" space-after="9pt" text-align="justify">
            The secondary is on standby and the entire system can
            quickly be started to allow recovery in a shorter time
            period than a hot-cold configuration.
          </fo:block>
        </fo:block>
    <fo:block line-height="16pt" space-after="9pt" text-align="justify">
      Typically kdb+ is deployed in a high-value system; hence, downtime
      can impact business, which justifies the hot-hot setup to ensure
      high availability.
    </fo:block>
    <fo:block line-height="16pt" space-after="9pt" text-align="justify">
      Usually, the secondary will run on completely separate
      infrastructure, with a separate file system, and save the data to
      a secondary database directory, separate from the primary. In this
      way, if the primary system or underlying infrastructure goes
      offline, the secondary would be able to take over completely.
    </fo:block>
    <fo:block line-height="16pt" space-after="9pt" text-align="justify">
      The following diagram shows the basic kdb+ tick components, with
      the primary (A) processes in green and the secondary (B) processes
      in red. In addition to the standard tick processes shown below,
      there could be a combination of gateways, Complex Event Processor
      engines (CEPs) or query router processes. These generally provide
      various methods to carefully manage any querying of the database.
    </fo:block>
    <fo:block line-height="16pt" space-after="9pt" text-align="justify">
      
        <fo:external-graphic content-width="scale-down-to-fit" src="url(img/image12.jpeg)" width="100%"/>
      
    </fo:block>
    <fo:block line-height="16pt" space-after="9pt" text-align="justify">
      <fo:inline font-family="Material Icons" font-size="14pt"></fo:inline> Whitepaper –
      <fo:basic-link external-destination="url('http://code.kx.com/q/wp/query_routing_a_kdb_framework_for_a_scalable_load_balanced_system.pdf')"><fo:inline color="#0070cd">Query
      Routing: a kdb+ framework for a scalable load-balanced
      system</fo:inline></fo:basic-link><fo:footnote><fo:inline font-size="8pt" alignment-baseline="hanging">4</fo:inline><fo:footnote-body><fo:block font-size="8pt">4.
									http://code.kx.com/q/wp/query_routing_a_kdb_framework_for_a_scalable_load_balanced_system.pdf</fo:block></fo:footnote-body></fo:footnote>
    </fo:block>
    <fo:block line-height="16pt" space-after="9pt" text-align="justify">
      As mentioned above, the usual strategy for failover is to have a
      complete mirror of the production system (feedhandler,
      tickerplant, and real-time subscriber), and when any critical
      process goes down, the secondary is able to take over. Switching
      from production to disaster recovery systems can be implemented
      seamlessly using kdb+
      <fo:basic-link external-destination="url('http://code.kx.com/q/cookbook/ipc/')"><fo:inline color="#0070cd">interprocess
      communication</fo:inline></fo:basic-link><fo:footnote><fo:inline font-size="8pt" alignment-baseline="hanging">5</fo:inline><fo:footnote-body><fo:block font-size="8pt">5.
									http://code.kx.com/q/cookbook/ipc/</fo:block></fo:footnote-body></fo:footnote> (via <fo:inline font-family="Pragmata Pro">.z.pc</fo:inline>).
    </fo:block>
    <fo:block line-height="16pt" space-after="9pt" text-align="justify">
      An alternative method is to have a parent control process
      monitoring everything within the system with heart-beating: a
      simple message sent from the control process to another asking
      <fo:inline font-style="italic">Are you alive?</fo:inline> In the event that a process is
      unreachable, the parent control process signals failover of the
      whole system to the secondary. This is obviously more complex than
      using <fo:inline font-family="Pragmata Pro">.z.pc</fo:inline> between system processes to trigger
      the failover, but the advantage is that the whole system can be
      controlled from a single process pair (primary and secondary).
    </fo:block>
    <fo:block line-height="16pt" space-after="9pt" text-align="justify">
      While there are other ways to provide additional backup, there is
      generally no real alternative to having a parallel collection
      system for a high-availability kdb+ tick solution.
    </fo:block>
    <fo:block line-height="16pt" space-after="9pt" text-align="justify">
      In designing the disaster-recovery plan, all possible failure
      events should be identified, assessed in terms of overall risk,
      impact and likelihood of occurrence, and planned for. The
      following section considers the options available for the various
      kdb+ tick system components within this simplified architecture.
    </fo:block>
  </fo:block>
  <fo:block id="what-to-do-when">
    <fo:block id="idm106820759024" font-size="18pt" line-height="22pt" margin-right="36pt" page-break-before="always" space-after="60pt" text-align="left">What to do when…</fo:block>
    <fo:block id="a-real-time-database-goes-down">
      <fo:block font-weight="bold" font-size="14pt" line-height="17pt" margin-right="36pt" page-break-after="avoid" space-after="6pt" space-before="18pt" text-align="left">…a real-time database goes down</fo:block>
      <fo:block line-height="16pt" space-after="9pt" text-align="justify">
        It is important to design the system so it is resilient and
        fault-tolerant, including adequately specifying the memory
        requirements for the RDB and the system as a whole.
        Considerations for determining these requirements include the
        peak message rate, expected daily data volumes, schema width,
        query load and complexity and many others.
      </fo:block>
      <fo:block line-height="16pt" space-after="9pt" text-align="justify">
        If a real-time database does go down, then it is usual practice
        to trigger a failover to the secondary RDB, such that the
        gateway will only route queries to the secondary RDB while the
        primary is offline or recovering. Failover can be signaled via
        <fo:inline font-family="Pragmata Pro">.z.pc</fo:inline>. Usually, the handles to the secondary
        RDB are opened prior to any failover occurring, and it is simply
        a case of pointing the query to the secondary RDB handle, in
        place of the primary RDB handle. In larger applications, an
        application delivery controller (ADC) is often used to control
        the routing of queries between the application and the client.
        It is up to the system architect to decide if the secondary RDB
        should continue to serve queries when the primary is available
        again, or revert to using the primary RDB again.
      </fo:block>
      <fo:block line-height="16pt" space-after="9pt" text-align="justify">
        A failback process includes data recovery of any data loss
        during the downtime and restoring the operations back to a
        pre-failed over state, i.e. primary and secondary systems are
        running and available. Generally, it is recommended that
        fail-back to the primary be scheduled during a downtime window.
        This is to ensure sufficient time to validate that data is
        synchronized, functional and that processing is fully restored
        at the primary system. There are some applications where you may
        not have maintenance or downtime windows available for
        fail-back. If this is the case the restoration of the primary
        must occur in parallel with normal operations on the secondary
        system and the data synchronization, reconciliation and
        fail-back processes automated.
      </fo:block>
      <fo:block line-height="16pt" space-after="9pt" text-align="justify">
        To recover the primary RDB it should be restarted either
        manually, or using system tools, but it will have lost all the
        intra-day data published by the tickerplant so far for the day.
        To recover this intra-day data and regain a subscription to the
        data feed on restart, the RDB sends a subscription request to
        the tickerplant which returns a message containing the location
        of the log file and the number of lines to read. The RDB replays
        the number of lines specified from the log file, storing the
        results in memory. In this way, the RDB has recovered all
        tickerplant data up to the line number received at the start of
        replay. If required to recover other non-tick data, recovery
        steps should be built into the initial state function, which
        would run during the start-up of the RDB and with this data
        being loaded from the specified source.
      </fo:block>
      <fo:block line-height="16pt" space-after="9pt" text-align="justify">
        The restarted RDB receives all subsequent updates from the
        tickerplant. If updates arrive whilst the RDB is reading from
        the log file, they are buffered in the TCP/IP buffer. If the log
        file is large enough, then the replay may take enough time that
        the buffers fill. In this case, the messages are kept in the
        main tickerplant process memory (this can be seen in
        <fo:inline font-family="Pragmata Pro">.z.W</fo:inline> in the TP).
      </fo:block>
      <fo:block line-height="16pt" space-after="9pt" text-align="justify">
        Replaying the log can potentially take several minutes or more,
        depending on the size of the tickerplant log file, therefore the
        gateway queries should continue to be routed to the secondary
        until recovery and failover are complete, and the primary RDB is
        available to capture data and serve queries again.
      </fo:block>
      <fo:block line-height="16pt" space-after="9pt" text-align="justify">
        <fo:inline font-family="Material Icons" font-size="14pt"></fo:inline> Whitepaper
        <fo:basic-link external-destination="url('http://code.kx.com/q/wp/data_recovery_for_kdb_tick.pdf')"><fo:inline color="#0070cd">Data
        recovery for kdb+tick</fo:inline></fo:basic-link><fo:footnote><fo:inline font-size="8pt" alignment-baseline="hanging">6</fo:inline><fo:footnote-body><fo:block font-size="8pt">6.
									http://code.kx.com/q/wp/data_recovery_for_kdb_tick.pdf</fo:block></fo:footnote-body></fo:footnote> for a complete understanding of the
        recovery from a tickerplant log file, including how to deal with
        a corrupted log file
      </fo:block>
    </fo:block>
    <fo:block id="a-historical-database-fails-to-load">
      <fo:block font-weight="bold" font-size="14pt" line-height="17pt" margin-right="36pt" page-break-after="avoid" space-after="6pt" space-before="18pt" text-align="left">…a historical database fails to load</fo:block>
      <fo:block line-height="16pt" space-after="9pt" text-align="justify">
        Some well-developed kdb+ systems survived many market events
        which brought other systems to their knees, however, it is
        possible that an HDB may become unreachable, but through proper
        database maintenance, access rights, correct dimensioning and
        recovery procedures, users can avoid incapacitating an HDB. HDB
        process failures and <fo:inline font-family="Pragmata Pro">wsfull</fo:inline> errors are
        usually an indication that a process has been incorrectly
        dimensioned, or access control has not been applied.
      </fo:block>
      <fo:block line-height="16pt" space-after="9pt" text-align="justify">
        If an HDB fails to load, there could be various underlying
        causes (corrupted partition, corrupted sym file, spurious file
        in the HDB directory, read/write access), but the most usual
        case is a corrupted partition. This could potentially be due to
        an error in the end-of-day write from the RDB.
      </fo:block>
      <fo:block line-height="16pt" space-after="9pt" text-align="justify">
        If the HDB is serving queries to the gateway similar to those
        described in the preceding section, then it would be recommended
        to route future queries to the secondary HDB. This could be
        signaled via error handling in the end-of-day function when the
        re-load of the HDB following write down fails.
      </fo:block>
      <fo:block line-height="16pt" space-after="9pt" text-align="justify">
        To restore functionality of the HDB, due to the complexity and
        variety of potential issues, it is not recommended to automate
        any editing onto database partitions. However, auto detection of
        many bad states of an HDB is possible and desirable in a lot of
        cases and this can guide any necessary manual intervention to
        fix the issue.
      </fo:block>
      <fo:block line-height="16pt" space-after="9pt" text-align="justify">
        Some common issues along with possible fixes:
      </fo:block>
      <fo:block id="a-missing-table-from-a-partition">
        <fo:block font-size="12pt" font-weight="bold" line-height="17pt" margin-right="36pt" page-break-after="avoid" space-after="6pt" space-before="18pt" text-align="left">A missing table from a partition</fo:block>
        <fo:block line-height="16pt" space-after="9pt" text-align="justify">
          This is fixed using either <fo:inline font-family="Pragmata Pro">.Q.chk</fo:inline> to save
          empty copies of all tables or <fo:inline font-family="Pragmata Pro">.Q.bv</fo:inline> to
          create in-memory mapping of missing tables. This may arise due
          to a lack of data having been received for this date, and the
          end-of-day write function therefore skips this save. Unless
          <fo:inline font-family="Pragmata Pro">.Q.bv</fo:inline> is in use within HDB processes then
          the end-of-day write function should save empty tables as well
          as those containing data, either directly, or via
          <fo:inline font-family="Pragmata Pro">.Q.chk</fo:inline>.
        </fo:block>
      </fo:block>
      <fo:block id="schema-changes">
        <fo:block font-size="12pt" font-weight="bold" line-height="17pt" margin-right="36pt" page-break-after="avoid" space-after="6pt" space-before="18pt" text-align="left">Schema changes</fo:block>
        <fo:block line-height="16pt" space-after="9pt" text-align="justify">
          These can cause issues if not implemented correctly. Often
          this could be missing files in older partitions for the new
          fields. This can be resolved by saving a file of null values
          of the same type and the same length as the other fields for
          that date. The <fo:inline font-family="Pragmata Pro">.d</fo:inline> file should be updated
          also. <fo:inline font-family="Pragmata Pro">fixtable</fo:inline> from dbmaint can be used to
          save these empty columns, but a manual example is given below:
        </fo:block>
        <fo:block font-family="Pragmata Pro" font-size="10pt" margin-bottom="12pt" margin-left="10pt" page-break-inside="avoid" white-space="pre">
/hdb
/set a list of null floats as newcol in trade table for an older partition 
`:./2016.12.01/trade/newcol set count[get `:./2016.12.01/trade/sym]#0nf 
/update .d file in old date partition
`:./2016.12.01/trade/.d set cols trade
</fo:block>
      </fo:block>
      <fo:block id="a-length-error">
        <fo:block font-size="12pt" font-weight="bold" line-height="17pt" margin-right="36pt" page-break-after="avoid" space-after="6pt" space-before="18pt" text-align="left">A length error</fo:block>
        <fo:block line-height="16pt" space-after="9pt" text-align="justify">
          This is usually due to a column vector being a different
          length from the rest of the table. The only method to fix it
          is to count each column file and compare, then manually saving
          the erroneous one to the correct length. This can occur when
          not using the standard save commands (.e.g
          <fo:inline font-family="Pragmata Pro">.Q.dpft</fo:inline>), but rather setting each column
          individually, and some logic has caused the column lengths to
          vary.
        </fo:block>
      </fo:block>
    </fo:block>
    <fo:block id="the-tickerplant-fails">
      <fo:block font-weight="bold" font-size="14pt" line-height="17pt" margin-right="36pt" page-break-after="avoid" space-after="6pt" space-before="18pt" text-align="left">…the tickerplant fails</fo:block>
      <fo:block line-height="16pt" space-after="9pt" text-align="justify">
        In an adequately specified 64-bit system, one tickerplant is
        usually enough to capture all the data from a feed. A kdb+
        process is able to process and store up to 4.5 million streaming
        records per second per core and up to 10 million if batching. If
        the throughput is expected to exceed these figures then
        additional tickerplants or cores may be required.
      </fo:block>
      <fo:block line-height="16pt" space-after="9pt" text-align="justify">
        It is very unusual that a tickerplant process would fail on its
        own, as it is limited to certain specialized tasks; though if
        incorrectly configured it may fail. If a tickerplant does go
        down and fails to restart, then there will likely be significant
        data loss on the primary system. If the tickerplant is capturing
        high volumes of market data, even being down for a few seconds
        could lead to hundreds of thousands of missed rows of data.
      </fo:block>
      <fo:block line-height="16pt" space-after="9pt" text-align="justify">
        Failure in a multiple tickerplant environment can be treated in
        several different ways, depending on the application and failure
        type. One possibility is to switch the entire system to the
        secondary system. This is the standard approach taken in most
        applications. The easiest way of failing over in this way is to
        shut down the primary processes, causing all further queries to
        route to the secondary. Then, during a scheduled downtime
        period, the underlying cause of the failure can be investigated
        and addressed.
      </fo:block>
      <fo:block line-height="16pt" space-after="9pt" text-align="justify">
        Alternatively, in some more complex systems, it may be
        preferable to only failover individual processes or a selection
        of relevant processes instead of the entire system. In this
        scenario, if the tickerplant dies, this could trigger a failover
        of the subscriber to the secondary process. The subscriber isn’t
        lost and could reconnect to the secondary TP, allowing seamless
        transition. Clients that have subscribed to the tickerplant will
        receive a closed connection callback to
        <fo:inline font-family="Pragmata Pro">.z.pc</fo:inline>. They could then use this to seamlessly
        switch over to the secondary. Depending on the time taken to
        switch and connect to the secondary tickerplant, there may be
        some data loss. However, the primary and secondary systems are
        usually in separate data centers, therefore this procedure would
        force data to be considered in large volumes between sites.
        Hence, this option needs to be carefully considered against the
        specific architecture of the system.
      </fo:block>
      <fo:block line-height="16pt" space-after="9pt" text-align="justify">
        Following failover of the system or subscribers, due to the data
        loss or risk thereof, it is necessary to reconcile the data
        between the primary and secondary databases. Refer to the
        following section on data reconciliation for further details.
      </fo:block>
      <fo:block line-height="16pt" space-after="9pt" text-align="justify">
        As mentioned previously, during replay of a large tickerplant
        log file, pending messages will sit in the IPC queue waiting
        until the subscriber (RDB) has completed the replay. If however,
        the log file is sufficiently large that the IPC buffer fills
        with pending messages, then the overflow can bloat the TP
        memory. In the financial world, this can be the case late in the
        trading day when the log files are nearly at the maximum size,
        so restarting an RDB just before market close is the worst
        scenario. Unless protected from slow subscribers, the
        tickerplant memory footprint may, in extreme circumstances, grow
        to an unmanageable level, resulting in a
        <fo:inline font-family="Pragmata Pro">wsfull</fo:inline> error. Therefore, aside from suitable
        dimensioning of the TP host machine for RDB restarts, the
        tickerplant log files should reside on high-performance,
        dedicated drives with no contention from other processes. This
        ensures the RDBs will have shortest recovery time, and therefore
        reduce the risk of <fo:inline font-family="Pragmata Pro">wsfull</fo:inline> failures on the
        tickerplant.
      </fo:block>
      <fo:block line-height="16pt" space-after="9pt" text-align="justify">
        In some implementations intra-day write-down solutions are used
        to limit the size of these tickerplant log files to less than a
        whole day to limit the recovery time of any replaying of these
        logs. Other implementations have a ring-fenced instance for
        write-downs often termed PDB (persisting database) or WDB (write
        database).
      </fo:block>
    </fo:block>
    <fo:block id="the-hardware-fails">
      <fo:block font-weight="bold" font-size="14pt" line-height="17pt" margin-right="36pt" page-break-after="avoid" space-after="6pt" space-before="18pt" text-align="left">…the hardware fails</fo:block>
      <fo:block line-height="16pt" space-after="9pt" text-align="justify">
        Infrastructure should be designed and implemented with no single
        points of failure at the primary and disaster recovery
        facilities. Even with these precautions, hardware failures do
        happen. To help recover from a hardware failure or reduce its
        impact, we recommend that a system have multiple copies of data
        running on different infrastructure, in addition to
        point-in-time backups stored on physically separate storage
        devices.
      </fo:block>
      <fo:block line-height="16pt" space-after="9pt" text-align="justify">
        Data can be copied, backed up and synchronized using a
        combination of file system utilities, kdb+ processes, and
        storage hardware and software disk-based replication. The choice
        of approach depends on availability, RTO, RPO, performance
        requirements and available budget.
      </fo:block>
      <fo:block line-height="16pt" space-after="9pt" text-align="justify">
        Data recovery from a point-in-time backup will require restoring
        the system using both the point-in-time backup and tickerplant
        log files, which will contain any messages received since the
        last backup. Therefore it is extremely important to have
        sufficient backups of tickerplant log files in addition to
        backups of the database.
      </fo:block>
      <fo:block line-height="16pt" space-after="9pt" text-align="justify">
        During a hardware failure, such as a disk failure, the
        performance of a primary system may be degraded during the
        recovery process, which may necessitate failing over to a
        secondary or disaster recovery system. Provided the secondary
        systems are running on different hardware, a hardware failure
        can be treated similarly to a software failure and failover can
        again be controlled via <fo:inline font-family="Pragmata Pro">.z.pc</fo:inline> signaling. If
        however, the secondary system is running on the same server or
        infrastructure, no recovery can easily be made until the
        hardware issue has been resolved. There will possibly be data
        loss as well in this scenario.
      </fo:block>
    </fo:block>
    <fo:block id="there-are-network-connection-issues">
      <fo:block font-weight="bold" font-size="14pt" line-height="17pt" margin-right="36pt" page-break-after="avoid" space-after="6pt" space-before="18pt" text-align="left">…there are network connection issues</fo:block>
      <fo:block line-height="16pt" space-after="9pt" text-align="justify">
        Network connection problems can sometimes be hard to diagnose.
        However they are treated similarly to a software failure as an
        unreachable process is deemed indistinguishable from a
        non-running process. A full failover to the secondary system
        should be made if there is a network failure on the primary.
        This can again be controlled via <fo:inline font-family="Pragmata Pro">.z.pc</fo:inline>
        signaling.
      </fo:block>
    </fo:block>
  </fo:block>
  <fo:block id="data-reconciliation-and-synchronization">
    <fo:block id="idm106820716352" font-size="18pt" line-height="22pt" margin-right="36pt" page-break-before="always" space-after="60pt" text-align="left">Data reconciliation and synchronization</fo:block>
    <fo:block line-height="16pt" space-after="9pt" text-align="justify">
      In the event of a failover from the primary to the secondary
      system, there will always be a risk of data loss on the primary
      side. Even if callbacks were used to switch subscription to the
      secondary, during this switching period there may be some data
      missed, for example, the secondary tickerplant may have processed
      additional messages prior to the receipt of the subscription
      request (<fo:inline font-family="Pragmata Pro">.u.sub</fo:inline>) from the primary RDB. As such,
      it is usual for a process of data reconciliation to be configured
      for the end of the day.
    </fo:block>
    <fo:block line-height="16pt" space-after="9pt" text-align="justify">
      In the most basic setup, this could simply be a copy of today’s
      HDB partition (and syncing the <fo:inline font-style="italic">sym</fo:inline> file). A
      more thorough method would be running some checks on the primary
      data for the day to check if any data was missed prior to any
      copies. Control reports are often used to perform periodic checks
      to ensure both systems remain in sync near-real time. These checks
      can be used as part of the reconciliation period to ensure that
      the process worked, or to check which data is required for
      syncing. A basic example would be getting the totals or counts
      from each table. As most data feeds publish sequence numbers these
      can also be used for assessing data correctness.
    </fo:block>
    <fo:block line-height="16pt" space-after="9pt" text-align="justify">
      In most systems, there are other types of non-tick data stored in
      the database, such as reference data. Depending on the design of
      the reference data loading process and the nature of the DR event
      this data may be required to be synced between both systems during
      the reconciliation process. Again this may be a copy from the
      secondary system to the primary system, but more often reference
      data is loaded directly into the primary system, usually from a
      CSV file, and then checks are performed to ensure that the correct
      reference data was loaded.
    </fo:block>
  </fo:block>
  <fo:block id="what-is-the-best-configuration-for-high-availability">
    <fo:block id="idm106820711648" font-size="18pt" line-height="22pt" margin-right="36pt" page-break-before="always" space-after="60pt" text-align="left">What is the best configuration for high availability?</fo:block>
    <fo:block line-height="16pt" space-after="9pt" text-align="justify">
      A hot-hot configuration (primary and secondary) along with a cold
      offline/offsite backup (DR) is usually the best configuration to
      support high-availability and data security, but dependent on
      hardware resources and availability requirements, a cold DR
      copy/backup may be sufficient.
    </fo:block>
    <fo:block line-height="16pt" space-after="9pt" text-align="justify">
      Of critical importance is backing up the sym file within the HDB
      root, sufficiently and regularly. This file is the key to the HDB,
      (all symbol type fields within the database are enumerated against
      this file) and if lost then the data will be very difficult to
      recover.
    </fo:block>
    <fo:block line-height="16pt" space-after="9pt" text-align="justify">
      The sym file is an ordered list of symbols, (e.g.
      <fo:inline font-family="Pragmata Pro">`GOOG`APPL`MSFT…</fo:inline>) sometimes tens of thousands
      of items long. Symbol type columns within the HDB are stored as
      integers that map to the position index within the sym vector on
      loading of the HDB. If the ordering is lost, or incorrect then the
      mapping of symbol type columns within the HDB will be wrong.
    </fo:block>
    <fo:block line-height="16pt" space-after="9pt" text-align="justify">
      If it is necessary to recover the sym file, then the previous sym
      file backup should be used, against which any new symbols received
      into the system may be added, in order. Fortunately the
      tickerplant log files contain these new symbols, so it is simply a
      matter of replaying the necessary TP logs files in order and
      enumerating the replayed data against the restored sym file.
      However it is important to enumerate the data in the same order
      that the data was originally saved, and also be aware of any other
      non-tick data which may have been enumerated against the
      <fo:inline font-style="italic">sym</fo:inline> file since the last backup.
    </fo:block>
  </fo:block>
  <fo:block id="how-does-the-system-recover">
    <fo:block id="idm106820706512" font-size="18pt" line-height="22pt" margin-right="36pt" page-break-before="always" space-after="60pt" text-align="left">How does the system recover?</fo:block>
    <fo:block line-height="16pt" space-after="9pt" text-align="justify">
      When a system has failed over to the secondary side, this is the
      last line of defense and usually, there are no more levels of
      active redundancy. If anything further should happen, the system
      will likely be offline, and depending on the nature of the failure
      event, there could be data loss and potential problems with
      recovery. As such, it is important to restore the primary to a
      functional state as soon as possible.
    </fo:block>
    <fo:block line-height="16pt" space-after="9pt" text-align="justify">
      For a process to have failed, there could have been a variety of
      reasons. Therefore the first step in fully recovering the system
      is understanding why the failure occurred in the first place.
      Perhaps the rate of streaming data increased significantly beyond
      the expectations when the server was originally specified. Perhaps
      a poorly-formed query caused the memory usage in the RDB to spike.
    </fo:block>
    <fo:block line-height="16pt" space-after="9pt" text-align="justify">
      Usually failing back to the primary node should be done outside of
      operational hours. This allows for the necessary downtime to
      investigate the underlying cause and allows for the possibility of
      other issues occurring on restart. However as mentioned previously
      this may not be possible in systems that have no maintenance or
      downtime window available for fail-back.
    </fo:block>
    <fo:block line-height="16pt" space-after="9pt" text-align="justify">
      If this is the case the restoration of the primary must occur in
      parallel with normal operations on the secondary system. During
      this period all queries should be routed to the secondary, and it
      is only when recovery is complete and functionality is validated
      that the primary should be made available for receiving new data
      and serving queries. As the typical configuration is hot-hot, then
      once the primary is restored fail-back is complete as both primary
      and secondary are available for data capture and to serve queries.
    </fo:block>
    <fo:block line-height="16pt" space-after="9pt" text-align="justify">
      In alternative configurations such as hot-warm, an additional step
      is required to fully restore the primary as the hot system. This
      can be as simple as re-routing queries to only hit the primary
      system.
    </fo:block>
  </fo:block>
  <fo:block id="additional-kdb-features-for-disaster-recovery">
    <fo:block id="idm106820701792" font-size="18pt" line-height="22pt" margin-right="36pt" page-break-before="always" space-after="60pt" text-align="left">Additional kdb+ features for disaster recovery</fo:block>
    <fo:block line-height="16pt" space-after="9pt" text-align="justify">
      The previous sections describe the suggested approach for
      designing disaster recovery and failover from a kdb+ tick point of
      view, with database backups and ticker-plant log files used to
      avoid data loss. However, kdb+ also provides in-built logging,
      checkpointing and replication which can provide a level of
      redundancy in more complex setups.
    </fo:block>
    <fo:block line-height="16pt" space-after="9pt" text-align="justify">
      A basic example of these features could be using the replication
      to support CEP processes in syncing a table between primary and
      secondary. This process pair can be configured such that the
      secondary subscribes to the primary, and all state changes on the
      master (primary) are automatically replicated to the secondary.
      These changes are logged to disk as well, to allow for recovery in
      the event that the process fails.
    </fo:block>
    <fo:block line-height="16pt" space-after="9pt" text-align="justify">
      <fo:inline font-family="Material Icons" font-size="14pt"></fo:inline> Cookbook:
      <fo:basic-link external-destination="url('http://code.kx.com/q/cookbook/logging/')"><fo:inline color="#0070cd">Logging</fo:inline></fo:basic-link><fo:footnote><fo:inline font-size="8pt" alignment-baseline="hanging">7</fo:inline><fo:footnote-body><fo:block font-size="8pt">7.
									http://code.kx.com/q/cookbook/logging/</fo:block></fo:footnote-body></fo:footnote> for full
      details on these options
    </fo:block>
  </fo:block>
  <fo:block id="control-for-kx">
    <fo:block id="idm106820697424" font-size="18pt" line-height="22pt" margin-right="36pt" page-break-before="always" space-after="60pt" text-align="left">Control for Kx</fo:block>
    <fo:block line-height="16pt" space-after="9pt" text-align="justify">
      <fo:inline font-style="italic">Control for Kx</fo:inline> is a code- and
      process-management system that provides robust failover management
      along with standardized logging, error handling, and process
      monitoring. These features, coupled with the process templates,
      provide the framework for quicker and easier development of custom
      kdb+ applications. Tried-and-tested failover solutions provide the
      flexibility and scalability needed to meet any resilience or
      availability requirements.
    </fo:block>
    <fo:block line-height="16pt" space-after="9pt" text-align="justify">
      <fo:inline font-family="Material Icons" font-size="14pt"></fo:inline>
      <fo:basic-link external-destination="url('https://kx.com/solutions/#EnterpriseFeatures')"><fo:inline color="#0070cd">kx.com/solutions</fo:inline></fo:basic-link><fo:footnote><fo:inline font-size="8pt" alignment-baseline="hanging">8</fo:inline><fo:footnote-body><fo:block font-size="8pt">8.
									https://kx.com/solutions/#EnterpriseFeatures</fo:block></fo:footnote-body></fo:footnote>
    </fo:block>
  </fo:block>
  <fo:block id="conclusion">
    <fo:block id="idm106820693408" font-size="18pt" line-height="22pt" margin-right="36pt" page-break-before="always" space-after="60pt" text-align="left">Conclusion</fo:block>
    <fo:block line-height="16pt" space-after="9pt" text-align="justify">
      This whitepaper has stressed the importance of disaster-recovery
      planning. There is always a possibility of disastrous events
      occurring, therefore it is vital to plan effectively to mitigate
      any impact on the system if problems occur. The paper has covered
      the majority of possible failure scenarios which can occur in a
      kdb+ tick system and explained how the operations should be
      designed to handle these failures. DR should cover permutations
      based on a risk assessment (impact and likelihood), therefore as
      part of DR planning one of the main questions to consider is
      
		‘does the plan account for all possible permutations
      considered in the risk assessment?’
	 That is, does the
      design include steps if all/any of the critical and/or
      non-critical components are compromised? If not, then the DR plan
      is incomplete.
    </fo:block>
    <fo:block line-height="16pt" space-after="9pt" text-align="justify">
      Murphy’s Law states that if it can go wrong, it will go wrong. By
      this adage, if the DR design has omitted a potential failure path,
      then it will happen. Therefore, all possible event paths must be
      accounted and planned for in the design, such that any system
      impact caused by future events can be minimized.
    </fo:block>
    <fo:block line-height="16pt" space-after="9pt" text-align="justify">
      In our example system, the main goal was to minimize downtime and
      maintain availability of the system following major incidents and
      DR events. Depending on the use case, minimizing downtime may be
      prioritized over preventing data loss. In both cases, if the
      design has not correctly allowed for regular data backups, there
      will be issues when trying to recover during failover. This is why
      backups of the tickerplant log files are necessary in addition to
      backups of the database, as any 
		‘lost’
	 tick data can
      be replayed from these logs.
    </fo:block>
    <fo:block line-height="16pt" space-after="9pt" text-align="justify">
      It is important to note that even with the best planning, unknown
      events can still occur. The issues experienced by an airline in
      the example given in the opening paragraph were caused by human
      error, which was completely unplanned for. With this in mind, it
      is important to understand the implications of these failure
      events, and through a proper recovery plan based on a risk
      assessment and RPO and RTO specifications design a solution which
      can minimize downtime (i.e. high availability), recover quickly
      and accurately from these events, and most importantly prevent and
      minimize data loss.
    </fo:block>
    
  </fo:block>
  <fo:block id="appendix">
    <fo:block id="idm106820685728" font-size="18pt" line-height="22pt" margin-right="36pt" page-break-before="always" space-after="60pt" text-align="left">Appendix</fo:block>
    <fo:block line-height="16pt" space-after="9pt" text-align="justify">
      The examples here are edits of <fo:inline font-family="Pragmata Pro">r.q</fo:inline> from kdb+
      tick.
    </fo:block>
    <fo:block line-height="16pt" space-after="9pt" text-align="justify">
      As discussed in the main paper, often the primary and secondary
      sites are separate, hence large data transfer between the two
      could be problematic.
    </fo:block>
    <fo:block font-family="Pragmata Pro" font-size="10pt" margin-bottom="12pt" margin-left="10pt" page-break-inside="avoid" white-space="pre">
/If TP subscription is killed fail over to backup
TP .z.pc:{neg[hopen[`:localhost:6010]]".u.sub[`;`]";}
</fo:block>
    <fo:block line-height="16pt" space-after="9pt" text-align="justify">
      Opening a handle to a separate process takes some time.
      Additionally, as the server may be busy it is recommended that the
      all necessary handles are opened on startup.
    </fo:block>
    <fo:block font-family="Pragmata Pro" font-size="10pt" margin-bottom="12pt" margin-left="10pt" page-break-inside="avoid" white-space="pre">
/Add to r.q
/open handles to primary and backup tp, using details passed in at startup 
.tp.handles:(),@[{hopen `$":",x};;()] each .u.x 0 1;
/replace default .u.rep call with one referencing .tp.handles 
.u.rep .(first .tp.handles)"(.u.sub[`;`];`.u `i`L)";
/.z.pc now just has to subscribe to the backup TP and the handle is already open 
.z.pc:{.tp.handles:.tp.handles except x;(first .tp.handles)".u.sub[`;`]";}
</fo:block>
    <fo:block line-height="16pt" space-after="9pt" text-align="justify">
      Full details on GitHub at 
      <fo:basic-link external-destination="url('https://github.com/KxSystems/kdb-tick/blob/master/tick/r.q')"><fo:inline color="#0070cd">KxSystems/kdb-tick/tick/r.q</fo:inline></fo:basic-link><fo:footnote><fo:inline font-size="8pt" alignment-baseline="hanging">9</fo:inline><fo:footnote-body><fo:block font-size="8pt">9.
									https://github.com/KxSystems/kdb-tick/blob/master/tick/r.q</fo:block></fo:footnote-body></fo:footnote>
    </fo:block>
  </fo:block>
</fo:block></fo:flow></fo:page-sequence></fo:root>
